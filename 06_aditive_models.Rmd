# Extensions for linear models

## Introduction

As we have already mentioned, the linear model suffers some serious limitations, which steam from the asumption that the model follows a linear treand. In real life, relations between predictors and reaction are (almost always) more complicated. They can be changing as the preidctors take on higher/lower values. When we explained linear regression we also show polynomial regression, which was able to better explain some of those alternations and optimize our models' performance. In this chapter we will look into auther such extensions of the linear models. As a generall idea, we will be attempting to capture variations on this relationship, by spliting the data on different ranges of the predictors (where the variation occures), and fitting a different model on each.

Following that though, Aditive models are usuful when:

* On the scatterplot we observe that the way the predictor is relatated to the reaction changes for deferent ranges of the predictor. If you spot various 'curving' patterns.

* In such cases, algorithms such us desition tree based may result in similar or better performance models, with less of the complexity. You may want to use an Additive model, if inference is key in your analysis or as a comparison point in your benchmarking. As always, this will highly depend on your problem, the data you have and your resources. It is adviced to try out various models and cross validate them.

## Step Function

This method suggests breaking down the model into _bins_. Each bin will have its own coefficients that are only valide for a certain range of the predictors $X$. Each bin is assigned a constant $C_1$, $C_2$, $C_3$, $C_k$ which can either be 1 or 0, indicatating if the value that $X$ takes, is inside or outside the bin's range. Doing so allows us to represent the function as follows and sove using least squares.

$ y_j = \beta_0 + \beta_1 C_1 x_1 + \beta_2 C_2 x_1 + ... + \beta_k C_k x_1 + \beta_1 C_1 x_2 + \beta_2 C_2 x_2 + ... + \beta_k C_k x_2 + ... + \beta_1 C_1 x_j + \beta_2 C_2 x_j + ... + \beta_k C_k x_j$

where for each $x_j$ you can have up to one $C_k=1$ (one or none $C_k$ will be true), since it can only have up to one coefficient depending on which bin it bellongs to.

This method comes with a major disadvatage. Since each bin's coeffients are derived using only data that is within the bin's range, we might miss capturing global trends.

Generally, you whould consider using this method if :

* The bins you want to split your data into are well defined by nature. For example, this method is popular in disciplines such us biostatistics and epidemology, where the relationships studied are very different for patiants with 5-year differences (bins whould split the data for every age group).

* You have a lot of coefficients out of which some are usufull for sertain ranges and others are usufll for other ranges

```{r}
# we will use the dataset Wage, made by ISLR to fit well for additive models
library(ISLR)
data(Wage)

# the cut() function will help us split the data in bins. We can select how many bins we want, here we have selected 4. Furthermore, we can set the cutpoints, which whould be used to place each bin with the option breakes=,. If this is not spacified R will choose to place each bin uniformly acrros the range of x. In this case R has set a bin for every 15.5 year group.
t <-table(cut(Wage$age,4))
t

model <- lm(wage~cut(age,4), data = Wage)
# here you can see that each age group has its own coefficient estimate. 
# notice that the age group 17.9 to 33.5 does not have a coefficient. For those values prediction whould simply be their avarage wage given by the intercept $94.158.
model

# let's see how this looks like
library(ggplot2)

x1 <- seq(17.9, 33.5, 0.1)
y1 <- rep(94.158, times = length(x1) )

y2 <- integer()
x2 <- seq(33.5, 49, 0.1)
for(i in 1:length(x2)){
  y2[i] <-  94.158 + 24.053* x2[i]
}

y3 <- integer()
x3 <- seq(49, 64.5, 0.1)
for(i in 1:length(x3)){
  y3[i] <-  94.158 +  23.665* x3[i]
}

y4 <- integer()
x4 <- seq(64.5, 80.1, 0.1)
for(i in 1:length(x4)){
  y4[i] <-  94.158 +  7.641* x4[i]
}


x <- c(x1,x2, x3, x4)
y <- c(y1,y2, y3, y4)

d=data.frame(x=x, y=y)
ggplot() +
geom_step(data=d, mapping=aes(x=x, y=y)) +
geom_step(data=d, mapping=aes(x=x, y=y), direction="vh", linetype=3) +
geom_point(data=d, mapping=aes(x=x, y=y), color="red") 

```


## Splines

As we can see from the graph, the endpoints are not fitting well, the model is not 'smooth'. And although it is clearly not as strick and overgenralized as a simple linear model, it may not do a great job of describing the revaltionship between age and wage. If we look at the scatterplot, we can see that a curve rather than a line chould be more fitting.

```{r}
plot(Wage$age, Wage$wage)

```

We already know that we can use a polynomial in order to fit a curve to the data. We chould try various degrees of polynomials and cross validate them to see which performed the best. Polynomials however, have various pitfalls. They usually perform well on quite high degrees. High degrees of polynomials will curve the data too much in unessasry places and can cause inaccurate analysis, among other issues. Instead we whould like to use _piecewise polynomial regression_, where we whould choose a polynomial of smaller degree but achieve optimal performance. In piecewise polynomial regression we fit a model using deferent coefficiants for various ranges of $x$. Quite often the cubic (3rd degree polynomial) performs well, here is what a function for cubic piecewise polynomial regression whould look like:

$y_i = \left\{ \begin{array}{ll} \beta_01 + \beta_11x_j + \beta_21x_j^2 + \beta_31x_j^3 & \mbox{if } x_y \geq c \\ \beta_02 + \beta_12x_j + \beta_22x_j^2 + \beta_32x_j^3 & \mbox{if } x_j < c \end{array} \right.$ 

Where the range of $x$ is been 'cut' on the point $c$. The point $c$ is called a knot. We can add more knots if we want to increase the flexibility of the model.

When we apply such a function we have manged to overcome some of the pitfalls of polynomial regression. However, the model whould be discontinious at the points of the knots, here is an example where we chosen a single knot at age 50. 

```{r}
data1 <- Wage[which(Wage$age >= 50),]
data2 <- Wage[which(Wage$age<50),]

fit1 <- lm(wage ~ age + I(age^2) + I(age^3), data = data1)
fit2 <- lm(wage ~ age + I(age^2) + I(age^3), data = data2)

age.grid1 <- seq(from=18,to=50)
age.grid1 <- seq(from=50,to=80)

y1<- predict(fit1,list(age=age.grid1))
y2<- predict(fit2,list(age=age.grid2))

plot(Wage$wage ~ Wage$age, data = Wage,col="#CCBADC")
with(data.frame(age.grid1), lines(x = age.grid1, y = y1))
with(data.frame(age.grid2), lines(x = age.grid2, y = y2))

```

We now need to 'smoothen' the grapth. In other words, we want we want the rate of change for its slope (which discribes its shape) to be the same at the point of the knots, so as to avoid weird 'bamps'. This is given by the second derivative. So we can add the condition that the second derivatives of the left and right functions need to be the same, at the point of the knot (50 in this example). Adding this consrain will create what is called a _natural_ spline. There are other ways to smoothen the grath such us a _clamp_, where instead we require the first dervivatives to be equal to zero.

Here is what happens when we add this condition


```{r}
#library for fitting cubic splines
library(splines)
model <- lm(wage~bs(age,knots=c(50)), data=Wage)
agelims <- range(Wage$age)
age.grid <- seq(from=agelims[1],to=agelims[2])

pred<- predict(model,list(age=age.grid),se=T)
plot(Wage$age, Wage$wage, col="#CCBADC")
lines(age.grid, pred$fit, lwd=2)

```

```{r}
#if you are not sure how many knots to place and where you can instead choose your degrees of freedom, the knots are placed uniformly by R:

model <- lm(wage~ns(age, df=4), data=Wage)
pred <- predict(model, newdata = list(age=age.grid), se=T)
plot(Wage$age, Wage$wage, col="#CCBADC")
lines(age.grid, pred$fit, lwd=2)

# if we increase the degrees of freedom we allow for more 'wiggle' room and risk overfitting

model <- lm(wage~ns(age, df=30), data=Wage)
pred <- predict(model, newdata = list(age=age.grid), se=T)
plot(Wage$age, Wage$wage, col="#CCBADC")
lines(age.grid, pred$fit, lwd=2)

```

## Smoothing splines



## Local Regression








