---
title: "recommendation systems"
author: "Sofia Kyriazidi"
date: "19 November 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Recomending similar books. Content based filtering
Fist we will create the algorithm that is used to recomend 'similar books' to the one the user just bought. That algorithm will simply look at how similar the book we just bought is, to ather books avalable, acorsing to the given attributes
(author, genre, age). This aproach is called Content based filtering. 

## What is similarity ?

For us humans, distinguising similar items is a simple task. We look at the properties of the item (in this case the book's author theme and age)
and we can group together books by the same author, same theme and similar age. If we want a machine to perform that task we need to generalize this intution  and describe it using a set of clear instractions.

In machine learning, similarity if ofeten described by how far two items are from each other. Distance is an effective metric as it quatifiable and clear. This distance whould be in terms of the given attributes.

To understand this, we can imagine ploting all of our points, using their attributes as coordinates. The closer each point is to another the more 'similar' they whould be. For example if we only used the x axis (a straigh line), whose values  represent the age of each book. We chould then mark where on that line each book is according to its age. Marks that are close to  each other whould show us books that have similar age. To define 'similarity' in a more realistic manner we need to use the rest of attributes. So instead of just a line we plot our points in a graph where each axis (known as dimention) represent an attribute.

This distance between points can be measured using various techniques such us: 

*Euclidian distance. Similar to computing distances in 2 dimentional distances, we use the co-rdinates of the points to draw a 
right triangle with two known sides and compute the unknown, wich represents the distance, using the pythagorium theorum.

*Correlation distance. Basicly measures how 'in-synch' (do they both go up or down) the diviations from the mean of each point are
 for each user. Users that have rated the same product higher than their avagare and the same products lower than their avarage
are considered close to each other.

*Hammer's distance. Simply measures the persentage of agreement. How many times two users gave the exact same rating.

## How can we find similar books?
As we discussed above, similarity is dicribed in terms of distance between points. So we whould have to 'plot books over the axis of age, author and theme'.
and find which books are close to each other. Author and theme, are however categorical varibales (non-numeric), so we can not easily plot them in the way we want to. An simple way to bypass this, whould be to assign a numeric 'dummy' variables, representitive of each value a categoric attribute takes on. For example, theme love story whould be 1, theme piretes 2, aliens 3, drama 4 and so on. Once this is complete we can plot the books in terms of theme using those dummy variables. The books with the same theme whould be close to each other, corecly discribing similarity. What is the problem?

The problem is on the order of dummy variables. When we plot this our model will deside that love story (number 1) is more similar to pirates (number 2), as they are closer to each other, than let's say drama which happens to be number 4. We whould have to give an order that makes logical sence, which can be exhaustive. Sometimes such an order may not even be meaningful or very challenging, in the example of authors what whould make an author be more similar to another author?

For that reason, we have chosen to work with Hammer's distance for the categorical variables (simply looks whether or not two values are the same) and absolute distances for the age. We have created our own algorithm, which follows our intuition of what similarity is, let's see how it works!

```R

#first we load the dataset i have prapered, it contains the following observations:
#books, which contain the book number (from 1 to 20), author (a,b,c,d), the genre (piret,alien,love,drama)
#and how many years ago the book was written
books <- read.csv("~/Projects/statistical_analysis/recomendationSystems/books.txt")

#for every book that a user buys, we want to recomend 'similar' ones. So every time the user clicks 'Submit' to buy a specific book. We want to claculate 
#the distances from his book to the rest of the books avalble and recoment the ones that are closer (efficiency considerations and optimizations are discussed #later, for now let's keep things simple for the perpuse of understanding the concepts).

#So the first step is to get the distances of this book to the rest. This is what this function does.

#it receives the book number of the book that was just bought as a parameter
get.distance <- function(bookNumber){
  #it returns a data frame containing various distance metrics
  df <- data.frame(CompBook=integer(),
                   hammingDist=integer(),
                   ageDist = integer(),
                   totalDist = integer()
                   ) 
  #first we capure the autor, genre and age of the book that was bought,
  #since those values will be used to compare similarity
  author <-books[books$book==bookNumber,]$author 
  genre <- books[books$book==bookNumber,]$genre
  age <- books[books$book==bookNumber,]$age
  
  
  #then for every other book avalble we 
  for (i in 1:20){
  #capture its author, genre and theme 
  authorComp  <-books[books$book==i,]$author 
  genreComp  <- books[books$book==i,]$genre
  ageComp  <- books[books$book==i,]$age
  
  #reset our distance metrics to zero
  hammingDistance <- 0
  ageDist <- 0
  totalDist <-0
  
  #compute our Haming distance
  #if they do not have the same author, we want the distance to increase so we add one
  if (author != authorComp){
    hammingDistance = hammingDistance+1
  }
  #if they do not have the same genre, we want the distance to increase so we add one more
  if (genre != genreComp){
    hammingDistance = hammingDistance+1
  }
  
  #for the age, we simly get the absolute diference of the age from the book we just bough to the book we are comparing it to
  ageDist <- abs(age - ageComp)
  
  #it whould not make sence to simple add our hamming distance with the age distance, as the hamming distance can only go up to 1 for each value, whethereas the
  #age can get up to very high values. Age whould become far more significant than the rest of the attributes. Instead we use domain knowledge to interprete the 
  #this value. 
  
  if (ageDist > 10) {
  #if the age is greater that 10 years we add 0.5 to the distance (we assume this has been chosen from domain knowledge and studies avalble)
    totalDist <- hammingDistance + 0.5
    
  #if it is only grater that 5 we add 0.25
  }else if (ageDist > 5){ 
    totalDist <- hammingDistance + 0.25
    
  #otherwise total distance is just the one we have previously calculated, the hamming distance
    }else{
      totalDist <-hammingDistance
    }
  
  #we just need to add our calculated metrics to the data frame we need to return
  newdata <- c(i, hammingDistance,ageDist, totalDist)
  df<- rbind(df, newdata)
  names(df) <- c("CompBook", "hammingDist", "ageDist", "totalDist"  )
  
  }
  
  #and return them :)
  return(df)
  
}

#this function, is the one actually called when the user hits submit
getClosest3 <- function(bookNumber){

 #it makes use of the previous function to get the calculated distances from all our points
  distDataSet <- get.distance(bookNumber)
  
  #it removes the distance of our point to itself, as we do not want to remomment to the user the book he just bough (although teqnically it has the
  smallest distance)
  distDataSet <- distDataSet[! (distDataSet$CompBook==bookNumber),]
  
  #we order the resulting distances in ascending order (smallest first)
  orderedDistances <- distDataSet[order(distDataSet$totalDist, decreasing=FALSE), ]
  
  #choose the 3 first from the ordered data frame
  closest <- head(orderedDistances,3)
  
  #and return them! :)
  return(closest$CompBook)
}



```

# recomending books that were liked by 'similar' users, Collaborative filering   

###Similar users?

Similar user are just users that have provided similar ratings or feedback, purchased or previed similar objects and so on. In our case, we will focuse on users that have provided similar ratings. Although, our recommendations will be optimized with more and more data, it does not mean that we require every user to have rated every book for the algorithm to work. 

The idea here is that we use the rating matrix (see image 2, its simply a table presenting the rating of every user) and try to fill in the gaps, from missing ratings. Our model assumes that if a rating is missing that user has not read the respective book. So it wants to predict what his ratings whould have been if he had read it. If that prediction returns positive, the model will recomend that book to that user.


The model fills those gaps by looking at the ratings of other user's that had other similar ratings. Again, similarity can be identified by the distance of a user to onother, where their coordinates are the ratings for each book. 

Let's use an example to understand this. We want to recomend some books to reader A, reader A has already read a couple of our books and he has rated them. Reader A has not rated book1 and book 10, so we assume he has not read them and we want to see if it is worth recommending them.
First we find some other readers with ratings similar to reader A (from the rating matrix). Then we look at the ratings for book1 and book10 that were given by the group of similar users. For each book we get the avarage rating that was given by them. We can take a weighted avarage, weighted by how close each of those reader is to A, the closer the more significant. We have found that the avarage rating, given by readers close to A, is 2 for book 1 and 5 for book 10, so we recomend book 10 to reader A. 

The main advantage of this aproach is that we do not require information about the books themselfs. This can be extremely valuble for cases where accuaring that inforamtion is exhaustive and mayby even expesive and slow. Describing each item often requires manual work by humans that are experts on the relevant field. The disadvantage is that we need feedback from the users in order to recomend items. Not all users give feedback and often users tend to give feedback only on extreme cases such us very good or very bad, leading to models missing out on valable insight for a lot of products.

```R 

# We load the data, this is a list that i have prepared, it contsins the user, the book and the rating per row.
# each user has rated some of our books from 1 to 5 (where 5 is perfect)
# Not every users has rated every book
readersFeedback <- read.csv("~/Projects/statistical_analysis/recomendationSystems/readersFeedback.txt")

# From that data, we need to create the rating matrix as shown in the image. At the moment we have normalized data (user, book, rating)
# so we have from 1 to 20 entries for each user. This is not usufull if we want to find the distance of each user using his ratings. The
# ratings are the attributes that need to used as coordinates to plot each point.So we need to transform that data to a data that 
# has one entry for each user, and 20 columns containg ratings, one for each book.

#first we make the 20 vectors containing the ratings for each user, those will become coloumns
for (i in 1:20){
  assign(paste("book",i,sep=""), readersFeedback[readersFeedback$book==i,]$rate)
}
#also make a vector containing the readers numbers in order
readers <- c(1:15)

#now we make the new data set
readers.data <- data.frame(readers, book1, book2,book3,book4,book5,book6,book7,book8,book9,book10,book11,book12,book13,book14,book15
                                    ,book17,book18,book19,book20)

#this library contains the function knnImputation, which can fill in the missing ratings, using the process we discussed previously
install.packages("DMwR")
library(DMwR)

#this is the function that is called every time a users submites a new rating on our website
#it receives all the ratings as parameters (some may be null and this is fine)
getRecomendations <- function(book1, book2,book3,book4,book5,book6,book7,book8,book9,book10,book11,book12,book13,book14,book15
                              ,book16,book17,book18,book19,book20){
  
  #since we dont have a log in page, we dont know who the user is. For now we can jsut assume this ratings are comming from a new user
  #so we need to add this new user number to our rating matrix
  newUserId <- nrow(readers.data) + 1
  
  #we capture his ratings in this vector
  newEntry <- c(newUserId, book1, book2,book3,book4,book5,book6,book7,book8,book9,book10,book11,book12,book13,book14,book15
                ,book16,book17,book18,book19,book20)
                
  #then we add his ratings and umber to the existing matrix
  newMatrix <- rbind(readers.data,newEntry)
  
  #we fill in the gaps from missing ratings, as explained before using the function provided by the library
  completeMatrix <- knnImputation(newMatrix, k = 3, scale = T, meth = "weighAvg",
                                  distData = NULL)
                                  
  #capture the users ratings after we filled in the gaps 
  readersRatingComplete <- completeMatrix[completeMatrix$readers == newUserId,]
  #capture the books that the user has not rated
  readersRatingsMissing <- which(is.na(newEntry)) 
  
  #recomend books that the user has not rated and for which the predicted ratings are more than 3.4
  recommended <- integer()
  for (i in 2:20){
  if (i %in%  readersRatingsMissing){ 
    if(readersRatingComplete[i]>3.4){
      recommended <- c(recommended, readersRatingComplete[i])
    }
   }
  }
  
  #return the books that sutisfy that :)
  return( recommended)
  
}
  

```

# Recomendings items that are often bought together (mining item association rules)

We want to look at the transaction history and recomend items that users tend to buy together or over a short period of time. This approach to recomandation systems is called assosiation rules learning. Given that a user just bough an item we want to find out how likely he is to buy other 'assosiated' items and recomend the ones we are most confident that the user will be interested in.

To understand how this works, lets use the example where we want to find out if we should recomend book 2 to a user that just bought book 1 (only by looking at the transcation history of our shop). In other words, we want to find out if the assosiation rule of bying 2 given that we bough 1 is strong enough.

To measure the strenght of 'assisiation' between some items, we se use the following metrics:

* First we identify what proportion of all the transactions include both book 1 and book 2, this is called the SUPPORT of this rule.
A high support means that there are a lot of transations were both items have been bought together.

* This is some evidence that there is a strong relationship. However there is a chance that book 2 just happens to be very popular is found in various transactions, including the ones where book 1 is also bought. We want to be able to deal with such cases, this is what CONFIDENCE measures. 
We want to measure how often book 1 and 2 are sold together as opposed tohow often book 2 is sold. This is given by the ratio of the proportion of all transactions that included both 1,2 (support) over the proportion of transactions that includ book 2. It is nothing more than the conditional propability of 2 given 1.

* Last we want to find out how much the propability of a user buying book 2 has increased once he bought book 1. This is called LIFT. We know the original propability of bying book 2, is just the proportion of all transactions that include book 2. The propability of bying 2 given that we have just bought 1 is what we just measured above, the confidence. If we divide the original propability of buying 2 to the coditional propability of 2 given 1, we can measure what is called Lift. A hight lift measurment means that the liklyhood of the reader byuing 2 once he bought 1 has increaed.

For an assisiation rule to be considered we want to have a hight Support Confidence and Lift

We now have an understanding of how we measure 'assosiation' between products. But how do we go about selecting which products to measure that assosiation for, in the first place. Well one approach is to brute force it. Measure all the assosiations for every 2 possible pairs between  our products, then the 3 pairs, the 4 pairs untill we reach n pairs of products that are possible. From all those measurments we then pick the assosications with the hiest strenghts. This is clearly computationally expensive. 

There are various aproaches to this problem, a popular one, which we will use is the Apriori algorithm. This algorithm investigates assosiations incrementally, it starts from single variables, pairs of 2, 3 and so on. Every time it checks if the pairs sutisfy a minimum support value, the pairs that do are used to generate rules. It then moves to the next pairs, using only the items that were left out priviously. It repeats until no items are left, that can sutisfy that minimun support threshold.

```R 
#this package will help us impliment Apriori to mine assosiation rules
install.packages("arules")
library(arules)

#our first step is, as always, to load the data. I have prepared a transaction history of 43 transactions, in a normilized 'sigle' transactionId-item format.
#we want R to recognise the file as a transaction history, in order to be able to impliment Apriori with ease. There are two formats that R can work with for 
#transactions. One is single, the one we use (you can view the file for details), where each entry contains the transaction id and the item bought, if there are #multiple items bought in the same transaction, there whould be multiple entries with the same transaction id.
#The alternative is basket format, where each entry contains the id and all the products bought in that transaction in a single entry.
trans = read.transactions("~/Projects/statistical_analysis/recomendationSystems/transactionsNoUserName.txt", format = "single", sep = ",", cols = c(1,2))

#Now we can impliment Apriori, we can set the threshold for support and confidence as shown bellow 
assoc_rules <- apriori(trans, parameter = list(sup = 0.03, conf = 0.6,target="rules"));
#We can inspect rules that were found 
inspect(assoc_rules)


#last we only need to impliment those rules. This is the function that is called every time a user submits a book purchase.
#Since the user is only buying one book, we only only the rules that are relevnt to single item purchase.
useRules <- function(bookNumber){
  suggest <- integer()
  if (bookNumber == 4) {
    suggest <- 2
  }
  if (bookNumber == 16) {
    suggest <- c(17,18)
  }
  if (bookNumber == 12) {
    suggest <- 11
  }
  if (bookNumber == 19) {
    suggest <- 18
  }
  if (bookNumber ==3 ) {
    suggest <- 1
  }
  if (bookNumber == 1) {
    suggest <- 2
  }
  if (bookNumber == 17) {
    suggest <- 2
  }
  if (bookNumber == 18) {
    suggest <- 17
  }
  return(suggest)
}


```

#Further Discussions:

##Optimizations

Although our models are currently doing their job pretty good, they whould not scale well. For example, if the users increased from 15 to 100.000 and our books from 20 to 100.000.000.000 we whould be having a lot of trouble. Let's see why...

At the moment, our content based filtering algorithm requires all the attributes of the book that we just bough to be checked against all the attributes of all the books that are avalable. There chould be countless books, and in a more complex scenario the attributes we are comparing them to, whould not only be three (e.g. we might include book title, popularity, publicer, city and so on). Why might really need check every attribute of every book, but we might be able to reduce the presentage of books we want to compute the precice distance to. For example if a user just bought a book about aliens, is it really woth comparing that book to love stories? Chould we before applying the alorithm to every single book, filter out ones that we think are completly irelevant. Even better chould we have built some pre-difined group's of books that share similar attributes and use them as a guide. 

In the case of collaborative filering, at the moment every time a user is providing some ratings, we need to update the ratings' matrix and then re-fill the gaps, using the new information. That is very expensive and not maintanble, in a system where we are receiving handrets of rating per day. What we chould do instead is store the new ratings daily in some temporary storage and update the permenat rating matrix in batches.Moreover, if we have thousends of books, preding the rating each user may not be ideal, so we might consider some filtering here as well. 

Generally, a lot of effective predictive systems will choose to employ a compination of methods. For example, we might choose to claster books in certain groups according using content based filtering, those chould be very inclusive clusters (they whould still include multiple books). We chould then use collaborate filtering, within each cluster, to get more specific recomendations. This whould save a lot of cumputational overhead, as the clusters chould be pre-defined and the collaborate filetering whould require only a portion of the original data (as it whould use a rating matrix that rafers to books only within the cluster).
Another example whould be to use both collaborate filering and association rules on each case. We chould then compare the recomendations returned from both the chosen algorithms and only return the ones found in both. 


A last note is to try and use pre-built packages and algorithms when appropriate, as they have been optimized for performace. Make sure however, you understand how those algorithms function in order to be able to provide them with the most appropriate data, and correct parameters. Furthermore, you need to be able to make adjustments when required, interpret their outcomes and meausure their performance as objectively as possible. 

## Alternatives

Onother approach to ptimizations chould be looking at alternative algorithms that may be more 'accurate' or efficient or mayby just more appropriate for your case. As an example let's find out about some alternative algorithms on collaborate filtering.

Previously we used Euclidean method, through a library provided by R, to identify similar users through their ratings. There are other well-known methods for identifying 'similar' users, such us the Pearson method or Jaccard method. In the pearson method each series of rating for every user is treated as a vector. The difference between two vectors is their angle wich can be found by taking the cosine of their magnitudes.

In the Jaccard similarity, the values of the ratings do not matter. Similar user's are just users that have read the same books/clicked on the same websites/seen the same movies etcra, this has some clear drawbacks (just cause they read a book it doesnt nesesarly mean they liked it) but it does not explicitly require users' to have given out feedback.

Last you might want to consider taking the corilation distance (mentioned on content based filetring) for someting like ratings. In this case, assosiation is described by whether or not the users have rated the same itemes above or bellow their avarage.

Up to now we have discussed collaborate filtering by looking at similar users. There is however an alternative aproach, called Laten Factor Analysis.This method attempts to identify the underlaying factors that caused each user to rate each film the way the did, we then use those factors to extrapolate (predict) what their rating whould be for the rest of the films. 

To acheive this, we need to assume that there is a factor/combination of factors for each reader that makes them rate books a specific way. (e.g. a reader is into drama books). We also need to assume that there is a factor/combination of factors relevant to each book that result in bad/good rating (e.g. the author is very popular). 

Those underling factors can be identified using PCA. I have dedicated a privious chapter for PCA, but for now all you need to know is that we can somehow tell which are the main factors motivating reader to give sertain ratings and books to receive sertain ratings.
We do not, however, have the values associated with those factors. We only have the outcomes that were produced by combining the factors of each user with each product (those are the ratings!). So for every rating we know that rating = F_reader * F_book

Each user has its own set of equations, and all we need to do is solve for F_reader and F_book. The issue is, that not all the ratings are avalable for each user, meaning that we will not be able to find the exact numbers. There will always be an error,
In other words rating - (F_reader * F_book) will not be zero.

To estimates the factors in the best way we can, we will attempt to find some F_reader F_book, that minimizes the error. (There
are various techniques for that such us the stocastic gradient disent that attempts to find a local minimum by trial... )

Once we have those values we can simply use them to fill in any gap in the matrix. For example, lets say we didn't know the rating reader 1 gave for book 12. We had however anought data to calculate a factor 2 for that user and a factor 2 for that book. The rating whould be 4, and so we should recomend book 12 to reader 1. (of cource this is a very simplified example to discribe the underlying mechanics. In reality a lot more factors whould be involved and we whould also need to account for other issues such us overfitting in our equestions)

Apart from measuring our alternatives in terms of algorithms, we should also consider alternatives in terms of data. Aside from explicit ratings, costumer feedback takes on many forms such us comments in either the providor's platform or social media. It is possible, through sentimental analysis to capture the essence of those comments and use them to train our models and give more precise recommentdations. A lot of those methods look at the presence and frequency of words and attempt to classiffy comments as possitive or negative. For example the naive Bayer's algorithm will measure the propability of a comment been possitive and the propability of it been negative and decide on the one with the hiest propability. Those propabilities are assigned by compering the presence and frequency of certain words compared to their avarage frequency and the label(negative/positive) avalable from the training sets (past data, labeled by humans used to train the algorithm). There is a lot more to sentimental analysis and its value on recomendation systems. It can often be used to deduce additional features/attributes, in the case of books, we chould classify books depending on the frequency of sertain words (e.g. vampires, fairy, solders).

Last we should not exclude the option of classifing custemers themselfs. Although, it is generally prefered to adopt a product oriented aproach, as the product is less complicated and more well defined, we can always recomend products that were bought by users that share similar attributes (e.g. sex, age, interests).

# Conclusion
This is by no means a complete guide on recomendation systems but i hope you were able to understand the concepts and apriciate the potential advatages as well as the challenges around them.









