Usage
Similar to linear regression (part of the family of Generalized Linear Models). However it is part of a group of models called classifiers (tries to group elements/events) and this most often involves qualitative rather than quantitative data.  
Unlike linear regression, which is used mostly chosen for inference(studying the relationship between variables, effects
of each other, strength and direction), we usually use logistic regression for predicting (or sometimes studying) a
binary outcome (has only two outcomes, patient has diabetes or not, transaction is fraud or legit).  
The model does that by measuring the probability associated with each of the two and based on a chosen threshold will
decide on one (e.g if $p > 0.5$ a transaction is fraud).
It assigns those probabilities by taking into account the predictors we have provided and their approximated
coefficients as calculated from the data, that we have trained the model with.
For example location of the transaction, time or money.  

As with linear regression, quantitative data is handled by assigning a dummy variable boolean (e.g isFraud = 1,
isNotFraud = 0).
We are interested in finding how close a point is to $1$ (in other words its probability of success (fraud in this case)
plotted on the $y$ axis), as measured by the predictors (x, z ... axis).  
The issue is that we can get any value between $0$ and $1$, as well as values above and bellow that (e.g if the amount of
money used in a transaction is extremely large we might get something like p = 2.3 according to whatever coefficients
in $y = a + bx + cx + \ldots + e$ are calculated).
This is not as useful and sometimes does not make sense, that's why we use a transformation to fit our data exactly
between $0$ and $1$.

Formula
There are various transformation for that, in logistic regression we use the logistic function:
$P(x) = e^{(b0 + b1X)} \div (1 + e^{(b0 + b1X)})$, where $b0$ and $b1$ are our coefficients, this makes an S-shaped curve where
the edges are $0$ and $1$.
We want to try and bring this in a more linear form, after manipulation we can see that:
$P(x) \div (1 - P(x))=e^{(b0 + b1X)}$
$P(x) \div (1 - P(x))$, is called the the odds. It is the probability of success over the probability of failure. It is
used for example in horse-racing where if the odds are $\frac{1}{3}$ the horse is likely to win one race but loose $3$.
To make our equation more linear we can take the log. We notice that this is basically a linear equation using
the log of the odds on y-axis $log(P(x) \div (1-P(x))) = b0 + b1X$, this makes things a lot easier (see more about why
its helpful later on)

Best fit using maximum likelihood
Another issue is how we can find the 'best fit line', that best describes all our data points. In linear
regression we used the least squares approach. This required measuring the residuals (distance of our points to
the line) and trying to minimize that value by moving the line. However the distance of the points from a random
line (close to the real) in linear regression is somehow constant, the error has a constant standard deviation.
In logistic regression when a predictor reaches extreme points the distance will tend to $\pm \infty$. For
example if we try and fit a line for the probability of a transaction been fraud based on the money involved; a
line close to the best fit would have very large distance on the points where the amount of money involved in a
transaction is large, or where it is very little. For that reason the method of maximum likelihood is preferred

We use the following steps to do that:

1. Take  random straight line (a candidate 'best fit, just like in regression), that we think describes our points well.

2. Use the $P(x) =  e^{(b0 + b1X)} \div (1 + e^{(b0 + b1X)})$ (given above) to transform that street line to the S-shaped
curve that plots the probability from $0$ to $1$ for each of our $x$

3. We can now use those calculated $p(x)$ foe each of our $x$ to access how good this candidate line is. For each $x$
we can get the 'predicted' by the curve $y$ (its $P(x)$), which in this case is also called the 'likelihood'

4. We then calculate the likelihood for ALL of the success cases ($P(x)$) and unsuccessful cases ($1 - P(x)$) (e.g
probability, given by y value, of a fraud transaction actually been fraud, and probability, again as calculated by
our model of a legit transaction been not fraud). Whether a point is a success of not, we know from the data we
have already collected. To get the total, we just need to multiply everything. In a way this calculates 
'how likly we are' to predicting everything correctly, so the higher that is the better.

5. In reality we prefer to calculate the log of that probability instead. 'It just so happens' that the log of
the maximum of the log of the likelihood is the same as the maximum of the likelihood.
The reason why the log is favoured steams from the fact that it is easier to differentiate a product. When looking
for the max of an equation, we differentiate that equation and look for where it is equal to $0$. If we take the
log of the equation we turn a difficult differentiation of products to a addition ($log(a \times b)= log(a) + log(b)$), which
is easy to solve.

6. Finally, we choose the candidate line (which has our coefficients), that results in the higher log likelihood.
(Software like R does this very very easily using 'smart algorithms')

R-squared
....But if we can't calculate the residuals (distance of the predicted $y$ (from our line) to the true $y$ (from our points)
how do we calculate the R-squared and p-values. well this is not as simple as in linear regression and there are dozens
different ways to do it. An easy and common way, supported by R, is the MacFadden's Pseudo R-squared, this is very
similar to how its calculated in linear regression.

If we remember, R-squared is a value measuring how much of the variation in y is explained by our model, which indicates how well
it fits the data. To find that value we calculate the total variation not decribed by the line, by deviding a meause of the disctance of our points to the line to the distance of our points to the mean.The we take one minus the outcome. This basicly will show how much better our model is from just predicting using the mean/no predictors involved. So all we need to do is find a different way of calculating how fit the model is to the data and what the predictions whoud be if we used the mean instead. Well we already have the first, the log of the likelihood for our curve (LL(fit)), exacly explains how close our predictions are to the true y. We can get the propability of a case been successfull, whithout accounding for any predictors, simple by the definition of 
propabilities p(of success) = sucessfull cases/ total cases. Using that as the p(x) for ALL of our values we can can get the log of the likelihood as before (LL(overall)). We can then compare the two and see how much better our model is. Just like in linear regression the resulting R square will be from 0 to 1.

Suturated and Null Models
We know that the R-squared always takes values from 0 to 1. This helps us make objective conclusions on how well our model performs and compare the performance of different models. We know that it is given by 1 -(some measure of fitness of our model/some meausure of fitness of a 'bad' model), to show how much 'better' our model is. In linear regression our model's fitness is measured by the distance of the points to our line. This will only ever go as far as zero, since the perfect model will have zero distance. In linear regression we take the log of the liklihood using the product of all the propabilities, a propability will only ever go as far as 1 resulting in log(1)=0. What this means is that we had no need to provide an upper bound, all we needed was a lower bound, a meaure of 'bad' performance which was making prediction by just using a mean, since the perfect model whould always result having a zero as the nominator and giving us a R-squared = 1-0=1, a perfect fit.

However this is not always the case, in many other models the calculation of R-squared chould result in any number, and this whould not provide any clear indication of whether our model is doing well or not. This is why we need an upper bound. That upper bound is just a model that fits perfectly all our data points and is called the Saturated model. It requires all the data points as parameters to be able to make a model that perfectly fits them all and maximizes the likelihood. If you are wandering why dont we just use that model, if it mazimizes the liklihood, the answer is that it usually does not follow the format we are looking for. It is not genirized (it suffers from overfitting) and will not allow us to make any acurate predictions or study relationships. In a way, it is just a random line that crosses all the data points and does not really show any patterns, cannot be described by an algorithm and may result in unessesary overcomplications. The oposite of the satureted model is the Null model, a model that uses only one parameter (for example the mean in the linear regression), we already used that as the lower bound. The model that we are trying to measure the performance of by comparing it to the null and saturated is called the Proposed model.

The general form of R-squared can be discrebed by 1 - (a measure of bad fit - a measure of fit of our model/ (a measure of bad fit - a measure of perfect fit)), which will always result is 0 to 1, since NUll model is the minimum and Satureated the Maximum.
'it just so happened' that up to now the measure of perfect fit was zero so it chould be ommitted.

Residual and Null Devience
When we get the summary statistics og a logistic regression model, R outputs someting called Residual and Null Devience.
Those are nothing more that just more performance measurments for our model, which are derived by comparing how far our proposed model is from the Null (bad) model and how close it is to the Saturated (perfect) model. More specifically:
 >Residuall Deviance = 2*(log of likelihood of Saturated - log of likelihood of proposed)
 >Null deviance = 2*(log of likelihood of proposed - log of likelihood of null)
Those two output a chi-squre value that can be used to calculate a p value which indicates thether our proposed model is significantly far from the null and suturated model. We want those outputs to be as small as possible. When we try and optimize our model by adding and removing predictors we should take note of the effect on those values!!

P-values
For p values we can simply use the Chi-square distribution (see chapter Chi-square test). The chi -square value equals 
2(LL(fit)-LL(overall)) and the degrees of freedom are the difference in the parameters of LL(fit) and LL(overall). From those
you can calculate the aeria under the graph which will give the propability of getting such values randomly, if there is no corelation between your predictors and reaction. 

Demo in R
We will use the Titanic data to see if we can use the given attributes as predictors that determine whether or not a passanger
servived.

[//]: TODO: Convert R block below to useful Rmd chunks


```R

install.packages("titanic")
library(titanic)
data("titanic_train")

#set our data
data <- titanic_train

# replace missing values for Age with mean
data$Age[is.na(Test$Age)] = mean(Test$Age, na.rm = TRUE)

# Remove attributes that are not valuable
install.packages("dplyr")
library(dplyr)
data <- data %>% select(-c(Cabin, PassengerId, Ticket, Name))

# use factors for the following attributes
for (i in c("Survived","Pclass","Sex","Embarked")){
    data[,i] = as.factor(data[,i])
}

# creates a genral linear model (fimily = binomial specifies logistic regression)
gml.fit = glm(Survived~., data = data, family = binomial(link ='logit'))
# we can see our estimated coefficients and associated p-values
summary(gml.fit)


# getting the R-squared
install.packages("DescTools")
library('DescTools')
# you will notice this is relatively low, it not easy to predict the servivol of a passanger only given a few details about them
# a lot of random factors are involved. There are also a lot of limitations in our model.
PseudoR2(gml.fit)

# We notice that this is a multivariate logistic regression, that takes the form of: lof(p(X)/(1-p(X)) = b0 + b1x1+ b2x2 + ... + bnxn

################################################## SOME LIMITATIONS YOU SHOULD CONSIDER #############################################

##### Confounding

# We often use logistic regression (and other models) to study the effects of various varibles on an the occurance of an event.
# We will use the following examples:
# > how does smoking affect the propability of a patient developing lung dicesses
# > how does the fact that an individuall is a student affect their propability of defulting in the bank account 

# When we conduct such a study meauring how many smokers have developed lung desses versurs how many non smokers, and how many
# students have defaulted versus how many non-students, we will most propably find a positive relationship. We cannot however, 
# blindly trust the bivarate analysis, I mean unleast we managed to get a perfectly random and balanced across all ages, sexes,
# incomes...sample there is a good chanche that our results are partially the result of other variables masked under the ones we
# have chosen to study. For example studies have shown that there are more older people that are smokers, also older people tend 
# to be more prone to desesses. So when we only use smoker/non smoker part of why our coeefifnt for that predictor whould be so 
# hight is because smokers are also older people. This is called bias due to confounding and occurs when confounders ( such us age)
# are not included in our model. If we take the example of students, when we only use student/non-student to predict the propability 
# of someone defolting we will find a possitive corelation. However, studies have shown that students with the same income as 
# non-students are less likely to default. Here the confounder is income, it just so happens that students will tend to have lower
# income, which makes them prone to defaulting. Therefore not including this in our models chould result in biased predictions. 

# In the older days, where creating such models was more deficult due to computing power and software not been as readily avalable
# people used stratifying in order to reduce the effects of confunding. They whould have to separete their data in subsets (strata)
# that were free of confounders. For example split by age groups or by student/non student. THen they whould conduct the estimation
# of coefficinets separetly for each group and use techniques such us pooling or weighted avaraging (if it made sence) to get an 
# overall estimation of the coefficients.

# if you are interested to learn more abou this you can wach this video from a Harvard lecture
# https://www.youtube.com/watch?v=hNZVFMVKVlc

# For reference, if we were to stricly define what a counfouder is, it whould be a variable that sutisfies the 
# following conditions:
# > Confunders has to be related to one of the significant predictors (e.g older people tend to be smokers more than younger)
# > It has to able be independantly related to the reaction (e.g. age is also on its own a significant predictor of somone developing
# a desease)
# It is not part of the causuall pathway (e.g smoking does not cause you to be old)

##### Multicolinearity
# Logistic regression also assumes no or little multicolinearity among the predictors. (see previous chapter)

##### Interaction terms
# Exacly the same as with linear regression (see previous chapter). Including the product (combination) of certain predictors
# chould optimize the model.
# log(P(x) \div (1-P(x))) = b0 + b1X1 + b2x2 + b3x1x2

##### Heteroscadasity (not relevant)
# In logistic regression, we excpect a propability to be systematically further away from the predicted y, in other words 
# homoscedasticity is not an assumption in this case. This is why we are unable to use least squares to find our best fitting line
# on the first place, since the residuals (distance of our points to the line) are reaching -/+ infinity.


############################################ EXAMPLE 2 ###########################################################

# Let's use onether example. Predicting whether or not a patient is diabetic.
# Our sample data set contains attributes such us BMI, age and measurments on glucose concentration, as well as a boolean that
# indicates whether or not each paitient is diabetic. We hope to use those attributes as predictors for the patiants diabetic 
# condiction. This time we will use onother technique for measuring how well our model preforms in giving acurate predictions.
# It is a common technique used in various models and not just the logistic regression. Basicly we will split the data that we
# have in two, the training data and the testig data. Our training data will be 80% of the total and it willl be used to estimate
# the coefficients and draw the S-curve by assigning propabilities (in other words training our model). The test data will be then
# fed to the trained model (without the attribute that gives out if a patiant is diabetic) and our model will give a prediction for
# each of the row. That prediction will be according to a threshold we have assigned, for example we may choose that if the propability
# of a patiant been diabetit is p >= 0.5 then we predict they are diabetic. Since we actually know if the patiants in the test data
# are diabetic or not, we can compare the actual to the predicted outcomes and get a % accuracy. We will see how this is done
# in details, as well as how we can optimize the chosen threshold for our specific problem.


#we can use a csv that is avalable online for our data, we can load this in R using the following code
data <- read.csv(file = "http://www2.compute.dtu.dk/courses/02819/pima.csv", head = TRUE, sep = ",")
############## SPLITING THE DATA#################################################################################################

# first we need to split our data, we do this with the use of caTools library (there are other ways too) ##
# the following creates an array of TRUE/FALSE values for every row in our data set,
# where TRUE is 80% of the data. It does this 'randomly', later on we will see techniques for spreading the success and failure
# rates proportionally across training and testing data
install.packages("caTools")
library(caTools)
split <- sample.split(data, SplitRatio =  0.8)
split
#transform to factors
data$type <- af.factor(data$type)
#we then split the data using that array, where TRUE is for the training and FALSE is for the testing
trainingData <- subset(data, split == TRUE)
testingData <- subset(data, split == FALSE)

#create the model using the TRAINING data
model <- glm(type ~., trainingData, family = binomial(link = 'logit'))
# review our model
summary(model)
#as expected BMI ang glu are significant (you can try and remove the predictors with a larger p-value but do this intrementally and note 
#the effect it has on the null and deviance residuals (see above) )

########lets visulize
#we will use only glu to plot our grapths easier
gml.fitGlu = glm(type ~ glu, data = trainingData, family = binomial(link ='logit'))
summary(gml.fitGlu) 
#first we will plot the best fiting line of propability of survivol over age 
#we need to get the propabilities for each row 
p <- predict(gml.fitGlu, trainingData, type = "response")
# get the lof of the odds, which is our y
y <- log(p/(1 - p))
# substitude with our calculated coefficients for the x axis
x <- -5.940285 + 0.042616  * trainingData$glu 
plot(x, y)
#Now let's transform it to the S-shaped curve with y from 0 to 1
#this is just the logistic function where i have substituted the coefficient for the intercept for our x with the ones
#calculates from our model summary
f <- function(x) {
  1/(1 + exp( -5.940285  + (0.042616  * x )))}
x <- trainingData$glu
#this scales the grapth
x <- -500 : 700
plot(x, f(x), type = "l")
#################

########################### CONFUSION MATRIX####################################################################################
# The predict() function allows us to feed new data in the trained model and receive a propability of each patiant on that data,
# been diabetic, we will use this on our testing data. 
# type = response outpouts the propability of each patiant been diabetic, we save the array of propabilities on pdata
pdata <- predict(model, testingData, type = "response")

# We will use those propabilities to predict whether the patiant is diabetic or not according to our model,
# given a threshold (p>0.55 in this case) and then compare the outcomes to the real values. We already know if those patiants
# are acually diabetic. We will compare the predicted to the actuall values using a confusion matrix.

#we will use the Caret library for that
install.packages("caret")
library(caret)


#create two arrays that contain TRUE or FALSE for each row (is diabatic TRUE, is not diabetic FALSE)
# one for the predicted data and one for the actuall, in order to compare them in R they need to be type factors (enumarations) and 
# have of the same levels (take on the same values)

#for the predicted TRUE is a row of propability > 0.55
predicted <- as.factor(pdata > 0.55)
#for the actual data, thether or not a patiant is diabetic is given by yes or no, so true is yes
real <- as.factor(testingData$type == "Yes")

# use caret to compute a confusion matrix
install.packages("e1071")
cm <- confusionMatrix(data = predicted, reference = real)
cm
# We can see that this has outputed a table decribing the following in each cell:

# predicted true that are actally true, also refered to as True Positive (TP)
# predicted true that are actually false, also refered to as False Positive (FP)
# predicted false that are actually false, also refered to as False Negative (FN)
# predicted false that are actually true, also refered to as True Negative (TN)

############### INTERPRETING ACCURACY
# It has also outputed some metrics on the performance of our model, lets explain them:

# > Sensitivity (also known as true positive, recall or hit rate) measures how well your model does in predicting positive values
# (predicting that the patient is diabet when they actually are): given by TP/(TP+FN)

# > Specificifity (also known as true negative rate) measures how well your model does in predicting negative values,
# (predicting that the patient is not diabet when they actually aren't): given by TN/(TN+FP)

# > Accuraccy gives an overall performance measurment on the accuracy of the results on a scale of 0 to 1, with 1 meaning
# that the model predicted everything correctly. It is given by: (TP + TN)/ (TP + FP + FN + TN)

# There is a big problem with using this metric as a performace indicator. Let's use an example where we create a model
# that simple always outputs FASLE. We use that model to predict whether or not an idividual has a very rare desease that is 
# estimated only 1 on 100 people have. If we use that model on a very large sample of people (that are already diagnosed 
# and know thether or not they have the desease) and then see how our model did, we will measure something like a 
# 0.99 acuracy. That is amazing right, our model is "99% accurate" ! This illustrates how this metric is biased by
# what proportions of possitive and negative values are avalable, a model chould do well simple by chance like we show in our
# example.

############ The kappa coefficient
# Cohen's Kappa coefficient is a mectric that tries to tackle the problem of bias in the measurment of accuraccy.
# It does so by 'eliminating' the chance of randomly getting a correct prediction from the equation.

# this 'chance' is just given by propability of randomly selecting a true value from the sample plus the propability of randomly
# selecting a false from the sample, since we want to measure both the chance of predicting TRUE when TRUE and FALSE when FALSE.
# In the sample (confusion matrix), we chould select a fasle value either from the total predicted false or the total actual false.
# So we need to multipy the propabilities assosciated with both. Similarly we chould select a positive value either from the total
# predicted possitive or the total acually possitive.

#from our confusion matrix table we can get:
# FN + FP
totalFALSE <- 47 + 15
# TP +TN
totalTRUE <-  5 + 7
# NT + NF 
totalNegative <- 47 + 5
# PT + PF
totalPositive <- 15 + 7 
# everything
totalSAMPLE <- 47 + 5 + 15 + 7

P_of_true_from_actuall = (totalTRUE/totalSAMPLE)
P_of_true_from_predicted = (totalPositive/totalSAMPLE)
P_of_false_from_actuall = (totalFALSE/totalSAMPLE)
P_of_false_from_predicted = (totalNegative/totalSAMPLE)

P_of_chance_for_TRUE <- P_of_true_from_actuall * P_of_true_from_predicted
P_of_chance_for_FALSE <- P_of_false_from_actuall * P_of_false_from_predicted
P_of_chance <- P_of_chance_for_TRUE + P_of_chance_for_FALSE
# We can see that our P_of_chance is very hight, this is because we have more samples of FALSE, there are more people that 
# are NOT diagnosed with diabities
P_of_chance

#to get the Kappa coefficient we whould use the fallowing formula...

# first we need to select accuracy from our confusion matric metrics
accuracy <- cm$overall['Accuracy'][[1]][1]
#Kappa is given by:
Kappa <- (accuracy - P_of_chance)/(1-P_of_chance)
#we can see this is the same value given by R in the confusion matrix
Kappa
#So how do we interpret Kappa, this is a bit vague and there is no clear unswer but we can see some examples
# 1 means our model is prefect
# 0 means it as good as chance
# any negative value means it does worse than chance
# anything close to 0.3 and above is relativly good
# values of .8 and above are extremely rare
########################################################################################################################

##################################### OPTIMIZING THE THRESHOLD #########################################################
# changing the threashold will affect our models sensitivity and specificity. 
# If we had a very low therehold where p is close to 0, then almost always we whould predict that a pacient was diabetic. 
# This means that we whould predict correctly TRUE when TRUE, but also TRUE when FALSE, in other words we whould have 
# a very high true possitive rate.
# If we had a very high threshold, close to p=1, then almost always we whould predict that a patiant is not diabetic.
# This means we whould predict FALSE when FALSE, but also FALSE when TRUE,
# in other words we whould have a very high false positive rate.

# usually we want to balance this and we wnd up somewhere in the midle. However, we need to apply domain knowledge and think about
# thether we care more about true positive rate or false positive rate. For example what is the effects of:
# predicting that a patiant is diabetic when they aren't (having more FP), refered to as type I errors
# versus
# predicting that a patient is not diabetic when they are (having more TN), refered to as type II errors
# Usually in such cases we whould be more concered for the type 2 errors, since failing to diagnose a diabities can have a greater 
# inpact on the patients health, so we might choose a p somewhere lower that 0.5 mayby 0.4

# We can visualize how changing our threshold affects the true and false positive rate to help us choose
# the right p
install.packages("ROCR")
library(ROCR)

resForTraining <- predict(model, trainingData, type = "response")
#choosing a value
ROCRPred = prediction(resForTraining,trainingData$type)
ROCRPref <- performance(ROCRPred, "tpr", "fpr")
# we want something on the green region between 0.4 and 0.6, but closer to 0.4 to ensure less type 2 errors
plot(ROCRPref, colorize = TRUE)














```
