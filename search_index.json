[
["index.html", "Machine Learning Algorithms Guide 1 Introduction", " Machine Learning Algorithms Guide Sofia Kyriazidi 2018-12-19 11:15:30 1 Introduction In this guide we will analyse some of the most commonly used and powerful machine learning algorithms. We will walk through the intuition behind each algorithm, the required mathematical background, as well as its implementation in R, in a step by step approach. We will discuss and implement optimisation techniques, explain use cases, measure their performance and understand how to interpret and use their outcomes. On top of that, the guide is accompanied by live applications that make use the algorithms to help you understand their value, advantages and drawbacks of various approaches. This guide does not require any prior knowledge, although some basic mathematical and programming background would make things easier. For those that need to occasionally refresh their statistics, there is a chapter devoted to that. Chapters coming up up soon: General Additive Models (GAMs) Bayer’s for Classification Other classification Algorithms (k-means and knn) Deception Trees Support-Vector Machines Boosting For any queries contact at: sophia.97@windowslive.com Credits should be given to to the book ‘An introduction to Statistical Learning’ written by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, which helped a lot in my learning and origination of this project. We will be referring to this book and other resources thought the guide. "],
["linear-regression.html", "2 Linear Regression 2.1 Creating the Model 2.2 R-squared 2.3 Confidence Intervals 2.4 Prediction Intervals 2.5 Heteroscedasticity 2.6 Outliers 2.7 Multicollinearity 2.8 Interaction terms 2.9 Non-linear Transformations of Predictors 2.10 Qualitative Predictors", " 2 Linear Regression 2.1 Creating the Model Linear regression is useful for studying the the effect of predictors (dependant variable, \\(x\\) values) to a reaction (independent variable, \\(y\\) value). This is called interference, the coefficient of each predictor will show the strength and direction of that predictor (how much of an effect it has and whether it is a positive or negative relationship). It may also be used for predicting the \\(y\\) for specified \\(x\\) values. It is of the form: \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\epsilon\\), where \\(\\beta_0\\) is the intercept to the \\(y\\) axis, \\(\\beta_1\\) to \\(\\beta_n\\) are the coefficients, \\(x\\) are the predictors and \\(\\epsilon\\) is some error. We gather a sample of data which we use to estimate our coefficients. The coefficients are estimated using the least squared method. We plot a line of the form \\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\epsilon\\), that minimises the distance of our true \\(y\\) values for each \\(x\\) from the predicted by that line \\(y\\) (basically the line that is on average closer to each data point). That line represent the mean/expected value for each \\(x\\). # We will create a linear model in this example # Libraries install.packages(&quot;MASS&quot;) install.packages(&quot;ISLR&quot;) We will use the Boston dataset, it contains medv, a value for the mean house value and various other attributes such as rm rooms in the house and age of the house, we want to make a linear regression model that uses those values as predictors to predict the response, value of the house. library(MASS) library(ISLR) You can show the help page for the Boston dataset. ?Boston Not all the available attributes always make for good predictors. For example we might have something like the first name of the landlord which does not change the price as much. We say that those attributes will not have a significant coefficient; their coefficient will be close to zero. To identify the predictors that can give us the best price prediction we can start by bringing in one by one our attributes and comparing performance (forward approach), or starting with all and removing one by one (backward), or even a mix. # We will start with the first approach and analyse our model # We are bringing in rm (rooms) # This creates a linear model for medv as a function of rm lm.rm_fit &lt;- lm(medv~rm, data = Boston) # This will give us the basic information of the model, such as the coefficient # of our predictors and the intercept summary(lm.rm_fit) ## ## Call: ## lm(formula = medv ~ rm, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.346 -2.547 0.090 2.986 39.433 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -34.671 2.650 -13.08 &lt;2e-16 *** ## rm 9.102 0.419 21.72 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.616 on 504 degrees of freedom ## Multiple R-squared: 0.4835, Adjusted R-squared: 0.4825 ## F-statistic: 471.8 on 1 and 504 DF, p-value: &lt; 2.2e-16 We can see that rm is of high significance, it has a very small p-value (this means there is only a very small probability that the \\(H_0\\) (see the hypothesis testing chapter if you’re unfamiliar with the null hypothesis), rm has no effect on price is true). We also get the intercept and coefficient of the fit line so we can see that the approximated line is of the form: \\(y = 9x - 35\\). 2.2 R-squared We can use the R-square value (coefficient of determination), as a performance indicator. It measures the percentage of variance of our points, that can be described by the regression line. To find that value we calculate the total variation not described by the line: by dividing a measure of the distance of our points to the line, to a measure of the distance of our points from the mean. The rest of the variation will be described by our model so we can simple take 1 minus the result. This basically will show how much better our model is from just predicting using the mean. # We can get the measure of the disctance of our points to the line by # calculating the sum of squared error. # That is how far our points are from the line (how far the true y for each x is # from estimated by the line y): # Since we previously calculated our coefficients for the line we can use the # following to get the y estimated by the line. yForX &lt;- function(x){ 9 * x - 35 } # (We could also use the built in function &#39;predict&#39; but this makes it simpler # for now.) # The squared error would be their squared sum, where Boston$medv is the actual # y. SELine &lt;- sum((Boston$medv - yForX(Boston$rm)) ^ 2) SELine ## [1] 22541.65 # The measure of the distance of our points from the mean is given by the # squared difference of each y from the mean of y. meanOfY &lt;- mean(Boston$medv) SEy &lt;- sum((Boston$medv - meanOfY) ^ 2) SEy ## [1] 42716.3 # Now we can calculate the percentage of the total variation not described by # our line. DescribedByLine &lt;- SELine / SEy DescribedByLine ## [1] 0.5277061 # So the R-square, the percentage not described would be: rSquared &lt;- 1 - DescribedByLine # We can easily extract this calculation with R using the following: summary(lm.rm_fit)$r.squared ## [1] 0.4835255 If about \\(40\\%\\) of the mean is described by the rm, the rest is various other predictors that we have not accounted for and the fact that the actual relation may not exactly be linear. This is described partially by (the irreducible error) where \\(y\\) (the house value) \\(= f(x)\\) (some function that described it) \\(+ \\epsilon\\) (some random error). # To check how linear the actual relation is we can plot the graph plot( Boston$medv, Boston$rm, xlab = &quot;rm&quot;, ylab = &quot;medv&quot; ) # So if we bring more variable we expect this to increase # This is a multi-variant regression which would like: # y = b1*x1 + b2*x2 + b3*x3 ... + bn*xn + c lm.fitAll &lt;- lm(medv~., data = Boston) # We can see that the model now described about 74% of the variation, using a # couple more significant coefficients, such as crime in the area, taxes and so # on. summary(lm.fitAll) ## ## Call: ## lm(formula = medv ~ ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.595 -2.730 -0.518 1.777 26.199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** ## crim -1.080e-01 3.286e-02 -3.287 0.001087 ** ## zn 4.642e-02 1.373e-02 3.382 0.000778 *** ## indus 2.056e-02 6.150e-02 0.334 0.738288 ## chas 2.687e+00 8.616e-01 3.118 0.001925 ** ## nox -1.777e+01 3.820e+00 -4.651 4.25e-06 *** ## rm 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** ## age 6.922e-04 1.321e-02 0.052 0.958229 ## dis -1.476e+00 1.995e-01 -7.398 6.01e-13 *** ## rad 3.060e-01 6.635e-02 4.613 5.07e-06 *** ## tax -1.233e-02 3.760e-03 -3.280 0.001112 ** ## ptratio -9.527e-01 1.308e-01 -7.283 1.31e-12 *** ## black 9.312e-03 2.686e-03 3.467 0.000573 *** ## lstat -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.745 on 492 degrees of freedom ## Multiple R-squared: 0.7406, Adjusted R-squared: 0.7338 ## F-statistic: 108.1 on 13 and 492 DF, p-value: &lt; 2.2e-16 We should note that a multivariate model is far superior than regressing each predictor with the price using individual models, since a change in price could be the result of other predictors or combinations of predictors. Separate models could result in inaccurate calculations of significant coefficients. 2.3 Confidence Intervals They help us find a range within which the true regression line of the population would be. This is because we have used a SAMPLE to approximate the true coefficients and draw our regression line. Doing that creates some error, our true mean is \\(\\pm\\) that error, this is called the confidence interval. It is important to get the confidence interval for each of our estimated coefficients in order to determine how significant their true values are (how far from zero). # The intercept and coefficient are approximated using a sample of nrow(Boston) # samples. We cannot say that those values equal the true ones. There is an # error in using a sample to draw conclusions, called the standard error. The # true value would be our coefficient +/- the error, this is given by the # confidence interval. # In R we can get this by: confint(lm.rm_fit) ## 2.5 % 97.5 % ## (Intercept) -39.876641 -29.464601 ## rm 8.278855 9.925363 # Lets see how this is calculated for the coefficient of rm. # First we need to calculate the residual standard error. # (Residual means, what is left after we are done explaining the value of a # point using our regression line, so the error.) # It measures how far our actual values are from the regression line so it is # given by: the square root of the sum of all the differences of the estimated y # values to the actual y values squared, divided by total values minus 1, lets # apply this: standError &lt;- sqrt(sum((yForX(Boston$rm) - Boston$medv) ^ 2) / (nrow(Boston) - 1)) # We can see this value matches the one given by R on the summery statistics of # our model. standError ## [1] 6.681088 # In order to get the specific standard error due to the rm coefficient we need # to divide by the difference due to x for rm. xFory &lt;- function(y){ (y + 35) / 9 } # Given by this formula COSE &lt;- standError / (sqrt(sum((xFory(Boston$medv) - Boston$rm) ^ 2))) # This matches the value calculated from R by: coef(summary(lm.rm_fit))[, &quot;Std. Error&quot;] ## (Intercept) rm ## 2.6498030 0.4190266 # Knowing that we can construct a confidence interval, we will use a 97.5% to # match the one used by R for the coefficient true value. This would be our # value 9 +/- a critical value (driven by the confidence interval selected) # multiplied by the standard error. # We use a t value for our critical value since we do not know standard # deviation, our error is an estimate, the t value can be found in a table, or # using software. To find it we need to know the degrees of freedom (total rows # -2) and our selected confidence interval. t &lt;- qt(0.975, df = nrow(Boston) - 2) t ## [1] 1.964682 # So the confidence interval would be: # get the calculated coefficient coef &lt;- as.numeric(summary(lm.rm_fit)$coefficients[2, 1]) coef ## [1] 9.102109 left &lt;- coef + t * COSE left ## [1] 9.888954 right &lt;- coef - t * COSE right ## [1] 8.315264 # Left and right matches the confidence intervals given by R confint(lm.rm_fit) ## 2.5 % 97.5 % ## (Intercept) -39.876641 -29.464601 ## rm 8.278855 9.925363 2.4 Prediction Intervals Those are intervals for an INDIVIDUAL point, unlike confidence intervals that are used for the mean/expected value (of all \\(y\\)’s that have that \\(x\\)). It tries to answer, what is the particular value of \\(y\\) given some \\(x\\). They are useful when predicting a new \\(y\\), from given \\(x\\) values. They have wider range than confidence intervals. That is because not only do they depend on the mean error but also the individual random \\(\\epsilon\\) error \\(y = f(x) + \\epsilon\\). # We can see that the prediction intervals are wider when comparing them in R predict(lm.rm_fit, newdata = list(rm = 8), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 38.14625 36.62041 39.67209 predict(lm.rm_fit, newdata = list(rm = 8), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 38.14625 25.05835 51.23415 To calculate the prediction we use a similar approach to confidence intervals, however this time the error is both dependant on the standard error (variation of the mean) and the error of each individual point due to \\(\\epsilon\\) (the fact that our model is not a perfect fit for the truth). in other words, our mean (expected value of \\(y\\) for an \\(x\\) given by the regression) is not accurate and our point is not guaranteed to be exactly the same as the mean), this error is given by adding those to up. # We already have the standard error, so we can find the total error relevant to # our specific x point given by SEpedY. # We will use the example where rm = 8: rooms &lt;- as.numeric(Boston$rm) SEpedY &lt;- sqrt(standError ^ 2 * (1 + (1 / nrow(Boston)) + ((8 - mean(rooms)) ^ 2) / sum((rooms - mean(rooms)) ^ 2))) SEpedY ## [1] 6.72696 # We can now get the prediction intervals for rm = 2 by: PredictLeft &lt;- yForX(8) - t * SEpedY PredictLeft ## [1] 23.78366 PredictRight &lt;- yForX(8) + t * SEpedY PredictRight ## [1] 50.21634 Left and right matches the prediction intervals given by R 2.4.1 we can plot prediction and confidence intervals install.packages(&quot;ggplot2&quot;) library(ggplot2) ggplot(Boston, aes(x = rm, y = medv)) + geom_point() + geom_smooth(method = lm, se = TRUE) temp_var &lt;- predict(lm.rm_fit, interval = &quot;prediction&quot;) new_df &lt;- cbind(Boston, temp_var) ggplot(new_df, aes(rm, medv)) + geom_point() + geom_line(aes(y = lwr), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + geom_line(aes(y = upr), color = &quot;red&quot;, linetype = &quot;dashed&quot;) + geom_smooth(method = lm, se = TRUE) The shaded area shows where the true regression line of the population would be with a \\(95\\%\\) confidence. The fact that it is wider on the ages just means that the standard error for the intercept is higher than that of the coefficient of our \\(x\\) (rm). If we imagine moving the line along the shaded area we notice that the intercept changes more than the slope. The line represents the mean values of \\(y\\) for every \\(x\\), however the particular \\(x\\) we are studying may not exactly fall into the mean. This is where the dotted red lines are useful. \\(95\\%\\) of the values will fall within the dotted lines (prediction interval). 2.5 Heteroscedasticity One important assumption we have taken when calculating confidence and prediction intervals is that the variance of the errors is constant (equal scatter of the data points). However the variance may change as the response (\\(y\\)) is changing. We have heteroscedasticity, when that change is systematic, follows a pattern. Typically, it produces a distinctive fan or cone shape in residual plots. It can also impact the accuracy of the coefficients. # For this example we will use the cars dataset instead, as our data does not # have clear indications of heteroscedasticity not enough variance to justify # changes in data. install.packages(&quot;Ecdat&quot;) library(Ecdat) # Load our data set data(&quot;Bwages&quot;, package = &quot;Ecdat&quot;) ?Bwages lm.het_fit &lt;- lm(wage ~ school, data = Males) # We can use the following visualizations # Scatterplot # We can see a cone shape which may indicate heteroscedasticity. plot(Bwages$edu, Bwages$wage) # Residuals/fitted is of interest par(mfrow = c(2, 2)) plot(lm.het_fit) # It indicates heteroscedasticity, as there is a curve, the X values do not look # random. # Breusch-Pagan test for a more algorithmic approach install.packages(&quot;lmtest&quot;) library(lmtest) ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric # p value is very small so we can reject the H0, there is heteroscedasticity bptest(lm.het_fit) ## ## studentized Breusch-Pagan test ## ## data: lm.het_fit ## BP = 4.3708, df = 1, p-value = 0.03656 So what do we do? We can try and identify why this is happening, why the variance increase with the education and include this change in our model in order to optimise it. From domain knowledge we can say that individuals with higher education, have more choice in what to do. For example they may prioritise making money or having a more relaxed job. So we can calculate the rate at which it increases and add this as a coefficient of x that describes the variance: for \\(y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i\\) (where \\(\\epsilon_i\\) is the error) \\(Var(\\epsilon_i|x_i) = \\sigma^2 \\times kx_i\\), and not just the standard deviation \\(\\sigma^2\\), this is called weighted regression/least squares WLS. The difference in WLS from OLS (ordinary least squares) is that how much a point can affect the position of the best fit line is not equal, it is dependant on the variance associated with the particular \\(x\\) if it is a point of high variance it will affect the line less, if it is of low variance it will have a greater affect. As a result we get a better model. Other approaches where the heteroscedasticity is less systematic we can try data transformations such as \\(\\log\\), or \\(\\sqrt{}\\) or even a Box-Cox transformation. However, we have to keep in mind how this affects our original model. 2.6 Outliers Points that are far from the predicted \\(y\\) values (maybe due to errors in taking the measurements or very extreme/special cases) they may not always massively affect the best fit line, however they could have a significant effect in the calculations of errors because they are very far from the predicted \\(y\\), the R-squared value would decrease if we had more information about the data and how it was gathered, or maybe additional domain knowledge to interpret the outcomes, we could potentially just remove them or adjust them towards the mean. # We can see a few of them in the plot plot(Boston$rm, Boston$medv) abline(lm.rm_fit, col = &quot;red&quot;) # Simple way to find outliers on a set of a single variable is the univariate # approach. # Outliers are defined as values below or above 1.5*IQR (Inter Quartile Range = # Q3 - Q1). # We can visualise that in a boxplot where everything out of the whiskers is # treated as an outlier. boxplot(Boston$rm, horizontal = TRUE) # We can list them using the following code outlier_values &lt;- boxplot.stats(Boston$rm)$out There are also a few multi-variant approaches in defining outliers. For example the Cooks Distance which measures the influence of a row on the sample to the line. In general, points that have about 4 times the mean may be classified as influential. Those are also called leverage points, they may not affect the R-square value as much as other outlier but affect the fit of the regression line. # We can calculate them in R using the following code, for 2 or more variables cooksd &lt;- cooks.distance(lm.rm_fit) influential &lt;- as.numeric(names(cooksd)[(cooksd &gt; 4 * mean(cooksd, na.rm = T))]) BostonSubSet &lt;- c(Boston$rm, Boston$medv) BostonSubSet[influential] ## [1] 7.489 7.802 8.375 7.929 7.831 7.875 7.853 8.034 7.686 8.297 7.923 ## [12] 8.780 3.561 3.863 4.970 6.683 7.016 6.216 5.875 4.138 7.313 6.968 ## [23] 4.138 4.628 7.393 2.7 Multicollinearity When dealing with multivariate regression, some of the values may describe each other. For example, if in our case we were to add house condition and house age as two separate factors, we might find that there is some relation between age and condition (we are unsure how strong that relation is), but in some cases we might need to consider whether or not we should include both the two (or three or more) variables in our regression. This could have a negative affect on the stability of our results. There will be issues in identifying the effect of each individual predictor, and changes in \\(x\\) would result in greater (than the true) changes in \\(y\\) since the same effect is now accounted in multiple predictors. 2.7.1 Correlation # We can first look at some bivariate approaches (measuring the correlation of # each pair of two predictors) the correlation factor is a simple way of doing # this, a value from -1 to 1 showing direction and strength of the relationship. ?cor # Identical so cor = 1, positive linear relationship cor(1:5, 1:5) ## [1] 1 # Negative cor = -1 cor(1:5, 5:1) ## [1] -1 # Less correlation cor &lt; 1 cor(1:5, c(1, 2, 3, 4, 4)) ## [1] 0.9701425 # Lets use the example of tax and criminal activity # Variables need to be numerical to be compared typeof(Boston$crim) ## [1] &quot;double&quot; typeof(Boston$tax) ## [1] &quot;double&quot; cor(Boston$crim, Boston$tax) ## [1] 0.5827643 # This is calculated using the following formula x &lt;- Boston$crim y &lt;- Boston$tax meanX &lt;- mean(x) meanY &lt;- mean(y) nominator &lt;- sum((x - meanX) * (y - meanY)) denominator &lt;- sum((x - meanX) ^ 2) * sum((y - meanY) ^ 2) r &lt;- nominator / sqrt(denominator) # We can see this matches exactly with the value given by cor() function r ## [1] 0.5827643 # A few things to note about the formula: # The equation is bound by 1 # If there is a positive correlation where xi &gt; meanX and yi &gt; meanY then, form # the formula we can see that we will have some positive outcome, the other way # around we will get a negative value. If there is no relationship, the x is not # described by the y then the sum of (x - meanX) will approximately cancel out # with the sum of (y - meanY) and we will get a very small value. 2.7.2 Sample Correlation Coefficient to True Value # We have calculated a the correlation coefficient of a sample, we need to test # if that is enough evidence to prove that the true correlation coefficient is # far from zero (Ha). # In R we can easily complete a two tail test on our Hypothesis cor.test(Boston$crim, Boston$tax) ## ## Pearson&#39;s product-moment correlation ## ## data: Boston$crim and Boston$tax ## t = 16.099, df = 504, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.5221186 0.6375464 ## sample estimates: ## cor ## 0.5827643 # What the cor.test does is it calculates a p-value for the cor coefficient we # have just calculated, to show what would the probability of getting such a # number at random (if the H0 was true and there is no correlation) if that # probability is very small usually less than 0.05 we can reject the H0. # We can calculate it using the following steps (see project hypothesis testing # for the theory): t &lt;- (r * sqrt(nrow(Boston) - 2)) / sqrt(1 - r ^ 2) # Area &gt; t and Area &lt; t p &lt;- pt(q = t / 2, df = (nrow(Boston) - 1), lower.tail = FALSE) + pt(q = -t / 2, df = (nrow(Boston) - 1), lower.tail = TRUE) 2.7.3 Correlation Matrix # To get the complete correlation matrix for (all the pairs) cor(Boston) ## crim zn indus chas nox ## crim 1.00000000 -0.20046922 0.40658341 -0.055891582 0.42097171 ## zn -0.20046922 1.00000000 -0.53382819 -0.042696719 -0.51660371 ## indus 0.40658341 -0.53382819 1.00000000 0.062938027 0.76365145 ## chas -0.05589158 -0.04269672 0.06293803 1.000000000 0.09120281 ## nox 0.42097171 -0.51660371 0.76365145 0.091202807 1.00000000 ## rm -0.21924670 0.31199059 -0.39167585 0.091251225 -0.30218819 ## age 0.35273425 -0.56953734 0.64477851 0.086517774 0.73147010 ## dis -0.37967009 0.66440822 -0.70802699 -0.099175780 -0.76923011 ## rad 0.62550515 -0.31194783 0.59512927 -0.007368241 0.61144056 ## tax 0.58276431 -0.31456332 0.72076018 -0.035586518 0.66802320 ## ptratio 0.28994558 -0.39167855 0.38324756 -0.121515174 0.18893268 ## black -0.38506394 0.17552032 -0.35697654 0.048788485 -0.38005064 ## lstat 0.45562148 -0.41299457 0.60379972 -0.053929298 0.59087892 ## medv -0.38830461 0.36044534 -0.48372516 0.175260177 -0.42732077 ## rm age dis rad tax ## crim -0.21924670 0.35273425 -0.37967009 0.625505145 0.58276431 ## zn 0.31199059 -0.56953734 0.66440822 -0.311947826 -0.31456332 ## indus -0.39167585 0.64477851 -0.70802699 0.595129275 0.72076018 ## chas 0.09125123 0.08651777 -0.09917578 -0.007368241 -0.03558652 ## nox -0.30218819 0.73147010 -0.76923011 0.611440563 0.66802320 ## rm 1.00000000 -0.24026493 0.20524621 -0.209846668 -0.29204783 ## age -0.24026493 1.00000000 -0.74788054 0.456022452 0.50645559 ## dis 0.20524621 -0.74788054 1.00000000 -0.494587930 -0.53443158 ## rad -0.20984667 0.45602245 -0.49458793 1.000000000 0.91022819 ## tax -0.29204783 0.50645559 -0.53443158 0.910228189 1.00000000 ## ptratio -0.35550149 0.26151501 -0.23247054 0.464741179 0.46085304 ## black 0.12806864 -0.27353398 0.29151167 -0.444412816 -0.44180801 ## lstat -0.61380827 0.60233853 -0.49699583 0.488676335 0.54399341 ## medv 0.69535995 -0.37695457 0.24992873 -0.381626231 -0.46853593 ## ptratio black lstat medv ## crim 0.2899456 -0.38506394 0.4556215 -0.3883046 ## zn -0.3916785 0.17552032 -0.4129946 0.3604453 ## indus 0.3832476 -0.35697654 0.6037997 -0.4837252 ## chas -0.1215152 0.04878848 -0.0539293 0.1752602 ## nox 0.1889327 -0.38005064 0.5908789 -0.4273208 ## rm -0.3555015 0.12806864 -0.6138083 0.6953599 ## age 0.2615150 -0.27353398 0.6023385 -0.3769546 ## dis -0.2324705 0.29151167 -0.4969958 0.2499287 ## rad 0.4647412 -0.44441282 0.4886763 -0.3816262 ## tax 0.4608530 -0.44180801 0.5439934 -0.4685359 ## ptratio 1.0000000 -0.17738330 0.3740443 -0.5077867 ## black -0.1773833 1.00000000 -0.3660869 0.3334608 ## lstat 0.3740443 -0.36608690 1.0000000 -0.7376627 ## medv -0.5077867 0.33346082 -0.7376627 1.0000000 # To visualize this install.packages(&quot;corrplot&quot;) library(corrplot) ## corrplot 0.84 loaded corrplot(cor(Boston)) # We can also see each scatter plot using the following: # We will study correlation between the following predictors, to not overflow # the graphs. lm.fitMultivariet &lt;- lm(medv~ rm + crim + tax + crim, data = Boston) install.packages(&quot;GGally&quot;) library(GGally) ggpairs(lm.fitMultivariet) 2.7.4 Variance Inflation Scatterplots and correlation matrixes are useful, however they only look at the relations between pairs (bi-variate), we can use variance inflation VIF to account for interaction within multiple attributes. The idea here is to find whether a combination of predictors can describe another predictor \\(x_i\\), to find that we can run a regression line for each \\(x_i\\), \\(x_i = d_0 + d_1x_{2i} + d_2x_{3i} \\ldots\\) , where \\(x_{2i}\\), \\(x_{3i} \\ldots\\) are the rest of the predictors. We can then asses the fitness of that line using the R-squared value. If the line is a good fit then that predictor is well described by the rest of the predictors and we have multicollinearity. # Lets take the example of tax # First we regress the tax as a function of the rest of the predictors lm.tax &lt;- lm(tax~ . - medv, data = Boston) # We can then get the r squared Rsquared &lt;- summary(lm.tax)$r.squared # The variance inflation value is given by the following equation, where if VIF # is large, usually greater than 5, we can say there is multicollinearity # present. VIF &lt;- 1 / (1 - Rsquared) # We can see that in this case the VIF is quite large, this is explained by the # fact that predictors that are location related ones, rooms and house age, how # green the area is, criminal activity, surrounding population social/economical # statues and so on, that are used for predicting house price are also ideal for # predicting tax. VIF ## [1] 9.008554 # Last we can get the VIF values in R using the following code install.packages(&quot;car&quot;) library(car) ## Loading required package: carData ## ## Attaching package: &#39;carData&#39; ## The following object is masked from &#39;package:Ecdat&#39;: ## ## Mroz vif(lm.fitAll) ## crim zn indus chas nox rm age dis ## 1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 ## rad tax ptratio black lstat ## 7.484496 9.008554 1.799084 1.348521 2.941491 # Lets see what happens to our original line if we remove tax lm.fitAllMinusTax &lt;- update(lm.fitAll, ~ . - tax) summary(lm.fitAll)$r.squared ## [1] 0.7406427 summary(lm.fitAllMinusTax)$r.squared ## [1] 0.7349714 The R-squared has slightly gone down. However, tax is a result of factors that we have accounted (e.g location) a change in those factors would affect tax, and price would be affected both by the change in tax (which is only the result of the original change) and by the original change, multiplying its affects and ‘inflating’ the true price value. For example a change is the location of the house would result in an increase in the tax and therefore an increase in the price greater than the true. What would be interesting to do is find out why we are loosing some fitness when we remove the tax, as seen for the R-squared. Are there other factors describing the tax, that we have not included? Could we use them for the price prediction instead of tax? Statistically, we can find out if the change in a restricted model (where a variable is removed, in this case a model without tax) is significant using the F-statistic. # First we would calculate the SSR (regression sum of squared errors) for the # unrestricted and restricted model. summary(lm.fitAll) ## ## Call: ## lm(formula = medv ~ ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.595 -2.730 -0.518 1.777 26.199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** ## crim -1.080e-01 3.286e-02 -3.287 0.001087 ** ## zn 4.642e-02 1.373e-02 3.382 0.000778 *** ## indus 2.056e-02 6.150e-02 0.334 0.738288 ## chas 2.687e+00 8.616e-01 3.118 0.001925 ** ## nox -1.777e+01 3.820e+00 -4.651 4.25e-06 *** ## rm 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** ## age 6.922e-04 1.321e-02 0.052 0.958229 ## dis -1.476e+00 1.995e-01 -7.398 6.01e-13 *** ## rad 3.060e-01 6.635e-02 4.613 5.07e-06 *** ## tax -1.233e-02 3.760e-03 -3.280 0.001112 ** ## ptratio -9.527e-01 1.308e-01 -7.283 1.31e-12 *** ## black 9.312e-03 2.686e-03 3.467 0.000573 *** ## lstat -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.745 on 492 degrees of freedom ## Multiple R-squared: 0.7406, Adjusted R-squared: 0.7338 ## F-statistic: 108.1 on 13 and 492 DF, p-value: &lt; 2.2e-16 SSRun &lt;- anova(lm.fitAll)[&quot;Residuals&quot;, &quot;Sum Sq&quot;] SSRre &lt;- anova(lm.fitAllMinusTax)[&quot;Residuals&quot;, &quot; Sum Sq&quot;] # The f stat is given by: n &lt;- nrow(Boston) p &lt;- length(Boston) Fstat &lt;- ((SSRre - SSRun) / p) / (SSRun / (n - p - 1)) # From the f distribution we can find a critical value that represents the point # separating the curve to the rejection area of a = 0.05 like we do with t # distribution (see hypothesis testing repo). Fcrit &lt;- qf(.95, p, n - p - 1) # The Fstat falls under the rejection area and so we can accept the H0, removing # the tax does produce a significant change. # We can do this in r simply using the following code. anova(lm.fitAll, lm.fitAllMinusTax) ## Analysis of Variance Table ## ## Model 1: medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + ## tax + ptratio + black + lstat ## Model 2: medv ~ crim + zn + indus + chas + nox + rm + age + dis + rad + ## ptratio + black + lstat ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 492 11079 ## 2 493 11321 -1 -242.26 10.758 0.001112 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 2.8 Interaction terms Interaction terms just refers to the effect that certain combinations of predictors can have on the \\(y\\). Sometimes we can optimise our regression model by accounting for the effect of predictor combinations, we can add their products multiplied by a coefficient that shows the effect of the combination (\\(y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3x_1x_2 + \\epsilon\\)). A good example of where this is useful is the synergy effect, studied when analysing the effects of advertising using different means (TV, web, radio, …). It was found that it was most effective to spread the advertising budget across multiple means, rather than focusing it on the most profitable one. To measure such affects, interaction terms are necessary. # Simple way to do this in R for interaction of crim and tax lm.simpleInteraction &lt;- lm(medv~ crim + tax + crim * tax, data = Boston) # We get the coefficient of the interaction and its significance the same way we # get any other coefficient. summary(lm.simpleInteraction) ## ## Call: ## lm(formula = medv ~ crim + tax + crim * tax, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.553 -4.704 -2.180 2.928 33.470 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.319598 1.097800 29.440 &lt; 2e-16 *** ## crim -3.942555 1.534637 -2.569 0.0105 * ## tax -0.021070 0.002633 -8.002 8.57e-15 *** ## crim:tax 0.005634 0.002301 2.449 0.0147 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.996 on 502 degrees of freedom ## Multiple R-squared: 0.2486, Adjusted R-squared: 0.2441 ## F-statistic: 55.37 on 3 and 502 DF, p-value: &lt; 2.2e-16 # Easier way to write the above lm.simpleInteraction2 &lt;- lm(medv~crim * tax, data = Boston) summary(lm.simpleInteraction2) ## ## Call: ## lm(formula = medv ~ crim * tax, data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.553 -4.704 -2.180 2.928 33.470 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.319598 1.097800 29.440 &lt; 2e-16 *** ## crim -3.942555 1.534637 -2.569 0.0105 * ## tax -0.021070 0.002633 -8.002 8.57e-15 *** ## crim:tax 0.005634 0.002301 2.449 0.0147 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.996 on 502 degrees of freedom ## Multiple R-squared: 0.2486, Adjusted R-squared: 0.2441 ## F-statistic: 55.37 on 3 and 502 DF, p-value: &lt; 2.2e-16 # This returns all two-way interactions lm.allPairsInteractions &lt;- lm(medv~ . * ., data = Boston) # We can see that the R-squared when using the 2-way interactions has increased. summary(lm.allPairsInteractions) ## ## Call: ## lm(formula = medv ~ . * ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -7.9374 -1.5344 -0.1068 1.2973 17.8500 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.579e+02 6.800e+01 -2.323 0.020683 * ## crim -1.707e+01 6.554e+00 -2.605 0.009526 ** ## zn -7.529e-02 4.580e-01 -0.164 0.869508 ## indus -2.819e+00 1.696e+00 -1.663 0.097111 . ## chas 4.451e+01 1.952e+01 2.280 0.023123 * ## nox 2.006e+01 7.516e+01 0.267 0.789717 ## rm 2.527e+01 5.699e+00 4.435 1.18e-05 *** ## age 1.263e+00 2.728e-01 4.630 4.90e-06 *** ## dis -1.698e+00 4.604e+00 -0.369 0.712395 ## rad 1.861e+00 2.464e+00 0.755 0.450532 ## tax 3.670e-02 1.440e-01 0.255 0.798978 ## ptratio 2.725e+00 2.850e+00 0.956 0.339567 ## black 9.942e-02 7.468e-02 1.331 0.183833 ## lstat 1.656e+00 8.533e-01 1.940 0.053032 . ## crim:zn 4.144e-01 1.804e-01 2.297 0.022128 * ## crim:indus -4.693e-02 4.480e-01 -0.105 0.916621 ## crim:chas 2.428e+00 5.710e-01 4.251 2.63e-05 *** ## crim:nox -1.108e+00 9.285e-01 -1.193 0.233425 ## crim:rm 2.163e-01 4.907e-02 4.409 1.33e-05 *** ## crim:age -3.083e-03 3.781e-03 -0.815 0.415315 ## crim:dis -1.903e-01 1.060e-01 -1.795 0.073307 . ## crim:rad -6.584e-01 5.815e-01 -1.132 0.258198 ## crim:tax 3.479e-02 4.287e-02 0.812 0.417453 ## crim:ptratio 4.915e-01 3.328e-01 1.477 0.140476 ## crim:black -4.612e-04 1.793e-04 -2.572 0.010451 * ## crim:lstat 2.964e-02 6.544e-03 4.530 7.72e-06 *** ## zn:indus -6.731e-04 4.651e-03 -0.145 0.885000 ## zn:chas -5.230e-02 6.450e-02 -0.811 0.417900 ## zn:nox 1.998e-03 4.721e-01 0.004 0.996625 ## zn:rm -7.286e-04 2.602e-02 -0.028 0.977672 ## zn:age -1.249e-06 8.514e-04 -0.001 0.998830 ## zn:dis 1.097e-02 7.550e-03 1.452 0.147121 ## zn:rad -3.200e-03 6.975e-03 -0.459 0.646591 ## zn:tax 3.937e-04 1.783e-04 2.209 0.027744 * ## zn:ptratio -4.578e-03 7.015e-03 -0.653 0.514325 ## zn:black 1.159e-04 7.599e-04 0.153 0.878841 ## zn:lstat -1.064e-02 4.662e-03 -2.281 0.023040 * ## indus:chas -3.672e-01 3.780e-01 -0.971 0.331881 ## indus:nox 3.138e+00 1.449e+00 2.166 0.030855 * ## indus:rm 3.301e-01 1.327e-01 2.488 0.013257 * ## indus:age -4.865e-04 3.659e-03 -0.133 0.894284 ## indus:dis -4.486e-02 6.312e-02 -0.711 0.477645 ## indus:rad -2.089e-02 5.020e-02 -0.416 0.677560 ## indus:tax 3.129e-04 6.034e-04 0.519 0.604322 ## indus:ptratio -6.011e-02 3.783e-02 -1.589 0.112820 ## indus:black 1.122e-03 2.034e-03 0.552 0.581464 ## indus:lstat 5.063e-03 1.523e-02 0.332 0.739789 ## chas:nox -3.272e+01 1.243e+01 -2.631 0.008820 ** ## chas:rm -5.384e+00 1.150e+00 -4.681 3.87e-06 *** ## chas:age 3.040e-02 5.840e-02 0.521 0.602982 ## chas:dis 9.022e-01 1.334e+00 0.676 0.499143 ## chas:rad -7.773e-01 5.707e-01 -1.362 0.173907 ## chas:tax 4.627e-02 3.645e-02 1.270 0.204930 ## chas:ptratio -6.145e-01 6.914e-01 -0.889 0.374604 ## chas:black 2.500e-02 1.567e-02 1.595 0.111423 ## chas:lstat -2.980e-01 1.845e-01 -1.615 0.107008 ## nox:rm 5.990e+00 5.468e+00 1.095 0.273952 ## nox:age -7.273e-01 2.340e-01 -3.108 0.002012 ** ## nox:dis 5.694e+00 3.723e+00 1.529 0.126969 ## nox:rad -1.994e-01 1.897e+00 -0.105 0.916360 ## nox:tax -2.793e-02 1.312e-01 -0.213 0.831559 ## nox:ptratio -3.669e+00 3.096e+00 -1.185 0.236648 ## nox:black -1.854e-02 3.615e-02 -0.513 0.608298 ## nox:lstat 1.119e+00 6.511e-01 1.719 0.086304 . ## rm:age -6.277e-02 2.203e-02 -2.849 0.004606 ** ## rm:dis 3.190e-01 3.295e-01 0.968 0.333516 ## rm:rad -8.422e-02 1.527e-01 -0.552 0.581565 ## rm:tax -2.242e-02 9.910e-03 -2.262 0.024216 * ## rm:ptratio -4.880e-01 2.172e-01 -2.247 0.025189 * ## rm:black -4.528e-03 3.351e-03 -1.351 0.177386 ## rm:lstat -2.968e-01 4.316e-02 -6.878 2.24e-11 *** ## age:dis -1.678e-02 8.882e-03 -1.889 0.059589 . ## age:rad 1.442e-02 4.212e-03 3.423 0.000682 *** ## age:tax -3.403e-04 2.187e-04 -1.556 0.120437 ## age:ptratio -7.520e-03 6.793e-03 -1.107 0.268946 ## age:black -7.029e-04 2.136e-04 -3.291 0.001083 ** ## age:lstat -6.023e-03 1.936e-03 -3.111 0.001991 ** ## dis:rad -5.580e-02 7.075e-02 -0.789 0.430678 ## dis:tax -3.882e-03 2.496e-03 -1.555 0.120623 ## dis:ptratio -4.786e-02 9.983e-02 -0.479 0.631920 ## dis:black -5.194e-03 5.541e-03 -0.937 0.349116 ## dis:lstat 1.350e-01 4.866e-02 2.775 0.005774 ** ## rad:tax 3.131e-05 1.446e-03 0.022 0.982729 ## rad:ptratio -4.379e-02 8.392e-02 -0.522 0.602121 ## rad:black -4.362e-04 2.518e-03 -0.173 0.862561 ## rad:lstat -2.529e-02 1.816e-02 -1.392 0.164530 ## tax:ptratio 7.854e-03 2.504e-03 3.137 0.001830 ** ## tax:black -4.785e-07 1.999e-04 -0.002 0.998091 ## tax:lstat -1.403e-03 1.208e-03 -1.162 0.245940 ## ptratio:black 1.203e-03 3.361e-03 0.358 0.720508 ## ptratio:lstat 3.901e-03 2.985e-02 0.131 0.896068 ## black:lstat -6.118e-04 4.157e-04 -1.472 0.141837 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.852 on 414 degrees of freedom ## Multiple R-squared: 0.9212, Adjusted R-squared: 0.9039 ## F-statistic: 53.18 on 91 and 414 DF, p-value: &lt; 2.2e-16 Lets try and explain some of the significant interactions found: Combination of chas (close to river) and nox (greenery), that combination may indicate that the house is build in a very scenic area and therefore more expensive. crim (criminal activity) and lstat (\\(\\%\\) lower status of population) may indicate how degraded an area is and therefore less expensive rm (room per dwelling) and age, that combination may explain a lot of the variance in our dataset since, most recent houses tend to have less space yet are more expensive due to modern equipment and built. We need to consider which interactions are more significant and make the most sense from domain knowledge. Also keep in mind that we need to sustain an efficient and realistic model, not just a model with a very good R-squared value. 2.9 Non-linear Transformations of Predictors A common optimisation technique when dealing with linear regression is polynomial transformation of some the predictor (e.g \\(y = \\beta_0 + \\beta_1x + \\beta_2x^2 + \\epsilon\\)). This is polynomial regression, and is still a linear model. This is useful since often a relationship between variables is non linear, and this may be realised using scatterplots. # Lets use the example of lstat and medv plot(Boston$lstat, Boston$medv) # We can see that the scatterplot follows a curve, which indicates that # polynomials would be effective. # Lets compare the two: lm.linearRegression &lt;- lm(medv~ lstat, data = Boston) lm.PolynomialRegression &lt;- lm(medv~ lstat + I(lstat ^ 2), data = Boston) anova(lm.linearRegression, lm.PolynomialRegression) ## Analysis of Variance Table ## ## Model 1: medv ~ lstat ## Model 2: medv ~ lstat + I(lstat^2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 504 19472 ## 2 503 15347 1 4125.1 135.2 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # The anova tested the H0 where the models both fit the data equally(as # explained above). The F-statistic associated produced a very small providing # enough evidence to reject the H0. # We can visualise this: newdat &lt;- data.frame(lstat = seq(min(Boston$lstat), max(Boston$lstat), length.out = 100)) newdat$pred &lt;- predict(lm.PolynomialRegression, newdata = newdat) plot(medv ~ lstat, data = Boston) with(newdat, lines(x = lstat, y = pred)) abline(lm(medv~ lstat, data = Boston), col = &quot;red&quot;) # It is clear from the graph that the black line fits the data much better 2.10 Qualitative Predictors Up until now we have dealt with numbers; what if one of our predictors was qualitative (e.g. sex, colour, level of satisfaction)? To use such a variable in LR we have to assign a dummy number/factor/enumeration, just some sort of number that will always be associated with one particular response (e.g female \\(= 1\\), male \\(= 0\\)). ?Carseats # R creates dummy values for qualitative predictors automatically lm.fitQual &lt;- lm(Sales ~., data = Carseats) summary(lm.fitQual) ## ## Call: ## lm(formula = Sales ~ ., data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8692 -0.6908 0.0211 0.6636 3.4115 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.6606231 0.6034487 9.380 &lt; 2e-16 *** ## CompPrice 0.0928153 0.0041477 22.378 &lt; 2e-16 *** ## Income 0.0158028 0.0018451 8.565 2.58e-16 *** ## Advertising 0.1230951 0.0111237 11.066 &lt; 2e-16 *** ## Population 0.0002079 0.0003705 0.561 0.575 ## Price -0.0953579 0.0026711 -35.700 &lt; 2e-16 *** ## ShelveLocGood 4.8501827 0.1531100 31.678 &lt; 2e-16 *** ## ShelveLocMedium 1.9567148 0.1261056 15.516 &lt; 2e-16 *** ## Age -0.0460452 0.0031817 -14.472 &lt; 2e-16 *** ## Education -0.0211018 0.0197205 -1.070 0.285 ## UrbanYes 0.1228864 0.1129761 1.088 0.277 ## USYes -0.1840928 0.1498423 -1.229 0.220 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.019 on 388 degrees of freedom ## Multiple R-squared: 0.8734, Adjusted R-squared: 0.8698 ## F-statistic: 243.4 on 11 and 388 DF, p-value: &lt; 2.2e-16 # We can see those dummy values using the following code attach(Carseats) contrasts(ShelveLoc) ## Good Medium ## Bad 0 0 ## Good 1 0 ## Medium 0 1 "],
["logistic-regression.html", "3 Logistic Regression 3.1 Usage 3.2 Formula 3.3 Maximum Likelihood 3.4 R-squared 3.5 The Saturated and Null Models 3.6 Residual and Null Deviance 3.7 p-values 3.8 Introductory Demonstration in R 3.9 Limitations 3.10 Measuring Performance Using Confusion matrix 3.11 Optimising the Threshold", " 3 Logistic Regression 3.1 Usage Similar to linear regression (part of the family of Generalised Linear Models). However it is part of a group of models called classifiers (tries to group elements/events) and this most often involves qualitative rather than quantitative data. Unlike linear regression, which is used mostly chosen for inference (studying the relationship between variables, effects of each other, strength and direction), we usually use logistic regression for predicting (or sometimes studying) a binary outcome (has only two outcomes, patient has diabetes or not, transaction is fraud or legit). The model does that by measuring the probability associated with each of the two and based on a chosen threshold will decide on one (e.g if \\(p &gt; 0.5\\) a transaction is fraud). It assigns those probabilities by taking into account the predictors we have provided and their approximated coefficients as calculated from the data, that we have trained the model with. For example location of the transaction, time or money. As with linear regression, quantitative data is handled by assigning a dummy variable boolean (e.g \\(\\text{isFraud} = 1\\), \\(\\text{isNotFraud} = 0\\)). We are interested in finding how close a point is to \\(1\\) (in other words its probability of success (fraud in this case) plotted on the \\(y\\) axis), as measured by the predictors (\\(x\\), \\(z\\) … axis). The issue is that we can get any value between \\(0\\) and \\(1\\), as well as values above and bellow that (e.g if the amount of money used in a transaction is extremely large we might get something like \\(p = 2.3\\) according to whatever coefficients in \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\epsilon\\) are calculated). This is not as useful and sometimes does not make sense, that’s why we use a transformation to fit our data exactly between \\(0\\) and \\(1\\). 3.2 Formula There are various transformation for that, in logistic regression we use the logistic function: \\(P(x) = \\frac{e^{(\\beta_0 + \\beta_1x)}}{1 + e^{\\beta_0 + \\beta_1X}}\\), where \\(b0\\) and \\(b1\\) are our coefficients, this makes an S-shaped curve where the edges are \\(0\\) and \\(1\\). We want to try and bring this in a more linear form, after manipulation we can see that: \\(\\frac{P(x)}{1 - P(x)} = e^{\\beta_0 + \\beta_1x}\\) \\(\\frac{P(x)}{1 - P(x)}\\), is called the the odds. It is the probability of success over the probability of failure. It is used for example in horse-racing where if the odds are \\(\\frac{1}{3}\\) the horse is likely to win one race but loose \\(3\\). To make our equation more linear we can take the log. We notice that this is basically a linear equation using the log of the odds on \\(y\\)-axis \\(log(\\frac{P(x)}{1 - P(x)}) = \\beta_0 + \\beta_1x\\), this makes things a lot easier (see more about why it’s helpful later on) 3.3 Maximum Likelihood Another issue is how we can find the ‘best fit line’, that best describes all our data points. In linear regression we used the least squares approach. This required measuring the residuals (distance of our points to the line) and trying to minimise that value by moving the line. However the distance of the points from a random line (close to the real) in linear regression is somehow constant, the error has a constant standard deviation. In logistic regression when a predictor reaches extreme points the distance will tend to \\(\\pm \\infty\\). For example if we try and fit a line for the probability of a transaction been fraud based on the money involved; a line close to the best fit would have very large distance on the points where the amount of money involved in a transaction is large, or where it is very little. For that reason the method of maximum likelihood is preferred. We use the following steps to do that: Take a random straight line (a candidate ’best fit, just like in regression), that we think describes our points well Use the \\(P(x) = \\frac{e^{(\\beta_0 + \\beta_1x)}}{1 + e^{\\beta_0 + \\beta_1X}}\\) (given above) to transform that street line to the S-shaped curve that plots the probability from \\(0\\) to \\(1\\) for each of our \\(x\\) We can now use those calculated \\(P(x)\\) foe each of our \\(x\\) to access how good this candidate line is. For each \\(x\\) we can get the ‘predicted’ by the curve \\(y\\) (\\(P(x)\\)), which in this case is also called the ‘likelihood’ (in this case) We then calculate the likelihood for ALL of the success cases (\\(P(x)\\)) and unsuccessful cases (\\(1 - P(x)\\)) (e.g probability, given by \\(y\\) value, of a fraud transaction actually been fraud, and probability, again as calculated by our model of a legit transaction been not fraud). Whether a point is a success of not, we know from the data we have already collected. To get the total, we just need to multiply everything. In a way this calculates ‘how likely we are’ to predicting everything correctly, so the higher that is the better. In reality we prefer to calculate the \\(\\log\\) of that probability instead. ‘It just so happens’ that the \\(\\log\\) of the maximum of the \\(\\log\\) of the likelihood is the same as the maximum of the likelihood. The reason why the \\(\\log\\) is favoured stems from the fact that it is easier to differentiate a product. When looking for the max of an equation, we differentiate that equation and look for where it is equal to \\(0\\). If we take the \\(\\log\\) of the equation we turn a difficult problem of differentiating into differentiating a sum since \\(\\log(a \\times b) = \\log(a) + \\log(b)\\), which is easy to solve. Finally, we choose the candidate line (which has our coefficients), that results in the higher \\(\\log\\) likelihood. (Software like R does this very very easily using ‘smart algorithms’) 3.4 R-squared But if we can’t calculate the residuals (distance of the predicted \\(y\\) (from our line) to the true \\(y\\) (from our points) how do we calculate the R-squared and p-values. well this is not as simple as in linear regression and there are dozens different ways to do it. An easy and common way, supported by R, is the MacFadden’s Pseudo R-squared, this is very similar to how its calculated in linear regression. If we remember, R-squared is a value measuring how much of the variation in \\(y\\) is explained by our model, which indicates how well it fits the data. To find that value we calculate the total variation not described by the line, by dividing a measure of the distance of our points to the line, to the distance of our points to the mean. The we take one minus the outcome. This basically will show how much better our model is from just predicting using the mean, no predictors involved. So all we need to do is find a different way of calculating how fit the model is to the data and what the predictions would be if we used the mean instead. Well we already have the first, the \\(\\log\\) of the likelihood for our curve (LL(fit)), exactly explains how close our predictions are to the true \\(y\\). We can get the probability of a case been successful, without accounting for any predictors, simple by the definition of probabilities \\(P(\\text{of success}) = \\frac{\\text{successful cases}}{\\text{total cases}}\\). Using that as the \\(p(x)\\) for ALL of our values we can can get the log of the likelihood as before (LL(overall)). We can then compare the two and see how much better our model is. Just like in linear regression the resulting R-square will be from \\(0\\) to \\(1\\). 3.5 The Saturated and Null Models We know that the R-squared always takes values from \\(0\\) to \\(1\\). This helps us make objective conclusions on how well our model performs and compare the performance of different models. We know that it is given by \\(1 - \\frac{\\text{some measure of fitness of our model}}{\\text{some measure of fitness of a &#39;bad&#39; model}}\\), to show how much ‘better’ our model is. In linear regression our model’s fitness is measured by the distance of the points to our line. This will only ever go as far as zero, since the perfect model will have zero distance. In linear regression we take the \\(\\log\\) of the likelihood using the product of all the probabilities, a probability will only ever go as far as \\(1\\) resulting in \\(\\log(1) = 0\\). What this means is that we had no need to provide an upper bound, all we needed was a lower bound, a measure of ‘bad’ performance which was making prediction by just using a mean, since the perfect model would always result having a zero as the nominator and giving us a R-squared of \\(1 - 0 = 1\\), a perfect fit. However this is not always the case, in many other models the calculation of R-squared could result in any number, and this would not provide any clear indication of whether our model is doing well or not. This is why we need an upper bound. That upper bound is just a model that fits perfectly all our data points and is called the Saturated model. It requires all the data points as parameters to be able to make a model that perfectly fits them all and maximises the likelihood. If you are wondering why don’t we just use that model, if it maximises the likelihood, the answer is that it usually does not follow the format we are looking for. It is not genericised (it suffers from overfitting) and will not allow us to make any accurate predictions or study relationships. In a way, it is just a random line that crosses all the data points and does not really show any patterns, cannot be described by an algorithm and may result in unnecessary over-complications. The opposite of the saturated model is the Null model, a model that uses only one parameter (for example the mean in the linear regression), we already used that as the lower bound. The model that we are trying to measure the performance of by comparing it to the null and saturated is called the Proposed model. The general form of R-squared can be described by \\(1 - \\frac{\\text{a measure of bad fit} - \\text{a measure of fit of our model}}{\\text{a measure of bad fit} - \\text{a measure of perfect fit}}\\), which will always result is \\(0\\) to \\(1\\), since Null model is the minimum and Saturated the Maximum. ‘it just so happened’ that up to now the measure of perfect fit was zero so it could be omitted. 3.6 Residual and Null Deviance When we get the summary statistics of a logistic regression model, R outputs something called Residual and Null Deviance. Those are nothing more that just more performance measurements for our model, which are derived by comparing how far our proposed model is from the Null (bad) model and how close it is to the Saturated (perfect) model. More specifically: \\(\\text{Residual Deviance} = 2(\\text{log of likelihood of Saturated} - \\text{log of likelihood of proposed})\\) \\(\\text{Null deviance} = 2(\\text{log of likelihood of proposed} - \\text{log of likelihood of null})\\) Those two output a chi-square value that can be used to calculate a p-value which indicates whether our proposed model is significantly far from the Null and Saturated model. We want those outputs to be as small as possible. When we try and optimise our model by adding and removing predictors we should take note of the effect on those values! 3.7 p-values For p-values we can simply use the chi-square distribution (see chapter chi-square test). The chi-square value equals 2(LL(fit)-LL(overall)) and the degrees of freedom are the difference in the parameters of LL(fit) and LL(overall). From those you can calculate the area under the graph which will give the probability of getting such values randomly, if there is no correlation between your predictors and reaction. 3.8 Introductory Demonstration in R We will use the Titanic data to see if we can use the given attributes as predictors that determine whether or not a passenger survived. install.packages(&quot;titanic&quot;) library(titanic) data(&quot;titanic_train&quot;) # Set our data data &lt;- titanic_train # Replace missing values for Age with mean data$Age[is.na(data$Age)] = mean(data$Age, na.rm = TRUE) # Remove attributes that are not valuable install.packages(&quot;dplyr&quot;) library(dplyr) data &lt;- data %&gt;% select(-c(Cabin, PassengerId, Ticket, Name)) # Use factors for the following attributes: for (i in c(&quot;Survived&quot;,&quot;Pclass&quot;,&quot;Sex&quot;,&quot;Embarked&quot;)){ data[,i] = as.factor(data[,i]) } # Creates a general linear model (family = binomial specifies logistic # regression) gml.fit = glm(Survived~., data = data, family = binomial(link =&#39;logit&#39;)) # We can see our estimated coefficients and associated p-values summary(gml.fit) ## ## Call: ## glm(formula = Survived ~ ., family = binomial(link = &quot;logit&quot;), ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6235 -0.6098 -0.4222 0.6100 2.4512 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 16.414388 610.558089 0.027 0.97855 ## Pclass2 -0.924047 0.297882 -3.102 0.00192 ** ## Pclass3 -2.149626 0.297749 -7.220 5.21e-13 *** ## Sexmale -2.709611 0.201336 -13.458 &lt; 2e-16 *** ## Age -0.039320 0.007888 -4.984 6.21e-07 *** ## SibSp -0.322143 0.109545 -2.941 0.00327 ** ## Parch -0.095061 0.119028 -0.799 0.42450 ## Fare 0.002261 0.002462 0.918 0.35842 ## EmbarkedC -12.311604 610.557974 -0.020 0.98391 ## EmbarkedQ -12.341443 610.558025 -0.020 0.98387 ## EmbarkedS -12.757357 610.557962 -0.021 0.98333 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1186.66 on 890 degrees of freedom ## Residual deviance: 783.74 on 880 degrees of freedom ## AIC: 805.74 ## ## Number of Fisher Scoring iterations: 13 # Getting the R-squared install.packages(&quot;DescTools&quot;) library(&#39;DescTools&#39;) ## ## Attaching package: &#39;DescTools&#39; ## The following object is masked from &#39;package:car&#39;: ## ## Recode ## The following object is masked from &#39;package:Ecfun&#39;: ## ## BoxCox # You will notice this is relatively low, it not easy to predict the survival of # a passenger only given a few details about them a lot of random factors are # involved. There are also a lot of limitations in our model. PseudoR2(gml.fit) ## McFadden ## 0.3395424 We notice that this is a multivariate logistic regression, that takes the form of: \\(log(\\frac{p(x)}{1 - p(x)}) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n + \\epsilon\\) 3.9 Limitations 3.9.1 Confounding We often use logistic regression (and other models) to study the effects of various variables on an the occurrence of an event. We will use the following examples: How does smoking affect the probability of a patient developing lung diseases How does the fact that an individual is a student affect their probability of defaulting in the bank account When we conduct such a study measuring how many smokers have developed lung disease versus how many non smokers, and how many students have defaulted versus how many non-students, we will most probably find a positive relationship. We cannot however, blindly trust the bivariate analysis, I mean unless we managed to get a perfectly random and balanced across all ages, sexes, incomes … sample there is a good chance that our results are partially the result of other variables masked under the ones we have chosen to study. For example studies have shown that there are more older people that are smokers, also older people tend to be more prone to diseases. So when we only use smoker/non smoker part of why our coefficient for that predictor would be so high is because smokers are also older people. This is called bias due to confounding and occurs when confounders (such as age) are not included in our model. If we take the example of students, when we only use student/non-student to predict the probability of someone defaulting we will find a positive correlation. However, studies have shown that students with the same income as non-students are less likely to default. Here the confounder is income, it just so happens that students will tend to have lower income, which makes them prone to defaulting. Therefore not including this in our models could result in biased predictions. In the older days, where creating such models was more difficult due to computing power and software not been as readily available people used ‘stratifying’ in order to reduce the effects of confounding. They would have to separate their data in subsets (strata) that were free of confounders. For example split by age groups or by student/non student. Then they would conduct the estimation of coefficients separately for each group and use techniques such us pooling or weighted averaging (if it made sense) to get an overall estimation of the coefficients. If you are interested to learn more about this you can watch this video from a Harvard lecture: https://www.youtube.com/watch?v=hNZVFMVKVlc For reference, if we were to strictly define what a confounder is, it would be a variable that satisfies the following conditions: Confounders has to be related to one of the significant predictors (e.g older people tend to be smokers more than younger) It has to able be independently related to the reaction (e.g. age is also on its own a significant predictor of someone developing a disease) It is not part of the casual pathway (e.g smoking does not cause you to be old) 3.9.2 Multicollinearity Logistic regression also assumes no or little multicollinearity among the predictors. (see previous chapter) 3.9.3 Interaction terms Exactly the same as with linear regression (see previous chapter). Including the product (combination) of certain predictors could optimise the model. \\(\\log(\\frac{P(x)}{1 - P(x)}) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2\\) 3.9.4 Heteroscedasticity (not relevant) In logistic regression, we except a probability to be systematically further away from the predicted \\(y\\), in other words homoscedasticity is not an assumption in this case. This is why we are unable to use least squares to find our best fitting line on the first place, since the residuals (distance of our points to the line) are reaching \\(\\mp\\infty\\). 3.10 Measuring Performance Using Confusion matrix Let’s use another example. Predicting whether or not a patient is diabetic. Our sample data set contains attributes such as BMI, age and measurements of glucose concentration, as well as a boolean that indicates whether or not each patient is diabetic. We hope to use those attributes as predictors for the patients diabetic condition. This time we will use one other technique for measuring how well our model preforms in giving accurate predictions. It is a common technique used in various models and not just logistic regression. Basically we will split the data that we have in two, the training data and the testing data. Our training data will be \\(80%\\) of the total and it will be used to estimate the coefficients and draw the S-curve by assigning probabilities (in other words training our model). The test data will be then fed to the trained model (without the attribute that gives out if a patient is diabetic) and our model will give a prediction for each of the row. That prediction will be according to a threshold we have assigned, for example we may choose that if the probability of a patient been diabetic is \\(p \\geq 0.5\\) then we predict they are diabetic. Since we actually know if the patients in the test data are diabetic or not, we can compare the actual to the predicted outcomes and get a \\(%\\) accuracy. We will see how this is done in detail, as well as how we can optimise the chosen threshold for our specific problem. # We can use a csv that is available online for our data, we can load this in R # using the following code: data &lt;- read.csv(file = &quot;http://www2.compute.dtu.dk/courses/02819/pima.csv&quot;, head = TRUE, sep = &quot;,&quot;) 3.10.1 Splitting the Data # First we need to split our data, we do this with the use of caTools library # (there are other ways too). # The following creates an array of TRUE/FALSE values for every row in our data # set, where TRUE is 80% of the data. It does this &#39;randomly&#39;, later on we will # see techniques for spreading the success and failure rates proportionally # across training and testing data. library(caTools) split &lt;- sample.split(data, SplitRatio = 0.8) split ## [1] TRUE TRUE FALSE TRUE TRUE TRUE TRUE TRUE FALSE # Transform to factors data$type &lt;- as.factor(data$type) # We then split the data using that array, where TRUE is for the training and # FALSE is for the testing trainingData &lt;- subset(data, split == TRUE) testingData &lt;- subset(data, split == FALSE) # Create the model using the TRAINING data model &lt;- glm(type ~., trainingData, family = binomial(link = &#39;logit&#39;)) # Review our model summary(model) ## ## Call: ## glm(formula = type ~ ., family = binomial(link = &quot;logit&quot;), data = trainingData) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.2185 -0.6092 -0.3336 0.5978 2.3026 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -9.179437 1.419568 -6.466 1.00e-10 *** ## X -0.003374 0.001879 -1.796 0.07248 . ## npreg 0.130182 0.069738 1.867 0.06194 . ## glu 0.040785 0.006404 6.368 1.91e-10 *** ## bp -0.013902 0.014822 -0.938 0.34828 ## skin 0.002866 0.023919 0.120 0.90463 ## bmi 0.096078 0.034326 2.799 0.00513 ** ## ped 1.308363 0.545403 2.399 0.01644 * ## age 0.013224 0.021088 0.627 0.53062 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 326.38 on 258 degrees of freedom ## Residual deviance: 214.83 on 250 degrees of freedom ## AIC: 232.83 ## ## Number of Fisher Scoring iterations: 5 # As expected BMI and glu are significant (you can try and remove the predictors # with a larger p-value but do this incrementally and note the effect it has on # the Null and Deviance residuals (see above) ) 3.10.2 Visualisations # We will use only glu to plot our graphs easier gml.fitGlu = glm(type ~ glu, data = trainingData, family = binomial(link =&#39;logit&#39;)) summary(gml.fitGlu) ## ## Call: ## glm(formula = type ~ glu, family = binomial(link = &quot;logit&quot;), ## data = trainingData) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2557 -0.6869 -0.4769 0.6493 2.3822 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -6.211176 0.757805 -8.196 2.48e-16 *** ## glu 0.044028 0.005866 7.505 6.13e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 326.38 on 258 degrees of freedom ## Residual deviance: 246.37 on 257 degrees of freedom ## AIC: 250.37 ## ## Number of Fisher Scoring iterations: 4 # First we will plot the best fitting line of probability of survival over age # We need to get the probabilities for each row p &lt;- predict(gml.fitGlu, trainingData, type = &quot;response&quot;) # Get the log of the odds, which is our y y &lt;- log(p/(1 - p)) # Substitute with our calculated coefficients for the x axis x &lt;- -5.940285 + 0.042616 * trainingData$glu plot(x, y) # Now let&#39;s transform it to the S-shaped curve with y from 0 to 1. # This is just the logistic function where i have substituted the coefficient # for the intercept for our x with the ones calculated from our model summary. f &lt;- function(x) { 1 / (1 + exp(-5.940285 + (0.042616 * x )))} x &lt;- trainingData$glu # This scales the graph x &lt;- -500 : 700 plot(x, f(x), type = &quot;l&quot;) 3.10.3 Confusion Matrix Calculations # The predict() function allows us to feed new data in the trained model and # receive a probability of each patient being diabetic; we will use this on our # testing data. # type = response outputs the probability of each patient been diabetic, we save # the array of probabilities on pdata pdata &lt;- predict(model, testingData, type = &quot;response&quot;) # We will use those probabilities to predict whether the patient is diabetic or # not according to our model, given a threshold (p &gt; 0.55 in this case) and then # compare the outcomes to the real values. We already know if those patients are # actually diabetic. We will compare the predicted to the actual values using a # confusion matrix. # We will use the Caret library for that install.packages(&quot;caret&quot;) library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following objects are masked from &#39;package:DescTools&#39;: ## ## MAE, RMSE # Create two arrays that contain TRUE or FALSE for each row (is diabetic TRUE, # is not diabetic FALSE) one for the predicted data and one for the actual, in # order to compare them in R they need to be type factors (enumerations) and # have of the same levels (take on the same values) # For the predicted TRUE is a row of probability &gt; 0.55 predicted &lt;- as.factor(pdata &gt; 0.55) # For the actual data, either or not a patient is diabetic is given by yes or # no, so true is yes real &lt;- as.factor(testingData$type == &quot;Yes&quot;) # use caret to compute a confusion matrix install.packages(&quot;e1071&quot;) cm &lt;- confusionMatrix(data = predicted, reference = real) cm ## Confusion Matrix and Statistics ## ## Reference ## Prediction FALSE TRUE ## FALSE 44 12 ## TRUE 4 13 ## ## Accuracy : 0.7808 ## 95% CI : (0.6686, 0.8692) ## No Information Rate : 0.6575 ## P-Value [Acc &gt; NIR] : 0.01560 ## ## Kappa : 0.4729 ## Mcnemar&#39;s Test P-Value : 0.08012 ## ## Sensitivity : 0.9167 ## Specificity : 0.5200 ## Pos Pred Value : 0.7857 ## Neg Pred Value : 0.7647 ## Prevalence : 0.6575 ## Detection Rate : 0.6027 ## Detection Prevalence : 0.7671 ## Balanced Accuracy : 0.7183 ## ## &#39;Positive&#39; Class : FALSE ## # We can see that this has outputted a table describing the following in each # cell: # | Predicted | Actual | Name | # |-----------|--------|---------------------| # | True | True | True Positive (TP) | # | True | False | False Positive (FP) | # | False | False | True Negative (TN) | # | False | True | False Negative (FN) | # # If predict a patient is diabetic when they are not that&#39;s a False Positive, # also known as a type 1 error # # If we predict a patient is not diabetic when they are that&#39;s a False Negative; # also known as a type 2 error. # # Prediction: True # | # V # False Positive # ^ # | # Actual: False 3.10.4 Measuring Accuracy It has also outputted some metrics on the performance of our model, lets explain them: Sensitivity (also known as true positive, recall or hit rate) measures how well your model does in predicting positive values (predicting that the patient is diabetic when they actually are), given by: \\(\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\\) Specificity (also known as true negative rate) measures how well your model does in predicting negative values, (predicting that the patient is not diabetic when they actually aren’t), given by: \\(\\frac{\\text{TN}}{\\text{TN} + \\text{FP}}\\) Accuracy gives an overall performance measurement on the accuracy of the results on a scale of \\(0\\) to \\(1\\), with \\(1\\) meaning that the model predicted everything correctly, given by: \\(\\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{FP} + \\text{FN} + \\text{TP}}\\) There is a big problem with using this metric as a performance indicator. Let’s use an example where we create a model that simply always outputs False. We use that model to predict whether or not an individual has a very rare disease that is estimated only \\(1\\) on \\(100\\) people have. If we use that model on a very large sample of people (that are already diagnosed and know whether or not they have the disease) and then see how our model did, we will measure something like a \\(0.99\\) accuracy. That is amazing right, our model is “\\(99%\\) accurate”! This illustrates how this metric is biased by what proportions of positive and negative values are available, a model could do well simple by chance like we show in our example. 3.10.5 The Kappa Coefficient Cohen’s Kappa coefficient is a metric that tries to tackle the problem of bias in the measurement of accuracy. It does so by ‘eliminating’ the chance of randomly getting a correct prediction from the equation. This ‘chance’ is just given by probability of randomly selecting a true value from the sample plus the probability of randomly selecting a false from the sample, since we want to measure both the chance of predicting True when True and False when False. In the sample (confusion matrix), we could select a false value either from the total predicted false or the total actual false. So we need to multiply the probabilities associated with both. Similarly we could select a positive value either from the total predicted positive or the total actually positive. # From our confusion matrix table we can get: # FN + FP totalFALSE &lt;- 47 + 15 # TP +TN totalTRUE &lt;- 5 + 7 # NT + NF totalNegative &lt;- 47 + 5 # PT + PF totalPositive &lt;- 15 + 7 # Everything totalSAMPLE &lt;- 47 + 5 + 15 + 7 P_of_true_from_actual = (totalTRUE / totalSAMPLE) P_of_true_from_predicted = (totalPositive / totalSAMPLE) P_of_false_from_actual = (totalFALSE / totalSAMPLE) P_of_false_from_predicted = (totalNegative / totalSAMPLE) P_of_chance_for_TRUE &lt;- P_of_true_from_actual * P_of_true_from_predicted P_of_chance_for_FALSE &lt;- P_of_false_from_actual * P_of_false_from_predicted P_of_chance &lt;- P_of_chance_for_TRUE + P_of_chance_for_FALSE # We can see that our P_of_chance is very high, this is because we have more # samples of False, there are more people that are NOT diagnosed with diabetes. P_of_chance ## [1] 0.6369613 # To get the Kappa coefficient we would use the fallowing formula... # First we need to select accuracy from our confusion matric metrics accuracy &lt;- cm$overall[&#39;Accuracy&#39;][[1]][1] #Kappa is given by: Kappa &lt;- (accuracy - P_of_chance)/(1-P_of_chance) #we can see this is the same value given by R in the confusion matrix Kappa ## [1] 0.396268 So how do we interpret Kappa, this is a bit vague and there is no clear answer but we can see some examples: \\(1\\) means our model is prefect \\(0\\) means it as good as chance Any negative value means it does worse than chance Anything close to \\(0.3\\) and above is relatively good Values of \\(0.8\\) and above are extremely rare 3.11 Optimising the Threshold Changing the threshold will affect our models sensitivity and specificity. If we had a very low threshold where \\(p\\) is close to \\(0\\), then almost always we would predict that a patient was diabetic. This means that we would predict correctly True when True, but also True when False, in other words we would have a very high true positive rate. If we had a very high threshold, close to \\(p = 1\\), then almost always we would predict that a patient is not diabetic. This means we would predict False when False, but also False when True, in other words we would have a very high false positive rate. Usually we want to balance this and we wind up somewhere in the middle. However, we need to apply domain knowledge and think about whether we care more about true positive rate or false positive rate. For example what is the effects of: predicting that a patient is diabetic when they aren’t (having more FP), referred to as type I errors versus predicting that a patient is not diabetic when they are (having more TN), referred to as type II errors. Usually in such cases we would be more concerned for the type 2 errors, since failing to diagnose a diabetes can have a greater impact on the patients health, so we might choose a \\(p\\) somewhere lower than \\(0.5\\) maybe \\(0.4\\). # We can visualise how changing our threshold affects the true and false # positive rate to help us choose the right p install.packages(&quot;ROCR&quot;) library(ROCR) ## Loading required package: gplots ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess resForTraining &lt;- predict(model, trainingData, type = &quot;response&quot;) # Choosing a value ROCRPred = prediction(resForTraining,trainingData$type) ROCRPref &lt;- performance(ROCRPred, &quot;tpr&quot;, &quot;fpr&quot;) # We want something on the green region between 0.4 and 0.6, but closer to 0.4 # to ensure less type 2 errors plot(ROCRPref, colorize = TRUE) "],
["advanced-techniques-for-linear-algorithms.html", "4 Advanced techniques for linear algorithms 4.1 Introduction 4.2 Improved performance indicators (adjusted R-squared and alternatives) 4.3 Cross Validation 4.4 Selecting the optimal predictors for the model 4.5 Shrinkage/Regularisation methods", " 4 Advanced techniques for linear algorithms 4.1 Introduction In this chapter we will look into some more advanced ways of measuring the performance of our models as well improving it. Up to now we have been relating performance with how well our algorithm describes the variation in our training data (remember how we defined R-squared).There is a major issue with this, the more predictors we add the more our model will become ‘better’ according to this definition. Remember we are only using a SAMPLE of TRAINING data, both for designing the model and for measuring its performance. This sample of training data will contain some noise (some randomness), and when we add a new predictor, even if its completely irrelevant, a weak relationship (either positive or negative) will be found between that predictor and this randomness. The more of those irrelevant predictors we add, incrementally, we will start to describe this random variation better (in reality you can imagine this as shear luck). However this randomness is ONLY relevant to the sample data set that we happened to have, if we select another sample from the population we would have a different noise and all those predictors that we though are adding value to our model would only be causing issues. In this chapter we will look into how we can mitigate this issue and optimise our models as well us measure their performance more accurately. In this chapter we will look at doing so, while still building a linear-based model, you could also mitigate this by using algorithms that are non-linear such as tree based ones. 4.1.1 Bias Variance Trade Off The issue above is part off a major concern in machine learning, described as the the bias variance trade off. Where bias describes how well a model fits in the training data, and variance how well it fits to the future/test data. In most cases there is a point where optimising the model for the training data will start to cause over-fitting. In other words, it will make the model very specific to that training sample and not generalised enough to fit future samples. In this chapter We will also look into alternatives to using the least square technique, in order to fit our line better, as well as other approaches to fitting linear-based models on non-linear problems. The reason we really want to dive deep and try to optimise a linear model in that extend, instead of just using a non-linear one is because of its interpretability. Linear models, in real-life scenarios are superior in solving interference problems (finding relationships between predictors and reaction). 4.2 Improved performance indicators (adjusted R-squared and alternatives) As we mentioned before the R-squared, for measuring the performance of our linear model will continuously increase with the addition of relevant or irrelevant predictors, resulting in overestimation of the models’ fitness. A very simple but popular approach in mitigating this, is the use of the adjusted R-squared instead, which adds a penalty for increased predictors in the formula. If we recall the formula for R-squared is \\(R^2 = 1- \\frac{sum squared error}{total variation}\\). For the adjusted R-squared we want this to be decreasing as the number of predictors are increasing. It is given by \\(R^2 = 1- \\frac{\\text{sum squared error/(n-d-1)}}{\\text{total variation/(n-1)}}\\), where n is the total number of samples and d is the degrees of freedom (total number of predictors -1). Why are we accounting for the number of samples? A simple explanation is that the more samples we have, the more confident we can be that the accuracy we measured on this bigger sample will be the closer for the total population. As the number of samples (\\(n\\)) increases , we can see that the adjusted R-squared also increases, as we would expect from our intuition.This increase however, is relevant to how many predictors we have (since the nominator is divided by \\(n-d-1\\)). For example, if we take the case where d&gt;n (we have more predictors than samples) the R-squared will decrease. Lets see more about the relationship between predictors and samples. 4.2.1 The curse of dimensionality This relationship between number of predictors and samples, is actually very crucial in measuring the performance of a linear model. Remember that each predictor is a dimension in the space which our samples are placed. The more dimensions that space has (the more predictors), the more our points will be spread out. If we were comparing the accuracy of two models with the same number of samples, but different number of predictors, the model for which we could be more confident on its accuracy, would be the one with the least number of predictors! For the extreme case where the number of predictors are equal or less than the number of samples, the linear model is useless, as it would result in extremely overconfident results. Let’s see an example. Let’s imagine with have two predictors (this means we have a 2 dimensional space drawn in the x and a y axis). We have an equal amount of samples (2 points) from which we will create our linear model, using the least squares fit. To draw a straight line on the x and y axis we only need 2 points anyway, so our line fits perfectly the training data, we have 0 sum of squared error and therefore our classic R-squared is 1. Now we add another 100 samples from the same population on that plot and we see that the sum of squared error for those new points to the previously perfectly fit line is massive! The problems caused by high dimensions is referred to us the curse of dimensionality. A high dimensional problem is usually one where number of predictors is close to or less than the number of samples. In order to perform linear regression on such problems we usually result in techniques for reducing the number of dimensions. We will look into those techniques later on. For now we need to understand how the adjusted R-squared attempts to be a better measurement for model fitness, by accounting for this relationship between number of predictors and number of samples used. The adjusted R-squared is given by R automatically when we request the summary statistics for our model, and you have probably already noticed it. library(MASS) library(ISLR) # Let&#39;s use our previous linear model of house values as a function of the # Boston dataset attributes lm.rm_fit &lt;- lm(medv~., data = Boston) # We can see that the R-squared and the adjusted R-squared are not too far off. # The adjusted number is less, as expected it has paid the price of using # multiple predictors, but we have enough samples to support most predictors. # If we started removing predictors of less significance, we would notice that # the adjusted R-squared and R-squared would be closer to each other. summary(lm.rm_fit) ## ## Call: ## lm(formula = medv ~ ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.595 -2.730 -0.518 1.777 26.199 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646e+01 5.103e+00 7.144 3.28e-12 *** ## crim -1.080e-01 3.286e-02 -3.287 0.001087 ** ## zn 4.642e-02 1.373e-02 3.382 0.000778 *** ## indus 2.056e-02 6.150e-02 0.334 0.738288 ## chas 2.687e+00 8.616e-01 3.118 0.001925 ** ## nox -1.777e+01 3.820e+00 -4.651 4.25e-06 *** ## rm 3.810e+00 4.179e-01 9.116 &lt; 2e-16 *** ## age 6.922e-04 1.321e-02 0.052 0.958229 ## dis -1.476e+00 1.995e-01 -7.398 6.01e-13 *** ## rad 3.060e-01 6.635e-02 4.613 5.07e-06 *** ## tax -1.233e-02 3.760e-03 -3.280 0.001112 ** ## ptratio -9.527e-01 1.308e-01 -7.283 1.31e-12 *** ## black 9.312e-03 2.686e-03 3.467 0.000573 *** ## lstat -5.248e-01 5.072e-02 -10.347 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.745 on 492 degrees of freedom ## Multiple R-squared: 0.7406, Adjusted R-squared: 0.7338 ## F-statistic: 108.1 on 13 and 492 DF, p-value: &lt; 2.2e-16 4.2.2 Alternatives Although the motivation behind the adjusted R-squared is logical and it is a very popular approach, it can not really be supported by any statistical theory. This why there are other alternatives available such as: Mallows’ \\(C_p\\), computed by: \\(C_p = \\frac{1}{n} (\\text{sum of squared error}+ 2d \\sigma^2)\\), where \\(d\\) is the degrees of freedom and \\(\\sigma^2\\) is the approximated, using the training sample, population variance. A high \\(C_p\\) measure means that the model is not a good fit. This approach tries to account for bias (over-fitting due to additional irrelevant predictors) by looking at how spread out the data is within those predictors (remember each predictor can be seen as a dimension). It uses the measure of uneducable error \\(\\epsilon\\) as a penalty, \\(\\epsilon = 2d \\sigma^2\\). Again the more the predictors increase (\\(d\\) will increase) the higher the penalty will be. As for the \\(\\sigma^2\\) we can think of it as a regulator for that penalty. \\(\\sigma^2\\) measures how spread out the data is, it makes sense that the more variation there is, the more spread out the data will be and the more the error will increase. While when \\(n\\) increases \\(C_p\\) decreases, indicating better performance when more samples are available. Since \\(\\sigma^2\\) is estimated using the sample’s \\(\\sigma^2\\), this criterium requires enough data to get a good approximation and it will not perform well using small datasets. Furthermore, we have mentioned that the \\(\\epsilon\\) is underestimated when there are complications in the relationship of the predictors and the reaction ( e.g. multicollinearity), in such cases \\(C_p\\) will also not provide useful insight. Akaike information criterion (AIC), computed by: \\(AIC = 2k - 2ln(L)\\), where k is the number of predictor plus one, and L is the maximum value of the likelihood function for the model. Again a small AIC, like a small \\(C_p\\) indicated a good fit. As we know the maximum likelihood function is a way to find optimum fit. The higher that number is the more fit our model will be, resulting in a smaller AIC. However we still have a penalty related to how many predictors are used, this is the role of \\(2k\\) in the equation.We will not go into too much details, but it has been found that AIC has an overall good performance in any model and data available, and quite often outperforms other methods when used for choosing predictors and finding the best model. Bayesian information criterion (BIC), computed by: \\(BIC = ln(n)k - 2ln(L)\\) This is very similar to AIC, with the main deference been that a heavier penalty is given for models with increasing predictors, resulting in defining optimal models those with less predictors. 4.3 Cross Validation We now have a way to account for over-fitting, when accessing our model. However, we are still only assessing our model on how well it is performing on the same data that it was trained with. We do not have any way of assessing how it would do when new data comes in. You may think that this is a simple thing to do, we can split our data (like we did with confusion matrix) in a training sample (70-80%), used to train the model and a (20-30%) testing data, only used to test that trained model. We then just need to calculate the R-squared, adjusted R-squared, \\(C_p\\) or any other chosen criterion on the trained model, for that testing data (this method is called validation). We now have a performance measurement for a testing data! Yes but if we split the data, how do we split it. Any random 70-80% sample will result in different estimates for our coefficients, since it will randomly contain different values for each predictors/reaction. Similarly any random 20-30% sample will result in varying measurements for our performance criteria. Those variations can be quite significant, especially when we do not have enough samples. If we want more accuracy we need to take more measurements. This is where a new method of assessment comes in, Cross-Validation. Instead of splitting the data in two blocks, we will split it in \\(k\\) number of blocks containing an equal portion of the data. For each of those blocks we will: Use all the remaining data (not contained in that block) to train the model Use the data contained in that block to test the trained model. We calculate the chosen performance indicator ( e.g. sum of squared error (residuals)) for the test data (found in the block). We will repeat that for all the \\(k\\) blocks. We then take the average performance indicator from all the \\(k\\) indicators. This is our CV value. We can choose whatever value for k we want, to perform what is called k-fold cross validation. It is usually advised to choose 5-fold or 10-fold depending on how much data and resources are available. Apart from assisting in choosing the optimum coefficients for linear models, cross validation is also a great to way to compare the performance of different machine learning algorithms. We can perform cross validation in any model, in a similar manner to what was previously described. It also has other applications, which we will look at later on. I would recommend cross validation when you need to compare the performance of different machine learning algorithms on the same problem, when you do not have a lot of samples to simply trust the other methods, if computational power and complexity are not an issue, or if performance is crucial for your model. 4.3.1 Cross Validation in action For this example, which is also given by the book, we will use another dataset made by ISLR, the Auto dataset. The dataset consists of various car’s consumption of fuel per mile and a few features that could be related with this, such as their weight, horsepower and number of cylinder’s. We will try an analyse this relationship using a linear model (We will only use horsepower to keep things simple and to the point). We will use k-fold validation to assess our model’s performance. In particular we will look at two cases for CV: LOOCV (Leave One Out Cross Validation), which is just an extreme case of normal CV where the number of block (k) selected are equal to the number of samples available. In other words each time our model will be trained using all the samples apart from one, that one will be used to asses it. This is a clearly computational expensive method, but might be useful if we have very few samples to train our model with. 10-k fold validation, where k = 10 # This library has functions that will allow us to perform k-fold validation install.packages(&#39;boot&#39;) library(boot) # Setting the seed to the same number will reproduce &#39;randomness&#39;. # We need randomness in splitting the data to our blocks for k-fold cross # validation. set.seed(24) # gml() function without additional parameter will produce a linear model the # same way lm() would. # gml() however works with the k-fold function we need to use later. glm.fit &lt;- glm(mpg~horsepower, data = Auto) # Perform cv using the cv.gml function, if you don&#39;t specify how many blocks you # want to split your data to, the function performs LOOCV by default. cv &lt;- cv.glm(Auto, glm.fit) # The delta is parameter from above that contains the average error calculated # after performing CV for each sample. cv$delta[1] ## [1] 24.23151 # On its own an error does really show us something, it is valuable when we # compare it with errors from other models so that we can see which model is # doing better. # To illustrate this, we will try and compare the errors given when we perform # polynomial regression. Using a for() loop we will perform CV on mpg as a # function of: # horsepower, horsepower^2, horsepower^3, horsepower^4, horsepower^5. # Initialise a vector of 5 elements that will contain the errors for each model cv.error = c(1, 2, 3, 4, 5) # Loop 5 times, each time for every model and add the error calculated to # cv.error for (i in 1:5){ glm.fit = glm(mpg~poly(horsepower, i), data = Auto) cv.error[i] = cv.glm(Auto, glm.fit)$delta[1] } # If we print out the errors we can see a significant improvement from the # linear to the quadratic model (horsepower^2) cv.error ## [1] 24.23151 19.24821 19.33498 19.42443 19.03321 # Now we will repeat the evaluation of the same models, but using a 10-k fold CV cv.error = c(1, 2, 3, 4, 5) for (i in 1:5){ glm.fit = glm(mpg~poly(horsepower, i), data = Auto) # The only additional parameter is K, which we chose to set to 10 since we are # looking to perform a 10-k fold cv. cv.error[i] = cv.glm(Auto, glm.fit, K = 10)$delta[1] } # We can see that the results are quite similar (also the 10-k fold completed a # lot faster, this is important especially if we had more data) cv.error ## [1] 24.26469 19.35152 19.47426 19.42381 19.15667 4.4 Selecting the optimal predictors for the model We have seen how to measure the model’s fitness and account for over-fitting (using any or combinations of the methods we have been discussing), so we can compare various models with different sets of predictors and see which ones are more effective. In this section we will present various automated approaches for performing feature selection, in order to uncover which set of predictors will yield the optimal result. As we know, we want our model to have the lowest possible test error (which we can measure with cross validation) as well as a good balance between under-fitting and over-fitting (which we can measure using criterion such as AIC or BIC). So one could produce various models for his problem, using a deferent subset of the predictors each time. He could them cross validate each or measure their AIC and choose the one with the smallest error. A more methodical way of doing so is the best subset selection approach. 4.4.1 Best subset selection In this approach we will fit a models for every single possible combination of predictors, measure their performance and choose the optimal one. We will do so in a more organised and efficient way, while choosing the right performance criteria in each step of the selection process. Steps: First we define the ‘null Model’, this is a model of 0 predictors \\(M_0\\), that simply uses the total average of the reaction to give a prediction (this is necessary since all of the predictors could be irreverent, it acts as a measurement of comparison) Then we take every singe predictor and make a model containing only that predictor and the reaction. We measure their performance using the R-squared. The best out of them is called the \\(M_1\\) model, since it only contains a single predictor. Then we take every combination of 2 predictors and fit a model out of each pair, we measure all of their performance using the R-squared, the best out of them is called the \\(M_2\\) model. Then we take every combination of 3 predictors and fit a model out of each 3 pairs, we measure all of their performance using the R-squared, the best out of them is called the \\(M_3\\) model. We repeat until there are no more combinations possible, we have reached the total number of predictors, let’s call this the \\(M_p\\) model. We then take all our best models for each combination \\(M_0\\), \\(M_1\\), \\(M_2\\), \\(M_3\\)…\\(M_p\\) and perform cross validation or measure their adjusted R-square, the AIC or BIC criterion (or a combination of those performance indicators). And of course, we choose the one with the least error. install.packages(&quot;leaps&quot;) # leaps containS functions for subset selection library(leaps) # This lab, is again by ISLR, and uses the dataset Hitters. It contains various # statistics for the performance of Baseball players such as number of Hits and # Home runs, as well as their salary. We will try and fit a linear model that # studies the relationship between their performance and salary. library(ISLR) # The salary field for some of the players is empty. We will remove those # players from the dataset, as they are not valuable and will cause issues when # attempting to make the model. Hitters &lt;- na.omit(Hitters) # By default the regsubsets() selections looks up to pairs of 8, you can change # this by adding the following parameter: nvmax = &lt;number&gt; # We chose 19 since we have 19 predictors regfit.full &lt;- regsubsets(Salary~., Hitters, nvmax = 19 ) # The out put show with an asterix the predictors that yield the optimal model # for each pair. summary(regfit.full) ## Subset selection object ## Call: regsubsets.formula(Salary ~ ., Hitters, nvmax = 19) ## 19 Variables (and intercept) ## Forced in Forced out ## AtBat FALSE FALSE ## Hits FALSE FALSE ## HmRun FALSE FALSE ## Runs FALSE FALSE ## RBI FALSE FALSE ## Walks FALSE FALSE ## Years FALSE FALSE ## CAtBat FALSE FALSE ## CHits FALSE FALSE ## CHmRun FALSE FALSE ## CRuns FALSE FALSE ## CRBI FALSE FALSE ## CWalks FALSE FALSE ## LeagueN FALSE FALSE ## DivisionW FALSE FALSE ## PutOuts FALSE FALSE ## Assists FALSE FALSE ## Errors FALSE FALSE ## NewLeagueN FALSE FALSE ## 1 subsets of each size up to 19 ## Selection Algorithm: exhaustive ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 17 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 18 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 19 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN ## 1 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 17 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 18 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 19 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; # You can view performance indicators for each model such as: # adjusted R-squared summary(regfit.full)$adjr2 ## [1] 0.3188503 0.4208024 0.4450753 0.4672734 0.4808971 0.4972001 0.5007849 ## [8] 0.5137083 0.5180572 0.5222606 0.5225706 0.5217245 0.5206736 0.5195431 ## [15] 0.5178661 0.5162219 0.5144464 0.5126097 0.5106270 # The C_p value summary(regfit.full)$cp ## [1] 104.281319 50.723090 38.693127 27.856220 21.613011 14.023870 ## [7] 13.128474 7.400719 6.158685 5.009317 5.874113 7.330766 ## [13] 8.888112 10.481576 12.346193 14.187546 16.087831 18.011425 ## [19] 20.000000 # BIC criterion summary(regfit.full)$bic ## [1] -90.84637 -128.92622 -135.62693 -141.80892 -144.07143 -147.91690 ## [7] -145.25594 -147.61525 -145.44316 -143.21651 -138.86077 -133.87283 ## [13] -128.77759 -123.64420 -118.21832 -112.81768 -107.35339 -101.86391 ## [19] -96.30412 # Using that you can choose your optimal model. # For example the model with highest adjuster R-squared which.max(summary(regfit.full)$adjr2) ## [1] 11 # Or the model with lowest C_p which.min(summary(regfit.full)$cp) ## [1] 10 # Or if we want to combine various criteria, it may be useful to visualise par(mfrow = c(2, 2)) plot(summary(regfit.full)$adjr2, xlab = &quot;Number of virables contained in the model&quot;, ylab=&quot;Adjusted R-squared&quot;) plot(summary(regfit.full)$cp, xlab = &quot;Number of virables contained in the model&quot;, ylab=&quot;C_p&quot;) plot(summary(regfit.full)$bic, xlab = &quot;Number of virables contained in the model&quot;, ylab=&quot;BIC&quot;) # We can see that probably something close to 10 would be optimal, let&#39;s see the # selected 10 variables and their coefficients. coef(regfit.full, 10) ## (Intercept) AtBat Hits Walks CAtBat ## 162.5354420 -2.1686501 6.9180175 5.7732246 -0.1300798 ## CRuns CRBI CWalks DivisionW PutOuts ## 1.4082490 0.7743122 -0.8308264 -112.3800575 0.2973726 ## Assists ## 0.2831680 library(leaps) # Please know the library for regsubsets did not have a build in function for # predict, so we had to built one ourselves. # Here is the code: predict.regsubsets = function(object, newdata, id, ...) { form = as.formula(object$call[[2]]) mat = model.matrix(form, newdata) coefi = coef(object, id = id) mat[, names(coefi)] %*% coefi } # Another way to select between the best models from each pair # (m0, m1, m2...mp), instead of using criteria such as adjusted R-squared, would # be cross validation. We will see how this could be done here. We choose to # perform a 10-k fold cv. # Let&#39;s set the seed again to ensure someone can repeat the test and get the # same &#39;randomness&#39;. set.seed(1) # To perform 10-k fold cross validation, we first need to split the data in 10 # folds (10 equal sized blocks). k &lt;- 10 # The way we split the data is we make a vector of size equal to nrow(Hitters) # (the rows contained in the dataset Hitters), and each row will be assigned to # a fold from 1 to 10 (sample 1:10). Since we want multiple rows to be part of # the same folds we set replace = TRUE. Also because we haven&#39;t defined any # particular split ration, by default the split will be equal across the folds. folds &lt;- sample(1:k, nrow(Hitters), replace = TRUE) # We can see that each row of the Hitters dataset has been assigned to one of # the 10 k folds, in an equal manner. folds ## [1] 3 4 6 10 3 9 10 7 7 1 3 2 7 4 8 5 8 10 4 8 10 3 7 ## [24] 2 3 4 1 4 9 4 5 6 5 2 9 7 8 2 8 5 9 7 8 6 6 8 ## [47] 1 5 8 7 5 9 5 3 1 1 4 6 7 5 10 3 5 4 7 3 5 8 1 ## [70] 9 4 9 4 4 5 9 9 4 8 10 5 8 4 4 8 3 8 2 3 2 3 1 ## [93] 7 9 8 8 5 5 9 7 7 4 3 10 7 3 2 5 10 6 10 8 4 5 2 ## [116] 1 8 2 5 7 10 5 5 2 8 5 6 3 3 6 6 1 1 7 10 6 6 6 ## [139] 10 6 7 7 3 3 8 5 2 8 2 9 7 6 4 5 6 2 6 1 3 3 3 ## [162] 9 5 8 9 5 1 4 8 4 7 9 9 4 4 9 7 8 7 10 3 2 9 6 ## [185] 9 2 8 8 10 6 8 4 2 10 3 6 2 9 4 8 3 3 6 3 2 6 6 ## [208] 2 3 8 10 2 8 10 9 4 7 10 10 4 3 2 4 6 10 6 3 1 5 9 ## [231] 4 2 4 7 4 7 7 6 5 5 4 6 10 2 5 3 5 2 5 10 8 10 5 ## [254] 7 5 2 3 5 4 10 6 4 3 # We also need a variable that will store all the calculated errors. In this # case we will have 10 models (one for each fold) for every 19 different sets of # variables (M0, M1, M2, ....M19 after performing best subset selection). If we # want to be able to keep track of which model came from which fold and subset # we need a matrix instead of a simple vector. cv.errors &lt;- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19))) # We will fill this with the actual errors later cv.errors ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## [1,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [2,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [3,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [4,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [5,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [6,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [7,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [8,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [9,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA ## [10,] NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA NA # Now we need to create 10 models for each of the 19 subsets, using a different # combination of 9 out of the 10 blocks each time (leaving one for testing). We # will call the testing one j, in order to be able to distinguish it and only # use it for testing. Let&#39;s see this in practice: # For every j in k (we define j as a single fold. First it will be all the rows # who where assigned with 1, then 2, 3 ...) for (j in 1:k) { # Create the 19 models using all the data apart from j (!=j) best.fit &lt;- regsubsets(Salary ~., data = Hitters[folds!=j,], nvmax = 19) # Now all we have to do, is for each 10 sets of models with the same params # calculate the average testing error. for (i in 1:19) { # First we need to find out the values for salary that each model would # predict for our testing j fold. pred = predict(best.fit, Hitters[folds == j, ], id = i) # Then we measure the squared difference of the actual salary from the # predicted for each point, and we store their mean in our matrix. cv.errors[j, i] &lt;- mean((Hitters$Salary[folds==j]-pred)^2) } } # We can see the difference of the mean squared errors from the predicted to the # actual points. That is for every of the 10 folds for each of 19 subsets used. cv.errors ## 1 2 3 4 5 6 ## [1,] 187479.08 141652.61 163000.36 169584.40 141745.39 151086.36 ## [2,] 96953.41 63783.33 85037.65 76643.17 64943.58 56414.96 ## [3,] 165455.17 167628.28 166950.43 152446.17 156473.24 135551.12 ## [4,] 124448.91 110672.67 107993.98 113989.64 108523.54 92925.54 ## [5,] 136168.29 79595.09 86881.88 94404.06 89153.27 83111.09 ## [6,] 171886.20 120892.96 120879.58 106957.31 100767.73 89494.38 ## [7,] 56375.90 74835.19 72726.96 59493.96 64024.85 59914.20 ## [8,] 93744.51 85579.47 98227.05 109847.35 100709.25 88934.97 ## [9,] 421669.62 454728.90 437024.28 419721.20 427986.39 401473.33 ## [10,] 146753.76 102599.22 192447.51 208506.12 214085.78 224120.38 ## 7 8 9 10 11 12 ## [1,] 193584.17 144806.44 159388.10 138585.25 140047.07 158928.92 ## [2,] 63233.49 63054.88 60503.10 60213.51 58210.21 57939.91 ## [3,] 137609.30 146028.36 131999.41 122733.87 127967.69 129804.19 ## [4,] 104522.24 96227.18 93363.36 96084.53 99397.85 100151.19 ## [5,] 86412.18 77319.95 80439.75 75912.55 81680.13 83861.19 ## [6,] 94093.52 86104.48 84884.10 80575.26 80155.27 75768.73 ## [7,] 62942.94 60371.85 61436.77 62082.63 66155.09 65960.47 ## [8,] 90779.58 77151.69 75016.23 71782.40 76971.60 77696.55 ## [9,] 396247.58 381851.15 369574.22 376137.45 373544.77 382668.48 ## [10,] 214037.26 169160.95 177991.11 169239.17 147408.48 149955.85 ## 13 14 15 16 17 18 ## [1,] 161322.76 155152.28 153394.07 153336.85 153069.00 152838.76 ## [2,] 59975.07 58629.57 58961.90 58757.55 58570.71 58890.03 ## [3,] 133746.86 135748.87 137937.17 140321.51 141302.29 140985.80 ## [4,] 103073.96 106622.46 106211.72 107797.54 106288.67 106913.18 ## [5,] 85111.01 84901.63 82829.44 84923.57 83994.95 84184.48 ## [6,] 76927.44 76529.74 78219.76 78256.23 77973.40 79151.81 ## [7,] 66310.58 70079.10 69553.50 68242.10 68114.27 67961.32 ## [8,] 78460.91 81107.16 82431.25 82213.66 81958.75 81893.97 ## [9,] 375284.60 376527.06 374706.25 372917.91 371622.53 373745.20 ## [10,] 194397.12 194448.21 174012.18 172060.78 184614.12 184397.75 ## 19 ## [1,] 153197.11 ## [2,] 58949.25 ## [3,] 140392.48 ## [4,] 106919.66 ## [5,] 84284.62 ## [6,] 78988.92 ## [7,] 67943.62 ## [8,] 81848.89 ## [9,] 372365.67 ## [10,] 183156.97 # We want to get one average value for each of 19 models so we can compare then # and choose the optimal. # The function apply will help us with that. mean.cv.error &lt;- apply(cv.errors, 2, mean) mean.cv.error ## 1 2 3 4 5 6 7 8 ## 160093.5 140196.8 153117.0 151159.3 146841.3 138302.6 144346.2 130207.7 ## 9 10 11 12 13 14 15 16 ## 129459.6 125334.7 125153.8 128273.5 133461.0 133974.6 131825.7 131882.8 ## 17 18 19 ## 132750.9 133096.2 132804.7 # Lets plot them to see which models have the lowest errors par(mfrow = c(1, 1)) plot(mean.cv.error, type=&#39;b&#39;) # We can see that 10 and 11 are the smallest ones ( with 11 been the smallest), # which is quite close to what we got from using criteria like the adjusted # R-squared. # Note: # An alternative to CV, that would be more computationally advantageous, is # simple validation. We have mentioned that when introducing cross validation. # Basically we only split the data in two, a training and testing data set. We # create all our models until Mp (by best selection or other methods) using the # training data. We then use the testing data to measure the sum of squared # errors, or R-squared of that test data. We choose the one with the smallest # test error. Of course this calculation would be highly dependant on how the # data was split, and there is risk in not getting accurate measurements for the # error. As you can see this is a very inclusive process, which is very useful when you have only a few predictors to choose from. It is not however, computationally light. For \\(p\\) number of predictors, we have \\(p!\\) possible combinations. So even if we have something like 10 features, we end up with having to train 3628800 models!This is why the following approach was developed. 4.4.2 Stepwise Selection In this approach we either start from the null model and incrementally add predictors (forward), or start with all the predictors and incrementally reduce them (backward).The computational advantages from reducing the amount of models required comes from maintaining the previously selected best model, and only adding the most valuable predictor from the remaining ones, per iteration. Forward Stepwise Selection Again, we will define the ‘null Model’, this is a model of 0 predictors \\(M_0\\), that simply uses the total average of the reaction to give a prediction And start with considering all models that use one single predictor. Measure their R-squared and choose the best one, this is the \\(M_1\\) (up to know its the same as above) Now we will start, one by one adding more predictors to \\(M_1\\), each time measuring their performance and choosing to add the predictor that is adding the most value (increases the R-squared the most). First we create the \\(M_2\\) model, which has two predictors. The one predictor comes from \\(M_1\\) and the other will be selected form the remaining ones. To select that we need to add each remaining predictor to \\(M_1\\) and choose the one that yields the highest R-squared. Similarly we create the \\(M_3\\) model. We keep the two predictors from \\(M_2\\) and select another one from the remaining, which will add the most value (as found from fitting a model for all the remaining predictors and measuring their R-squared) We repeat until we have used all the predictors in the \\(M_p\\) model Just like before, we will choose to cross validate \\(M_0\\), \\(M_1\\), \\(M_2\\), \\(M_3\\)…\\(M_p\\), or measure some other performance criteria that accounts for over-fitting (or take a combination). Then we can choose the optimal model. forward &lt;- regsubsets(Salary~., data = Hitters, nvmax = 19, method = &quot;forward&quot;) summary(forward) ## Subset selection object ## Call: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = &quot;forward&quot;) ## 19 Variables (and intercept) ## Forced in Forced out ## AtBat FALSE FALSE ## Hits FALSE FALSE ## HmRun FALSE FALSE ## Runs FALSE FALSE ## RBI FALSE FALSE ## Walks FALSE FALSE ## Years FALSE FALSE ## CAtBat FALSE FALSE ## CHits FALSE FALSE ## CHmRun FALSE FALSE ## CRuns FALSE FALSE ## CRBI FALSE FALSE ## CWalks FALSE FALSE ## LeagueN FALSE FALSE ## DivisionW FALSE FALSE ## PutOuts FALSE FALSE ## Assists FALSE FALSE ## Errors FALSE FALSE ## NewLeagueN FALSE FALSE ## 1 subsets of each size up to 19 ## Selection Algorithm: forward ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 17 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 18 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 19 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN ## 1 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 17 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 18 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 19 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; Backward Stepwise Selection This time we start from the \\(M_p\\) model, which contains all the predictors We remove one predictor from \\(M_p\\) and measure the R-squared. We do this for all the predictors in \\(M_p\\). Now we have measurements for all possible combinations with predictors p-1. We choose the one with the highest R-squared, this is the \\(M_p-1\\) model. Note that since we removed a predictor the R-squared of the new model will be smaller (we have explained that additional predictors will improve these measurement even if they are irrelevant). However, this does not mean it is not potentially a better model, when we compute its BIC, AIC or adjusted R-squared or if we choose to cross validate, the issues of over-fitting will taken into account. We repeat the process until we reach \\(M_0\\), until all the predictors have been removed Like always, we will choose to cross validate \\(M_p\\), \\(M_p-1\\), \\(M_p-2\\), \\(M_p-3\\)….\\(M_0\\), or measure some other performance criteria that accounts for over-fitting (or take a combination). Then we can choose the optimal model. backward &lt;- regsubsets(Salary~., data = Hitters, nvmax = 19, method = &quot;backward&quot;) summary(backward) ## Subset selection object ## Call: regsubsets.formula(Salary ~ ., data = Hitters, nvmax = 19, method = &quot;backward&quot;) ## 19 Variables (and intercept) ## Forced in Forced out ## AtBat FALSE FALSE ## Hits FALSE FALSE ## HmRun FALSE FALSE ## Runs FALSE FALSE ## RBI FALSE FALSE ## Walks FALSE FALSE ## Years FALSE FALSE ## CAtBat FALSE FALSE ## CHits FALSE FALSE ## CHmRun FALSE FALSE ## CRuns FALSE FALSE ## CRBI FALSE FALSE ## CWalks FALSE FALSE ## LeagueN FALSE FALSE ## DivisionW FALSE FALSE ## PutOuts FALSE FALSE ## Assists FALSE FALSE ## Errors FALSE FALSE ## NewLeagueN FALSE FALSE ## 1 subsets of each size up to 19 ## Selection Algorithm: backward ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 2 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 17 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 18 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 19 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## CRBI CWalks LeagueN DivisionW PutOuts Assists Errors NewLeagueN ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 17 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 18 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 19 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; Forward and Backward selection will require a lot less computational power for larger p. However, since they do not check all possible combinations there is a risk that the concluding model may be not the most optimal. When having to choose between forward and backward, you should consider how much data is available. If you do not have samples that are greater than your predictors, backwards propagation will not perform well. Since it starts with a model using all the predictors, a lot of issues with dimensionality will occur (as we previously discussed). You may also want to consider whether you think most of your predictors are valuable (i would go with backwards) or only some of them are (I would go with forward). You may be able to stop the selection if at some point you do not see improvements, in order to be more resource and time efficient. If you want some middle ground between computational efficiency and getting closer to the optimal model, you can try a hybrid of forward and backward stepwise selection. In this case you would start in a similar manner to forward selection, however after adding a new predictor each iteration (or in some of the iterations) you could also check to see which predictor you could remove. By constantly adding and removing predictors you get to see more variations of the models. This would start approaching the best subset selection while at the same time keep the focus only on incremental changes that offer the most value. 4.5 Shrinkage/Regularisation methods Such methods are used when we want to focus on reducing the variance of a linear mode (remember variance is associated with over-fitting, when our model has a lower training error but high testing error). Some reasons why you need to further reduce variance could be: You have only a few training samples. As we discussed in ‘curse of dimensionality’ when we have only a few samples in relation to how much predictors we have, our models will tend to be overconfident and do well with training data but not testing data. Even in the case of having less samples than predictors, those methods will be able to provide a working model. You are more interested in the predictive abilities of your model rather than its inference. In this case you are more interested on reducing the variance rather than the bias of the model. You want your model to do well on testing/new data. Multicollinearity is present on your model. Correlation between predictors causes your model to take into account the same effects (from predictors) multiple times, as a result your model is overconfident. You will have increased measurement of training error, but again a significantly worse measurement for testing error. Those methods are very similar to least squares, as we know it, however they will try to ‘shrink’ the estimates of the coefficients (move them more towards zero). Why would that cause a reduction in variance? If we remember the linear equation \\(y= b_0 + b_1x_1 + b_2x_2..\\), where \\(b_1\\), \\(b_2\\) are our coefficients, the more they get closer to zero the more \\(x_1\\) and \\(x_2\\) will tend to zero. In other words the effect of the predictors \\(x_1\\) and \\(x_2\\) (as it is measured from the given test sample) will have less of an effect to \\(y\\) (the reaction). So if our current sample will have less of an effect in defining the line, then that line is more generalised and (since it is less determined by just one sample) potentially it will perform better on testing/new data. There are a few ways to shrink (or as it also referred to as ‘regularise’) the coefficients, which we will be looking at in this chapter. 4.5.1 Ridge regression As we know this methods are all close to linear regression. Ridge regression will indeed, try to minimise the function of the leat squared error. However, it also wants to minimise the coefficients for our predictors. So we will also add the coefficient estimations to that function as a penalty. The weight that this penalty will have is going to be specific to the particular problem, but is generally represented as \\(\\lambda\\). In other words least regression attempts to minimise: \\(\\sum RSS +\\lambda (b_1^2 + b_2^2 + ...)\\) To calculate the \\(\\lambda\\) we use cross validation, we simply try out different values for it, and pick the one that yields the model with the least error. Generally, the closer the \\(\\lambda\\) is to zero, the less of an affect the penalty will have (e.g. if it is zero we are basically just minimising the least squares), while the grater it is the more the coefficients will tend to zero. # This library has the function glmnet, which allows us to perform ridge # regression and other such methods install.packages(&quot;glmnet&quot;) library(glmnet) ## Loading required package: Matrix ## Loading required package: foreach ## Loaded glmnet 2.0-16 # For our ridge regression we will need to choose some values for lambda, in # order to compare the models they yield and find out the optimal value for # lambda. # A standard inclusive set, that we will use, is from 10^-2 until 10^10 grid &lt;- 10^seq(10, -2, length = 100) # This function requires the parameter given a bit deferent that the usual y~. # For the predictors it wants them as a matrix, model.matrix() turns then in the # required format and also deals with quantitative features. x &lt;- model.matrix(Salary~.,Hitters)[,-1] # The reaction needs to also be clearly defined y &lt;- Hitters$Salary # Now that we have everything we can perform ridge regression: # alpha means we want to perform ridge regression (glmnet also perform other # methods) ridge.model &lt;- glmnet(x, y, alpha = 0, lambda = grid) # We can see the coefficients of a model with certain lambda using the # following: # This returns the coefficients with lambda = 50 (the intercept and the # following 19 coefficients) coef(ridge.model)[, 50] ## (Intercept) AtBat Hits HmRun Runs ## 407.356050200 0.036957182 0.138180344 0.524629976 0.230701523 ## RBI Walks Years CAtBat CHits ## 0.239841459 0.289618741 1.107702929 0.003131815 0.011653637 ## CHmRun CRuns CRBI CWalks LeagueN ## 0.087545670 0.023379882 0.024138320 0.025015421 0.085028114 ## DivisionW PutOuts Assists Errors NewLeagueN ## -6.215440973 0.016482577 0.002612988 -0.020502690 0.301433531 # Or using the predict() function predict(ridge.model, s = 50, type=&quot;coefficients&quot;)[1:20] ## [1] 4.876610e+01 -3.580999e-01 1.969359e+00 -1.278248e+00 1.145892e+00 ## [6] 8.038292e-01 2.716186e+00 -6.218319e+00 5.447837e-03 1.064895e-01 ## [11] 6.244860e-01 2.214985e-01 2.186914e-01 -1.500245e-01 4.592589e+01 ## [16] -1.182011e+02 2.502322e-01 1.215665e-01 -3.278600e+00 -9.496680e+00 # In order to choose which lambda to use, we need to follow a similar procedure # to what we have done previously. We would split our data in two sets, training # and testing. We would use the training to create our models and the test to # measure their testing mean squared error. We would then pick the one with the # least error. Or we could manually try out different lambdas and see at which # lambda we are getting best results. We should also compare our results to # linear regression where lambda simple equals zero. Let&#39;s see an example: set.seed(1) # Remember x is a matrix of all rows containing values of the predictors, we # split their indexes in 80%-20% for training and testing. train &lt;- sample(1:nrow(x), nrow(x)*0.8) train ## [1] 70 98 150 237 53 232 243 170 161 16 259 45 173 97 192 124 178 ## [18] 245 94 190 228 52 158 31 64 92 4 91 205 80 113 140 115 43 ## [35] 244 153 181 25 163 93 184 144 174 122 117 251 6 104 241 149 102 ## [52] 183 224 242 15 21 66 107 136 83 186 60 211 67 130 210 95 151 ## [69] 17 256 207 162 200 239 236 168 249 73 222 177 234 199 203 59 235 ## [86] 37 126 22 230 226 42 11 110 214 132 134 77 69 188 100 206 58 ## [103] 44 159 101 34 208 75 185 201 261 112 54 65 23 2 106 254 257 ## [120] 154 142 71 166 221 105 63 143 29 240 212 167 172 5 84 120 133 ## [137] 72 191 248 138 182 74 179 135 87 196 157 119 13 99 263 125 247 ## [154] 50 55 20 57 8 30 194 139 238 46 78 88 41 7 33 141 32 ## [171] 180 164 213 36 215 79 225 229 198 76 258 146 127 262 233 209 155 ## [188] 56 137 165 85 121 147 145 108 220 156 123 219 51 195 14 246 152 ## [205] 169 129 96 223 189 39 test &lt;- (-train) # This is the indexes of the reaction attribute for the test data. y.test = y[test] # Set up a vector to store mean squared error for each model for each value of # lambda. # In this case we have selected to start measuring the perfomance of various # lambda values, that have a siginificant distance from each other. We can then # see which performs better out of them and then make even more trials with # numbers close to that (notice how one of the values of lambda is zero. This # will result in performing simple least squares (linear regression). It is # useful to include such a model, in the comparison, to ensure ridge regression # is actually optimal for our problem) errors &lt;- c(0.2, 0, 1, 5, 10, 50, 100) # Now we can perform ridge regression just like before, using only the training # data. ridge.model &lt;- glmnet(x[train,], y[train], alpha = 0, lambda = grid) # And of each of the models with the lambda value we have chosen we will measure # the mean squared error. for (i in 1:7) { pred &lt;- predict(ridge.model, s = errors[i], newx = x[test,]) mserror &lt;- mean((pred - y.test)^2) errors[i] &lt;- mserror } errors ## [1] 66185.18 66082.36 66698.36 68681.08 70083.97 73561.11 74977.37 # We see that a large lambda seems to be improving the error (7 is the index for # 100) which.min(errors) ## [1] 2 # Let&#39;s see the suggested coefficients for lambda = 100 coef(ridge.model)[, 100] ## (Intercept) AtBat Hits HmRun Runs ## 198.9362665 -1.8765543 5.9755878 3.5756152 -0.8989841 ## RBI Walks Years CAtBat CHits ## -0.8334690 7.0608227 -11.7279516 -0.1450989 0.2072647 ## CHmRun CRuns CRBI CWalks LeagueN ## 0.2510835 1.5068197 0.5427306 -1.0010594 71.6000255 ## DivisionW PutOuts Assists Errors NewLeagueN ## -140.2565292 0.2298606 0.3043727 -1.1183827 -1.3603144 We noticed that even with such a large lambda the coefficients have been minimised but none of them is zero. Effectively what this means is that we cannot exclude any predictor. As we know, having less predictors could potentially improve our model’s predictive accuracy and if the coefficients are very small we have to consider if it is worth keeping the relative predictors. This is why the following method, LASSO was developed. It works very similar to ridge regression, however it can produce models coefficients equal to zero. Note: in this example we looking at linear regression. However, ridge regression and the following methods, can also be applied to logistic regression. The only difference would be that we would be looking to minimise the odds of the likelihood, instead of the mean squared error (plus of course, the estimates for the coefficients). 4.5.2 Lasso regression The lasso regression tries to minimise the following function: \\(\\sum RSS +\\lambda |b_1| + |b_2| + ...\\) This is very similar to Ridge regression, as we have already mentioned. Lasso minimises the absolute value instead of the squares of all of the coefficients. The lambda is also found in the same manner, using cross validation. The reason you would choose Lasso over Ridge regression, is if you where looking to perform feature selection. That is, if you knew that some of the attributes in your dataset are not actually useful. Unlike Ridge, Lasso can result in coefficients with estimated value of zero. set.seed(1) # alpha = 1, gives us the Lasso regression. lasso &lt;- glmnet(x[train,], y[train], alpha = 1, lambda = grid) # This time we will perform cross validation to find the best lambda using the # build in cv.glmnet function. cv.out &lt;- cv.glmnet(x[train,], y[train], alpha = 1) # Get the lambda with least mean squared error. Bestlambda &lt;- cv.out$lambda.min Bestlambda ## [1] 2.898582 # Calculate the error. pred &lt;- predict(lasso, s = Bestlambda, newx =x[test,]) mean((pred- y.test)^2) ## [1] 69073.69 # View the coefficients for the model produced using the optimal lambda. # First we should make a model using all the data, since we now know the lambda. model &lt;- glmnet(x, y, alpha = 1, lambda = grid) predict(model, type=&quot;coefficients&quot;, s = Bestlambda) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 118.2619108 ## AtBat -1.4924718 ## Hits 5.5443278 ## HmRun . ## Runs . ## RBI . ## Walks 4.6285657 ## Years -9.1334086 ## CAtBat . ## CHits . ## CHmRun 0.4924698 ## CRuns 0.6382992 ## CRBI 0.3935094 ## CWalks -0.5048902 ## LeagueN 31.7149476 ## DivisionW -119.2049416 ## PutOuts 0.2708047 ## Assists 0.1620721 ## Errors -1.9481096 ## NewLeagueN . As expected a couple of the coefficients were estimated at zero, reducing the number of predictors in our model. We have seen an alternative regularisation method for estimating coefficients when we know most of our features are useful (ridge regression) and an alternative for when we suspect some of them are not useful (lasso). What if we have no idea if our features are useful or not, what if we have so many features that we cannot know? In this case you would use the last option elastic net regression which is basically a hybrid of ridge and lasso regression. 4.5.3 Elastic Net Regression The Elastic Net regression tries to minimise the following function: \\(\\sum RSS +\\lambda_1 |b_1| + |b_2| + ...+\\lambda_2 (b_1^2 + b_2^2 + ...)\\) It is clear that this is a combination of ridge and lasso regression. It tries to combine the benefits of the two and is usually used for the case where too many features are available, and we cannot distinguishes if they are useful or not. Elastic net regression is also found to tackle multicollinearity more effectively. The optimal values for \\(\\lambda_1\\) and \\(\\lambda_2\\), are found using cross validation, where we trial different combinations of values for each parameter. set.seed(1) # alpha = 0.5 for elastic net elnet &lt;- cv.glmnet(x[train,], y[train], alpha = 0.5) Bestlambda &lt;- elnet$lambda.min Bestlambda ## [1] 37.2646 #error pred &lt;- predict(elnet, s = Bestlambda, newx =x[test,]) mean((pred - y.test)^2) ## [1] 75313.38 #coefficients model &lt;- glmnet(x, y, alpha = 0.5, lambda = grid) predict(model, type =&quot;coefficients&quot;, s = Bestlambda) ## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 30.16402739 ## AtBat . ## Hits 1.79149853 ## HmRun . ## Runs . ## RBI . ## Walks 2.21659602 ## Years . ## CAtBat . ## CHits 0.05473417 ## CHmRun 0.35259639 ## CRuns 0.17805457 ## CRBI 0.24176171 ## CWalks . ## LeagueN 1.83589836 ## DivisionW -96.60001610 ## PutOuts 0.21211648 ## Assists . ## Errors . ## NewLeagueN . "],
["dimension-reducing-algorithms.html", "5 Dimension Reducing Algorithms 5.1 Principal Component Analysis (PCA) 5.2 Linear Discriminant Analysis (LDA)", " 5 Dimension Reducing Algorithms We have previously seen a few alternatives for feature selection. In this chapter we will look into methods that allow us to reduce the number of predictors with the use of some linear algebra. We will see how we can reduce problems of multiple dimensions into 2-D space, by plotting new axis and extrapolating the data on them. We will also discover ways to visually represent multidimensional data and perform classifications. Dimension redaction is often performed when: The problem requires regression, classification, clustering We want to visualise data of multiple dimensions The data we have has multiple features that are highly correlated, and out of all the features, a few of them are the major drivers We do not have the response variable (unsupervised learning) ##Unsupervised learning Up until now the data that we were analysed had been labeled, that is we knew the associated y/reaction variable. If we remember, the salaries associated with the baseball players were known, when analysing the Hitters dataset, similarly the price of the properties on the Boston dataset was also given. That helped us gain a clear understanding of the problem. We wonted to study the relationship between the features and the reaction variable, and use it to make predictions. Having the reaction variable readily available, also meant that were able to measure the performance of our models with ease, using methods such us cross-validation and confusion matrix. But what if our problem was something like this? A company like Netflix wants to identify groups of users with similar taste, in order to improve their recommendations. What defines taste? We can look into the genre of the film, the users age and the users ratings, but there isn’t really a clearly defined problem or response variable. Similarly, they might want to classify movies depending on their popularity. How do we find What the predictors that make a movie popular, and what do we classify as popularity? A researcher wants to group patients that have a rare disease using their gene expressions, symptoms and recovery progress. This will allow him to conduct his studies easier and treat each patient more effectively depending on their variation of the studied attributes. We do not know what or how many groups, this classification will result into, neither do we know which of the attributes collected are useful. Unlike, our previous classification problem of bank account defaulting, where the outcome was either defaulted or not defaulted, in such cases we simply do not know what the outcome will be. When the response is not clear, for example we do not know what categories will result from the data, we say that this is an unsupervised problem. 5.1 Principal Component Analysis (PCA) PCA is an unsupervised machine learning algorithm, used for dimension reduction. Just as with supervised learning we will be trying to gain insight on our problem by studying the variations in the dataset, trying to explain them and use the explanations for grouping the data or making predictions. Often we are faced with datasets containing multiple predictors, which can be highly correlated, making it difficult to perform this analysis. PCA can reduce the number of variables and remove any multicollinearity effect, while maintaining the measures that explain most of of the variability in a dataset. PCA will attempt to summarise the data, by projecting it in new axis. Each of those new axis will be a linear combination of the features in the original dataset, that results in a line closest to the observations. For example a one dimensional summary of the data, would be a line closest, on average, to all the data points (taking the Euclidean distance). A two dimensional summary would be a plane that is closest to the data points. PCA will create as many of those ‘summary dimensions’ as there are features in the dataset, in order to explain all of the variation (unless there in not enough data to find all of them). However, usually the first few dimensions will be able to summarise the data quite well, explaining a significant percentage of the variation. So we choose to project the data in those fewer dimensions. We have already mentioned that in order for ‘summary dimensions’ to be able to explain the data, they are a combination of the features in the original dataset. In an n-dimensional space the first principal component will follow the direction of the dimension which shows the most variation. For example if we had a 2-D scatterplot where the observations mostly followed a straight line, parallel to the x axis, we would see that most of the action is happening on the x axis, while there is little variation on the y axis. The first principal component would follow the direction the x-axis, maybe slightly rotated to capture some of the variation on the y axis. Those ‘summary dimensions’ are called principal components, let’s how they are derived. We swift the data so that the centre of the data points is at the origin of the axis (they are shifted without loosing the relative distances to each other). This means that the mean of the data is now zero. For your information, this is not required however, having a mean of zero makes the calculations later on easy, so it is generally preferred. We find the best fit line between our data points, which are now centred. The best fit line, will be able to best describe our data (summarise it) and therefore it will become our fist principal component (PC1). This is done in a similar way to least squared distance in linear regression. However, since we are looking to find a vector (vectors always start from the origin, we want this since PC1 will become a new axis, it would be very unless if an axis did not start from the origin) we want this best fit line to pass through the origin. Because this is true instead of minimising the distance of our points to the best fitting line, we can instead maximise the distance of the projected (to the best fitting line) points to the origin. This distance is at a 90 degree angle from the distance of our points to the best fitting line, and since the distance of each point to the origin is always constant, they are inversely proportional (when the distance of our points to the best fitting line decreases, the distance from the projected points to the origin increases), from the Pythagorean theorem. The form of the PC1 will be : \\(y =\\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n\\), where \\(x_1\\) to \\(x_n\\), are the features on the original dataset. The coefficients are called the loading scores and will show us which features had the most effect on the PC (the higher the absolute value of the coefficient the more influence that feature had). 3.We want the next PC to be a (again) a linear combination of all features that explains the most (of the remaining!) variance. We do not want the next PC to be correlated to the previous PC, we want it to only explain the remaining variance and avoid multicollinearity. This means that this new line will be perpendicular to the previous (and still needs to pass through the origin to become a new axis). Collinearity in a graphical representation can be identified when two lines are close to each other and/or have the same direction. When they are perpendicular to each other there is clearly no correlation. We keep making Pc’s until all of the variation is explained. We choose a few of those PCs, whose combination explains most of the variation, and we we project the data on them. Warning: One thing to be careful of is using data of a different scale for some of the features. Data with higher scale will bias PCA to assume that most of the variation is happening in that axis. For example if we had scores of students in maths’ exam (from 1 to 10) and in history (from 1 to 100), the loading scores for history would be unreasonably hight. A standard practice to avoid this, is to divide all feature data by its standard deviation. # This lab was created by professor Bharatendra Rai from University of Massachusetts Dartmouth (it is been slightly modified for our purposes) # We are using a classic data set that comes with R, the iris dataset. It contains information about 3 different species of the iris flower, such us their pedal length and width. We want to know if we can use that information to cluster the different species and predict which species new data belongs to, from its attributes. data(&quot;iris&quot;) summary(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## # The iris dataset comes labeled, this means that we can use PCA in a &#39;supervised&#39; format. It just allows us to split the data in two, and measure the performance of the model in predicting species on the testing and training data. # Partition Data set.seed(111) # the ratio of the split is 80% for the training and 20% for the testing ind &lt;- sample(2, nrow(iris), replace = TRUE, prob = c(0.8, 0.2)) training &lt;- iris[ind==1,] testing &lt;- iris[ind==2,] # Scatter Plot &amp; Correlations install.packages(&quot;psych&quot;) library(psych) ## ## Attaching package: &#39;psych&#39; ## The following object is masked from &#39;package:boot&#39;: ## ## logit ## The following objects are masked from &#39;package:DescTools&#39;: ## ## AUC, ICC, SD ## The following object is masked from &#39;package:car&#39;: ## ## logit ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha # We can see that the data is highly correlated, especially petal.Length and petal.Width (almost 1!), this makes it a good candidate for PCA even tho we do not have that make dimensions pairs.panels(training[,-5], gap = 0, bg = c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;)[training$Species], pch=21) # Principal Component Analysis with prcomp() pc &lt;- prcomp(training[,-5], center = TRUE, scale. = TRUE) # we can see the loading values for each PC summary(pc) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.7173 0.9404 0.38432 0.1371 ## Proportion of Variance 0.7373 0.2211 0.03693 0.0047 ## Cumulative Proportion 0.7373 0.9584 0.99530 1.0000 # As we would expect there is zero correlation between the PCs (Orthogonality of PCs), so we are good to go pairs.panels(pc$x, gap=0, bg = c(&quot;red&quot;, &quot;yellow&quot;, &quot;blue&quot;)[training$Species], pch=21) # Scree plot can show us how much of the variation in the data set is explained by each PC pcVar &lt;- pc$sdev^2 pcaVarPercentage &lt;- round(pcVar/sum(pcVar)*100,1) # we can see that most of the variation can be explained by just the first 2 PCs barplot(pcaVarPercentage, main=&quot;Scree plot&quot;, xlab=&quot;PC&quot;, ylab = &quot;Percentage Variation&quot;) # We will now project our data in those two Pc&#39;s # Bi-Plot install.packages(&quot;devtools&quot;) devtools::install_github(&quot;vqv/ggbiplot&quot;) library(ggbiplot) ## Loading required package: plyr ## ------------------------------------------------------------------------- ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------- ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## Loading required package: scales ## ## Attaching package: &#39;scales&#39; ## The following objects are masked from &#39;package:psych&#39;: ## ## alpha, rescale ## Loading required package: grid #we can see that PCA has done a very good gob of grouping iris of the same species using only 2 dimensions #the red arrows help us understand how PC1 and PC2 are derived, we can use them to approximate the properties of each feature g &lt;- ggbiplot(pc, obs.scale = 1, var.scale = 1, groups = training$Species, ellipse = TRUE, circle = TRUE, ellipse.prob = 0.68) g &lt;- g + scale_color_discrete(name = &#39;&#39;) g &lt;- g + theme(legend.direction = &#39;horizontal&#39;, legend.position = &#39;top&#39;) print(g) # Prediction with Principal Components # contains predictions of species for the training data trg &lt;- predict(pc, training) trg &lt;- data.frame(trg, training[5]) tr ## function (m) ## { ## if (!is.matrix(m) | (dim(m)[1] != dim(m)[2])) ## stop(&quot;m must be a square matrix&quot;) ## return(sum(diag(m), na.rm = TRUE)) ## } ## &lt;bytecode: 0x680e168&gt; ## &lt;environment: namespace:psych&gt; # contains predictions of species for the testing data tst &lt;- predict(pc, testing) tst &lt;- data.frame(tst, testing[5]) # Multinomial Logistic regression with First Two PCs library(nnet) trg$Species &lt;- relevel(trg$Species, ref = &quot;setosa&quot;) mymodel &lt;- multinom(Species~PC1+PC2, data = trg) ## # weights: 12 (6 variable) ## initial value 131.833475 ## iter 10 value 20.607042 ## iter 20 value 18.331120 ## iter 30 value 18.204474 ## iter 40 value 18.199783 ## iter 50 value 18.199009 ## iter 60 value 18.198506 ## final value 18.198269 ## converged summary(mymodel) ## Call: ## multinom(formula = Species ~ PC1 + PC2, data = trg) ## ## Coefficients: ## (Intercept) PC1 PC2 ## versicolor 7.2345029 14.05161 3.167254 ## virginica -0.5757544 20.12094 3.625377 ## ## Std. Errors: ## (Intercept) PC1 PC2 ## versicolor 187.5986 106.3766 127.8815 ## virginica 187.6093 106.3872 127.8829 ## ## Residual Deviance: 36.39654 ## AIC: 48.39654 # Confusion Matrix &amp; Misclassification Error - training p &lt;- predict(mymodel, trg) tab &lt;- table(p, trg$Species) tab ## ## p setosa versicolor virginica ## setosa 45 0 0 ## versicolor 0 35 3 ## virginica 0 5 32 1 - sum(diag(tab))/sum(tab) ## [1] 0.06666667 # Confusion Matrix &amp; Misclassification Error - testing p1 &lt;- predict(mymodel, tst) tab1 &lt;- table(p1, tst$Species) tab1 ## ## p1 setosa versicolor virginica ## setosa 5 0 0 ## versicolor 0 9 3 ## virginica 0 1 12 1 - sum(diag(tab1))/sum(tab1) ## [1] 0.1333333 5.2 Linear Discriminant Analysis (LDA) Linear discriminant analysis is very similar to PCA, however it is a supervised method. When we feed data to LDA, we know beforehand what categories we are looking to group, and each observation is labeled. LDA makes use of this additional information and when it performs dimension reduction, it can achieve better separation of the categories. We can use LDA to summarise multidimensional data in a 2-D graph, without having to compromise important information from the various attributes. This can show us how effective current attributes are in grouping the data and what their relationship is. Like PCA, LDA creates new axis, by combining the existing features and projects the data points on them. LDA tries to find an axis which satisfies two conditions. First, when the data is projected on the new line, the distance between the means of the categories (the values the response takes on) is maximised. The further away the means of two categories are the more separated they will be, since the mean is where most of the data for each category is usually concentrated on. Secondly, their standard deviation is minimised. This is how scattered the data will be when projected on the best fit line (the more concentrated the data for each category is on its own mean, the better the separation will be). When we have 2 dimensional data (data with 2 features \\(a\\) and \\(b\\)), the function that will produce the best single axis for that data to be projected on, would be the result of maximising the following function: \\(\\frac{(\\mu_a - \\mu_b)^2}{\\sigma_a^2 + \\sigma_b^2}\\) If we had more than two dimensions in the original data, instead of taking each and every combination of distances that need to be maximised we perform the following steps: Find a central point to all the data \\(C_all\\) Find a central point for each category of the data \\(C_a\\), \\(C_b\\) … Compute the distance \\(d\\) for each \\(C_a\\), \\(C_b\\) … to \\(C_all\\) (defined as their squared difference) The function we will be looking to maximise would become as fallowing: \\(\\frac{d_a + d_b + d_c ...}{\\sigma_a^2 + \\sigma_b^2 + \\sigma_c^2}\\) LDA will always result in two axis that achieve the most separation data(&quot;iris&quot;) library(MASS) linear &lt;- lda(Species ~., iris) # Proportion of trace, shows the percentage of separation achieved by each axis # we see that LD1 separates the data almost perfectly linear ## Call: ## lda(Species ~ ., data = iris) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3333333 0.3333333 0.3333333 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 5.006 3.428 1.462 0.246 ## versicolor 5.936 2.770 4.260 1.326 ## virginica 6.588 2.974 5.552 2.026 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## Sepal.Length 0.8293776 0.02410215 ## Sepal.Width 1.5344731 2.16452123 ## Petal.Length -2.2012117 -0.93192121 ## Petal.Width -2.8104603 2.83918785 ## ## Proportion of trace: ## LD1 LD2 ## 0.9912 0.0088 # we can see the coefficients for the two lines (our new axis), that were created by LDA # the coefficients will show how each attribute affected each axis linear$scaling ## LD1 LD2 ## Sepal.Length 0.8293776 0.02410215 ## Sepal.Width 1.5344731 2.16452123 ## Petal.Length -2.2012117 -0.93192121 ## Petal.Width -2.8104603 2.83918785 install.packages(&#39;devtools&#39;) library(devtools) devtools::install_github(&#39;fawda123/ggord&#39;) library(ggord) # we can see that the separation using LDA is much better than with PCA p &lt;- ggord(linear, iris$Species) p "],
["extensions-for-linear-models.html", "6 Extensions for linear models 6.1 Introduction 6.2 Step Function 6.3 Splines 6.4 Smoothing splines 6.5 Local Regression", " 6 Extensions for linear models 6.1 Introduction As we have already mentioned, the linear model suffers some serious limitations, which steam from the assumption that the model follows a linear trend. In real life, relations between predictors and reaction are (almost always) more complicated. They can be changing as the predictors take on higher/lower values. When we explained linear regression we also show polynomial regression, which was able to better explain some of those alternations and optimise our models’ performance. In this chapter we will look into other such extensions of the linear models. As a general idea, we will be attempting to capture variations on this relationship, by splitting the data on different ranges of the predictors (where the variation occurs), and fitting a different model on each. Following that though, Additive models are useful when: On the scatterplot we observe that the way the predictor is related to the reaction changes for deferent ranges of the predictor. If you spot various ‘curving’ patterns. In such cases, algorithms such us decision tree based may result in similar or better performance models, with less of the complexity. You may want to use an Additive model, if inference is key in your analysis or as a comparison point in your benchmarking. As always, this will highly depend on your problem, the data you have and your resources. It is advised to try out various models and cross validate them. 6.2 Step Function This method suggests breaking down the model into bins. Each bin will have its own coefficients that are only valid for a certain range of the predictors \\(X\\). Each bin is assigned a constant \\(C_1\\), \\(C_2\\), \\(C_3\\), \\(C_k\\) which can either be 1 or 0, indicating if the value that \\(X\\) takes, is inside or outside the bin’s range. Doing so allows us to represent the function as follows and solve using least squares. $ y_j = _0 + _1 C_1 x_1 + _2 C_2 x_1 + … + _k C_k x_1 + _1 C_1 x_2 + _2 C_2 x_2 + … + _k C_k x_2 + … + _1 C_1 x_j + _2 C_2 x_j + … + _k C_k x_j$ where for each \\(x_j\\) you can have up to one \\(C_k=1\\) (one or none \\(C_k\\) will be true), since it can only have up to one coefficient depending on which bin it belongs to. This method comes with a major disadvantage. Since each bin’s coefficients are derived using only data that is within the bin’s range, we might miss capturing global trends. Generally, you would consider using this method if: The bins you want to split your data into are well defined by nature. For example, this method is popular in disciplines such us biostatistics and epidemiology, where the relationships studied are very different for patients with 5-year differences (bins would split the data for every age group). You have a lot of coefficients out of which some are useful for certain ranges and others are useful for other ranges # we will use the dataset Wage, made by ISLR to fit well for additive models library(ISLR) data(Wage) # the cut() function will help us split the data in bins. We can select how many bins we want, here we have selected 4. Furthermore, we can set the cut off points, which would be used to place each bin with the option breaks=,. If this is not specified R will choose to place each bin uniformly across the range of x. In this case R has set a bin for every 15.5 year group. t &lt;-table(cut(Wage$age,4)) t ## ## (17.9,33.5] (33.5,49] (49,64.5] (64.5,80.1] ## 750 1399 779 72 model &lt;- lm(wage~cut(age,4), data = Wage) # here you can see that each age group has its own coefficient estimate. # notice that the age group 17.9 to 33.5 does not have a coefficient. For those values prediction would simply be their avarage wage given by the intercept $94.158. model ## ## Call: ## lm(formula = wage ~ cut(age, 4), data = Wage) ## ## Coefficients: ## (Intercept) cut(age, 4)(33.5,49] cut(age, 4)(49,64.5] ## 94.158 24.053 23.665 ## cut(age, 4)(64.5,80.1] ## 7.641 # let&#39;s see how this looks like library(ggplot2) x1 &lt;- seq(17.9, 33.5, 0.1) y1 &lt;- rep(94.158, times = length(x1) ) y2 &lt;- integer() x2 &lt;- seq(33.5, 49, 0.1) for(i in 1:length(x2)){ y2[i] &lt;- 94.158 + 24.053 * x2[i] } y3 &lt;- integer() x3 &lt;- seq(49, 64.5, 0.1) for(i in 1:length(x3)){ y3[i] &lt;- 94.158 + 23.665 * x3[i] } y4 &lt;- integer() x4 &lt;- seq(64.5, 80.1, 0.1) for(i in 1:length(x4)){ y4[i] &lt;- 94.158 + 7.641* x4[i] } x &lt;- c(x1,x2, x3, x4) y &lt;- c(y1,y2, y3, y4) d=data.frame(x=x, y=y) ggplot() + geom_step(data=d, mapping=aes(x=x, y=y)) + geom_step(data=d, mapping=aes(x=x, y=y), direction=&quot;vh&quot;, linetype=3) + geom_point(data=d, mapping=aes(x=x, y=y), color=&quot;red&quot;) 6.3 Splines As we can see from the graph, the endpoints are not fitting well, the model is not ‘smooth’. And although it is clearly not as strict and overgeneralised as a simple linear model, it may not do a great job of describing the relationship between age and wage. If we look at the scatterplot, we can see that a curve rather than a line could be more fitting. plot(Wage$age, Wage$wage) We already know that we can use a polynomial in order to fit a curve to the data. We could try various degrees of polynomials and cross validate them to see which performed the best. Polynomials however, have various pitfalls. They usually perform well on quite high degrees. High degrees of polynomials will curve the data too much in unnecessary places and can cause inaccurate analysis, among other issues. Instead we would like to use piecewise polynomial regression, where we would choose a polynomial of smaller degree but achieve optimal performance. In piecewise polynomial regression we fit a model using deferent coefficients for various ranges of \\(x\\). Quite often the cubic (3rd degree polynomial) performs well, here is what a function for cubic piecewise polynomial regression would look like: \\(y_i = \\left\\{ \\begin{array}{ll} \\beta_01 + \\beta_11x_j + \\beta_21x_j^2 + \\beta_31x_j^3 &amp; \\mbox{if } x_j \\geq c \\\\ \\beta_02 + \\beta_12x_j + \\beta_22x_j^2 + \\beta_32x_j^3 &amp; \\mbox{if } x_j &lt; c \\end{array} \\right.\\) Where the range of \\(x\\) is been ‘cut’ on the point \\(c\\). The point \\(c\\) is called a knot. We can add more knots if we want to increase the flexibility of the model. We say that the above function has been split in two bias functions one for \\(x_j \\geq c\\) and one for \\(x_j &lt; c\\). When we apply such a regression, we have managed to overcome some of the pitfalls of polynomial regression. However, the model would be discontinuous at the points of the knots, here is an example where we chosen a single knot at age 50. #split the data at knot=50 data1 &lt;- Wage[which(Wage$age &lt;= 50),] data2 &lt;- Wage[which(Wage$age&gt;50),] #create the two bias functions fit1 &lt;- lm(wage ~ age + I(age^2) + I(age^3), data = data1) fit2 &lt;- lm(wage ~ age + I(age^2) + I(age^3), data = data2) # x values for each bias function age.grid1 &lt;- seq(from=18,to=50) age.grid2 &lt;- seq(from=50,to=80) #y values resulting from each bias function y1 &lt;- predict(fit1,list(age=age.grid1)) y2 &lt;- predict(fit2,list(age=age.grid2)) #plots plot(Wage$wage ~ Wage$age, data = Wage,col=&quot;#CCBADC&quot;) with(data.frame(age.grid1), lines(x = age.grid1, y = y1)) with(data.frame(age.grid2), lines(x = age.grid2, y = y2)) We now need to ‘smoothen’ the graph. In other words, we want we want the rate of change for its slope (which describes its shape) to be the same at the point of the knots, so as to avoid weird ‘bumps’. This is given by the second derivative. So we can add the condition that the second derivatives of the left and right functions need to be the same, at the point of the knot (50 in this example). Adding this constraint will create what is called a natural spline. There are other ways to smoothen the graph such us a clamp, where instead we require the first derivatives to be equal to zero. Here is what happens when we add this condition #library for fitting cubic splines library(splines) model &lt;- lm(wage~bs(age,knots=c(50)), data=Wage) # x values agelims &lt;- range(Wage$age) age.grid &lt;- seq(from=agelims[1],to=agelims[2]) # resulting y values pred&lt;- predict(model,list(age=age.grid),se=T) #plot plot(Wage$age, Wage$wage, col=&quot;#CCBADC&quot;) lines(age.grid, pred$fit, lwd=2) # if you are not sure how many knots to place and where, you can instead choose your degrees of freedom, the knots are placed uniformly by R: model &lt;- lm(wage~ns(age, df=4), data=Wage) pred &lt;- predict(model, newdata = list(age=age.grid), se=T) plot(Wage$age, Wage$wage, col=&quot;#CCBADC&quot;) lines(age.grid, pred$fit, lwd=2) # if we increase the degrees of freedom we allow for more &#39;wiggle&#39; room and risk overfitting model &lt;- lm(wage~ns(age, df=30), data=Wage) pred &lt;- predict(model, newdata = list(age=age.grid), se=T) plot(Wage$age, Wage$wage, col=&quot;#CCBADC&quot;) lines(age.grid, pred$fit, lwd=2) 6.4 Smoothing splines Smoothing splines is another way we can fit a curve to our data. Just like we did when fitting a line to the data, we will try to fit a curve that minimises the residual error ( a curve that is the closest to all points). Of course with a curve, we can always fit all the data perfectly, using potentially a very ‘wiggly’ curve that goes around all the points. Such a curve will have extreme problems of overfitting. Instead, what we really want to do is find a curve that fits the data well but also minimises overfitting. We can achieve this in a similar manner to ridge regression, adding a penalty of weight \\(\\lambda\\) to the function that minimises RSS. This penalty will be associated with how ‘wiggly’ the line is, since we are trying to find the best fitting ‘smooth’ curve. We have explain that smoothness of the curve can be associated with the second derivative of the curve’s function. This brings us to the function we will be minimising (don’t worry too much about the representation): \\(\\sum_{n=1}^{n} (y_i - g(x_i)^2) + \\lambda \\int g&#39;&#39;(t)^2\\,dt.\\) As you would expect, \\(\\lambda\\) is found using cross validation. # smooth splines using cross validation model &lt;- smooth.spline(Wage$age, Wage$wage, cv= TRUE) ## Warning in smooth.spline(Wage$age, Wage$wage, cv = TRUE): cross-validation ## with non-unique &#39;x&#39; values seems doubtful plot(Wage$age, Wage$wage, col=&quot;#CCBADC&quot;) lines(model,lwd=2) 6.5 Local Regression If all fails, there is the alternative of fitting a curve to your data using the local regression. This method creates the curve incrementally. It chooses its place at some point \\(x\\), by considering the location of the closets observations around that place. It resembles the algorithm k-nearest-neighbours (discussed in future chapters). Let’s see exactly how this is achieved: First we need to define the size of the window. That is how many closest observations we want the method to account for, when defining each new point of the curve. We will call this number \\(k\\) Then we want to define the new points of the curve. We do that by moving the original observation’s location to a new location. We start from the first observation and we find the k-closest observations to it. To find its new position we perform least squares on the k-closest points (including the original point). The original point is referred to us the vocal point. The closest those k points are, the more weight is given to them on the regression. They have more influence on the position of the best fit line. The vocal point has the most weight. We project the vocal point on the best fit line. The projection is the fist point of the curve we are trying to draw We repeat steps 2,3 and 4 for all the observations, in order to find all the points of our curve. We could now draw a ‘best fit curve’ by connecting all those new points. However, that curve may have a lot of ‘wiggle’, so we want to follow some additional steps in order to smoothen it. The ‘wiggles’ would be the result of points that are too hight/low compared to the average, and are influencing the position of the best fit line each time too much (remember leverage points). TO reduce their effect, we add additional weights based on how far the original points are to the projected ones. Points that are moved too much will have higher weights. We repeat the process of finding new points from the original observations using least squares. However, this time we also account for the additional new weights. Now we can fit a smoother curve in our data, by connecting the new points Notes: The process of regularisation (smoothening) may need to be repeated a couple of times in order to reach the desired level of smoothness. We may want to fit parabolas instead of lines on each window of data. Parabolas tend to work better on data that indicates a lot of curving patterns. In R you can use the function lowess() for a line and loess() for a parabola (or a line, it defaults to parabola) In theory we can perform Local Regression for multiple \\(x\\) predictors. However, when the dimensions are increasing the observations will become more sparse amongst them. This means that there would be greater distances between them and not enough nearby observations for each dimension. You need to ensure you have enough observations. In general local regression is performed with 3 or 4 predictors as a max. watch https://www.youtube.com/watch?v=Vf7oJ6z2LCc for a good visual explanation by Josh Starmer. # fitting parabolas # spam associates with degree of smoothing and is related to the window size selected proportional to the data fit1 = loess(wage~age,span=.2,data=Wage) fit2 = loess(wage~age,span=.5,data=Wage) plot(Wage$age, Wage$wage, col=&quot;#CCBADC&quot;) lines(age.grid,predict(fit1,data.frame(age=age.grid)),lwd=2) lines(age.grid,predict(fit2,data.frame(age=age.grid)),lwd=2, col = &quot;orange&quot;) legend(&quot;topright&quot;, legend=c(&quot;Span =0.2&quot;, &quot;Span=0.5&quot;), col=c(&quot;black&quot;,&quot;orange&quot;), lty=1, lwd=2, cex=.8) # fitting lines # f is similar to span, the greater f gives more smoothness plot(Wage$age, Wage$wage, col=&quot;#CCBADC&quot;) lines(lowess(Wage$age, Wage$wage, f=2/3), col=2) lines(lowess(Wage$age, Wage$wage,f=1/8)) legend(&quot;topright&quot;, legend=c(&quot;f=2/3&quot;, &quot;f=1/4&quot;), col=c(&quot;red&quot;,&quot;black&quot;), lty=1, lwd=2, cex=.8) "],
["recommendation-systems.html", "7 Recommendation Systems 7.1 Recommending similar books. Content based filtering 7.2 Recommending books that were liked by ‘similar’ users, Collaborative filtering 7.3 Recommending items that are often bought together (mining item association rules) 7.4 Further Discussions: 7.5 Conclusion", " 7 Recommendation Systems The application we will be referring to can be found here: https://datasupport.site 7.1 Recommending similar books. Content based filtering First we will create the algorithm that is used to recommend ‘similar books’ to the one the user just bought. That algorithm will simply look at how similar the book we just bought is, to other books available, according to the given attributes (author, genre, age). This approach is called Content based filtering. 7.1.1 What is similarity? For us humans, distinguishing similar items is a simple task. We look at the properties of the item (in this case the book’s author, theme and age) and we can group together books by the same author, same theme and similar age. If we want a machine to perform that task we need to generalise this intuition and describe it using a set of clear instructions. In machine learning, similarity if often described by how far two items are from each other. Distance is an effective metric as it quantifiable and clear. This distance would be in terms of the given attributes. To understand this, we can imagine plotting all of our points, using their attributes as coordinates. The closer each point is to another the more ‘similar’ they would be. For example if we only used the \\(x\\) axis (a straight line), whose values represent the age of each book. We could then mark where on that line each book is according to its age. Marks that are close to each other would show us books that have similar age. To define ‘similarity’ in a more realistic manner we need to use the rest of attributes. So instead of just a line we plot our points in a graph where each axis (known as dimension) represents an attribute. This distance between points can be measured using various techniques such as: Euclidian distance. Similar to computing distances in 2 dimensional distances, we use the co-ordinates of the points to draw a right angled triangle with two known sides and compute the unknown, which represents the distance, using the Pythagorean theorem. Correlation distance. Basically measures how ‘in-sync’ (do they both go up or down) the deviations from the mean of each point are for each user. Users that have rated the same product higher than their average and the same products lower than their average are considered close to each other. Hammer’s distance. Simply measures the percentage of agreement. How many times two users gave the exact same rating. 7.1.2 How can we find similar books? As we discussed above, similarity is described in terms of distance between points. So we would have to ‘plot books over the axes of age, author and theme’. and find which books are close to each other. Author and theme, are however categorical variables (non-numeric), so we can not easily plot them in the way we want to. An simple way to bypass this, would be to assign a numeric ‘dummy’ variables, representative of each value a categoric attribute takes on. For example, theme love story would be \\(1\\), theme pirates \\(2\\), aliens \\(3\\), drama \\(4\\) and so on. Once this is complete we can plot the books in terms of theme using those dummy variables. The books with the same theme would be close to each other, correctly describing similarity. What is the problem? The problem is on the order of dummy variables. When we plot this our model will decide that love story (number \\(1\\)) is more similar to pirates (number \\(2\\)), as they are closer to each other, than let’s say drama which happens to be number \\(4\\). We would have to give an order that makes logical sense, which can be exhaustive. Sometimes such an order may not even be meaningful or very challenging, in the example of authors what would make an author be more similar to another author? For that reason, we have chosen to work with Hammer’s distance for the categorical variables (simply looks whether or not two values are the same) and absolute distances for the age. We have created our own algorithm, which follows our intuition of what similarity is, let’s see how it works! # First we load the dataset I have prepared, it contains the following # observations: # Books, which contain the book number (from 1 to 20), author (a, b, c, d), the # genre (pirate, alien, love, drama) and how many years ago the book was written books &lt;- read.csv(&quot;books.txt&quot;) # For every book that a user buys, we want to recommend &#39;similar&#39; ones. So every # time the user clicks &#39;Submit&#39; to buy a specific book. We want to calculate the # distances from his book to the rest of the books available and recommend the # ones that are closer (efficiency considerations and optimisations are # discussed later, for now let&#39;s keep things simple for the purpose of # understanding the concepts). # So the first step is to get the distances of this book to the rest. This is # what this function does. # It receives the book number of the book that was just bought as a parameter get.distance &lt;- function(bookNumber){ # It returns a data frame containing various distance metrics df &lt;- data.frame(CompBook=integer(), hammingDist=integer(), ageDist = integer(), totalDist = integer() ) # First we capture the author, genre and age of the book that was bought, # since those values will be used to compare similarity author &lt;-books[books$book==bookNumber,]$author genre &lt;- books[books$book==bookNumber,]$genre age &lt;- books[books$book==bookNumber,]$age # Then for every other book available we for (i in 1:20){ # Capture its author, genre and theme authorComp &lt;-books[books$book==i,]$author genreComp &lt;- books[books$book==i,]$genre ageComp &lt;- books[books$book==i,]$age # Reset our distance metrics to zero hammingDistance &lt;- 0 ageDist &lt;- 0 totalDist &lt;-0 # Compute our Hamming distance # If they do not have the same author, we want the distance to increase so we # add one if (author != authorComp){ hammingDistance = hammingDistance+1 } # If they do not have the same genre, we want the distance to increase so we # add one more if (genre != genreComp){ hammingDistance = hammingDistance+1 } # For the age, we simply get the absolute difference of the age from the book # we just bough to the book we are comparing it to ageDist &lt;- abs(age - ageComp) # It would not make sense to simple add our hamming distance with the age # distance, as the hamming distance can only go up to 1 for each value, # whereas the age can get up to very high values. Age would become far more # significant than the rest of the attributes. Instead we use domain knowledge # to interpret this value. if (ageDist &gt; 10) { # If the age is greater that 10 years we add 0.5 to the distance (we assume # this has been chosen from domain knowledge and studies available) totalDist &lt;- hammingDistance + 0.5 # If it is only grater that 5 we add 0.25 } else if (ageDist &gt; 5){ totalDist &lt;- hammingDistance + 0.25 # Otherwise total distance is just the one we have previously calculated, the # hamming distance } else { totalDist &lt;-hammingDistance } # We just need to add our calculated metrics to the data frame we need to # return newdata &lt;- c(i, hammingDistance,ageDist, totalDist) df&lt;- rbind(df, newdata) names(df) &lt;- c(&quot;CompBook&quot;, &quot;hammingDist&quot;, &quot;ageDist&quot;, &quot;totalDist&quot;) } # And return them :) return(df) } # This function, is the one actually called when the user hits submit getClosest3 &lt;- function(bookNumber){ # It makes use of the previous function to get the calculated distances from # all our points distDataSet &lt;- get.distance(bookNumber) # It removes the distance of our point to itself, as we do not want to # recommend to the user the book he just bough (although technically it has # the smallest distance) distDataSet &lt;- distDataSet[! (distDataSet$CompBook==bookNumber),] # We order the resulting distances in ascending order (smallest first) orderedDistances &lt;- distDataSet[order(distDataSet$totalDist, decreasing=FALSE), ] # Choose the 3 first from the ordered data frame closest &lt;- head(orderedDistances, 3) # and return them! :) return(closest$CompBook) } 7.2 Recommending books that were liked by ‘similar’ users, Collaborative filtering 7.2.1 Similar users? Similar users are just users that have provided similar ratings or feedback, purchased or previewed similar objects and so on. In our case, we will focus on users that have provided similar ratings. Although, our recommendations will be optimised with more and more data, it does not mean that we require every user to have rated every book for the algorithm to work. The idea here is that we use the rating matrix (see image 2, its simply a table presenting the rating of every user) and try to fill in the gaps, from missing ratings. Our model assumes that if a rating is missing that user has not read the respective book. So it wants to predict what his ratings would have been if he had read it. If that prediction returns positive, the model will recommend that book to that user. The model fills those gaps by looking at the ratings of other user’s that had other similar ratings. Again, similarity can be identified by the distance of a user to another, where their coordinates are the ratings for each book. Let’s use an example to understand this. We want to recommend some books to reader A, reader A has already read a couple of our books and he has rated them. Reader A has not rated book1 and book 10, so we assume he has not read them and we want to see if it is worth recommending them. First we find some other readers with ratings similar to reader A (from the rating matrix). Then we look at the ratings for book 1 and book 10 that were given by the group of similar users. For each book we get the average rating that was given by them. We can take a weighted average, weighted by how close each of those reader is to A, the closer the more significant. We have found that the average rating, given by readers close to A, is 2 for book 1 and 5 for book 10, so we recommend book 10 to reader A. The main advantage of this approach is that we do not require information about the books themselves. This can be extremely valuable for cases where acquiring that information is exhaustive and maybe even expensive and slow. Describing each item often requires manual work by humans that are experts on the relevant field. The disadvantage is that we need feedback from the users in order to recommend items. Not all users give feedback and often users tend to give feedback only on extreme cases such as very good or very bad, leading to models missing out on valuable insight for a lot of products. # We load the data, this is a list that I have prepared, it contains the user, # the book and the rating per row. # Each user has rated some of our books from 1 to 5 (where 5 is perfect) # Not every users has rated every book readersFeedback &lt;- read.csv(&quot;readersFeedback.txt&quot;) # From that data, we need to create the rating matrix as shown in the image. At # the moment we have normalised data (user, book, rating) so we have from 1 to # 20 entries for each user. This is not useful if we want to find the distance # of each user using his ratings. The Ratings are the attributes that need to # used as coordinates to plot each point.So we need to transform that data to a # data that has one entry for each user, and 20 columns containing ratings, one # for each book. # First we make the 20 vectors containing the ratings for each user, those will # become columns for (i in 1:20){ assign(paste(&quot;book&quot;,i,sep=&quot;&quot;), readersFeedback[readersFeedback$book==i,]$rate) } # Also make a vector containing the readers numbers in order readers &lt;- c(1:15) # Now we make the new data set readers.data &lt;- data.frame(readers, book1, book2, book3, book4, book5, book6, book7, book8, book9, book10, book11, book12, book13, book14, book15, book17, book18, book19, book20) # This library contains the function knnImputation, which can fill in the # missing ratings, using the process we discussed previously. install.packages(&quot;DMwR&quot;) library(DMwR) # This is the function that is called every time a users submits a new rating on # our website. # It receives all the ratings as parameters (some may be null and this is fine) getRecommendations &lt;- function(book1, book2, book3, book4, book5, book6, book7, book8, book9, book10, book11, book12, book13, book14, book15, book16, book17, book18, book19, book20) { # Since we don&#39;t have a log in page, we don&#39;t know who the user is. For now we # can just assume this ratings are coming from a new user. # So we need to add this new user number to our rating matrix newUserId &lt;- nrow(readers.data) + 1 # We capture his ratings in this vector newEntry &lt;- c(newUserId, book1, book2, book3, book4, book5, book6, book7, book8, book9, book10, book11, book12, book13, book14, book15, book16, book17, book18, book19, book20) # Then we add his ratings and umber to the existing matrix newMatrix &lt;- rbind(readers.data,newEntry) # We fill in the gaps from missing ratings, as explained before using the # function provided by the library completeMatrix &lt;- knnImputation(newMatrix, k = 3, scale = T, meth = &quot;weighAvg&quot;, distData = NULL) # Capture the users ratings after we filled in the gaps readersRatingComplete &lt;- completeMatrix[completeMatrix$readers == newUserId,] # Capture the books that the user has not rated readersRatingsMissing &lt;- which(is.na(newEntry)) # Recommend books that the user has not rated and for which the predicted # ratings are more than 3.4. recommended &lt;- integer() for (i in 2:20){ if (i %in% readersRatingsMissing){ if(readersRatingComplete[i]&gt;3.4){ recommended &lt;- c(recommended, readersRatingComplete[i]) } } } # Return the books that satisfy that :) return(recommended) } 7.3 Recommending items that are often bought together (mining item association rules) We want to look at the transaction history and recommend items that users tend to buy together or over a short period of time. This approach to recommendation systems is called association rules learning. Given that a user just bough an item we want to find out how likely he is to buy other ‘associated’ items and recommend the ones we are most confident that the user will be interested in. To understand how this works, lets use the example where we want to find out if we should recommend book 2 to a user that just bought book 1 (only by looking at the transaction history of our shop). In other words, we want to find out if the association rule of buying 2 given that we bough 1 is strong enough. To measure the strength of ‘association’ between some items, we se use the following metrics: First we identify what proportion of all the transactions include both book 1 and book 2, this is called the SUPPORT of this rule. A high support means that there are a lot of transitions were both items have been bought together. This is some evidence that there is a strong relationship. However there is a chance that book 2 just happens to be very popular is found in various transactions, including the ones where book 1 is also bought. We want to be able to deal with such cases, this is what CONFIDENCE measures. We want to measure how often book 1 and 2 are sold together as opposed to how often book 2 is sold. This is given by the ratio of the proportion of all transactions that included both 1, 2 (support) over the proportion of transactions that include book 2. It is nothing more than the conditional probability of 2 given 1. Last we want to find out how much the probability of a user buying book 2 has increased once he bought book 1. This is called LIFT. We know the original probability of buying book 2, is just the proportion of all transactions that include book 2. The probability of buying 2 given that we have just bought 1 is what we just measured above, the confidence. If we divide the original probability of buying 2 to the conditional probability of 2 given 1, we can measure what is called Lift. A hight lift measurement means that the likelihood of the reader buying 2 once he bought 1 has increased. For an association rule to be considered we want to have a hight Support Confidence and Lift. We now have an understanding of how we measure ‘association’ between products. But how do we go about selecting which products to measure that association for, in the first place. Well one approach is to brute force it. Measure all the associations for every 2 possible pairs between our products, then the 3 pairs, the 4 pairs until we reach n pairs of products that are possible. From all those measurements we then pick the associations with the highest strengths. This is clearly computationally expensive. There are various approaches to this problem, a popular one, which we will use is the Apriori algorithm. This algorithm investigates associations incrementally, it starts from single variables, pairs of 2, 3 and so on. Every time it checks if the pairs satisfy a minimum support value, the pairs that do are used to generate rules. It then moves to the next pairs, using only the items that were left out previously. It repeats until no items are left, that can satisfy that minimum support threshold. # This package will help us implement Apriori to mine association rules install.packages(&quot;arules&quot;) library(arules) # Our first step is, as always, to load the data. I have prepared a transaction # history of 43 transactions, in a normalised &#39;single&#39; transactionId-item # format. We want R to recognise the file as a transaction history, in order to # be able to implement Apriori with ease. There are two formats that R can work # with for transactions. One is single, the one we use (you can view the file # for details), where each entry contains the transaction id and the item # bought, if there are multiple items bought in the same transaction, there # would be multiple entries with the same transaction id. # The alternative is basket format, where each entry contains the id and all the # products bought in that transaction in a single entry. trans &lt;- read.transactions(&quot;transactionsNoUserName.txt&quot;, format = &quot;single&quot;, sep = &quot;,&quot;, cols = c(1,2)) # Now we can implement Apriori, we can set the threshold for support and # confidence as shown bellow: assoc_rules &lt;- apriori(trans, parameter = list(sup = 0.03, conf = 0.6,target=&quot;rules&quot;)) # We can inspect rules that were found inspect(assoc_rules) # Last we only need to implement those rules. This is the function that is # called every time a user submits a book purchase. # Since the user is only buying one book, we only only the rules that are # relevant to single item purchase. useRules &lt;- function(bookNumber){ message(&quot;/rules&quot;) switch( toString(bookNumber), &quot;1&quot;=2, &quot;3&quot;=1, &quot;4&quot;=2, &quot;12&quot;=11, &quot;16&quot;=c(17,18), &quot;17&quot;=2, &quot;18&quot;=17, &quot;19&quot;=18, {0} ) } 7.4 Further Discussions: 7.4.1 Optimisations Although our models are currently doing their job pretty good, they would not scale well. For example, if the users increased from 15 to 100.000 and our books from 20 to 100.000.000.000 we would be having a lot of trouble. Let’s see why… At the moment, our content based filtering algorithm requires all the attributes of the book that we just bough to be checked against all the attributes of all the books that are available. There could be countless books, and in a more complex scenario the attributes we are comparing them to, would not only be three (e.g. we might include book title, popularity, publisher, city and so on). Why might really need check every attribute of every book, but we might be able to reduce the percentage of books we want to compute the precise distance to. For example if a user just bought a book about aliens, is it really worth comparing that book to love stories? Could we before applying the algorithm to every single book, filter out ones that we think are completely irrelevant. Even better could we have built some pre-defined groups of books that share similar attributes and use them as a guide. In the case of collaborative filtering, at the moment every time a user is providing some ratings, we need to update the ratings matrix and then re-fill the gaps, using the new information. That is very expensive and not maintainable, in a system where we are receiving hundreds of rating per day. What we could do instead is store the new ratings daily in some temporary storage and update the permanent rating matrix in batches. Moreover, if we have thousands of books, predicting the rating for each user may not be ideal, so we might consider some filtering here as well. Generally, a lot of effective predictive systems will choose to employ a combination of methods. For example, we might choose to cluster books in certain groups according using content based filtering, those could be very inclusive clusters (they would still include multiple books). We could then use collaborate filtering, within each cluster, to get more specific recommendations. This would save a lot of computational overhead, as the clusters could be pre-defined and the collaborate filtering would require only a portion of the original data (as it would use a rating matrix that refers to books only within the cluster). Another example would be to use both collaborate filtering and association rules on each case. We could then compare the recommendations returned from both the chosen algorithms and only return the ones found in both. A last note is to try and use pre-built packages and algorithms when appropriate, as they have been optimised for performance. Make sure however, you understand how those algorithms function in order to be able to provide them with the most appropriate data, and correct parameters. Furthermore, you need to be able to make adjustments when required, interpret their outcomes and measure their performance as objectively as possible. 7.4.2 Alternatives Another approach to optimisations could be looking at alternative algorithms that may be more ‘accurate’ or efficient or maybe just more appropriate for your case. As an example let’s find out about some alternative algorithms on collaborate filtering. Previously we used Euclidean method, through a library provided by R, to identify similar users through their ratings. There are other well-known methods for identifying ‘similar’ users, such as the Pearson method or Jaccard method. In the Pearson method each series of rating for every user is treated as a vector. The difference between two vectors is their angle which can be found by taking the cosine of their magnitudes. In the Jaccard similarity, the values of the ratings do not matter. Similar user’s are just users that have read the same books/clicked on the same websites/seen the same movies et cetera, this has some clear drawbacks (just cause they read a book it doesn’t necessarily mean they liked it) but it does not explicitly require users’ to have given out feedback. Last you might want to consider taking the correlation distance (mentioned on content based filtering) for something like ratings. In this case, association is described by whether or not the users have rated the same items above or bellow their average. Up to now we have discussed collaborate filtering by looking at similar users. There is however an alternative approach, called Latent Factor Analysis.This method attempts to identify the underlaying factors that caused each user to rate each film the way the did, we then use those factors to extrapolate (predict) what their rating would be for the rest of the films. To achieve this, we need to assume that there is a factor/combination of factors for each reader that makes them rate books a specific way. (e.g. a reader is into drama books). We also need to assume that there is a factor/combination of factors relevant to each book that result in bad/good rating (e.g. the author is very popular). Those underling factors can be identified using PCA. I have dedicated a previous chapter for PCA, but for now all you need to know is that we can somehow tell which are the main factors motivating reader to give certain ratings and books to receive certain ratings. We do not, however, have the values associated with those factors. We only have the outcomes that were produced by combining the factors of each user with each product (those are the ratings!). So for every rating we know that rating = F reader * F book Each user has its own set of equations, and all we need to do is solve for F reader and F book. The issue is, that not all the ratings are available for each user, meaning that we will not be able to find the exact numbers. There will always be an error, In other words rating - (F reader * F book) will not be zero. To estimates the factors in the best way we can, we will attempt to find some F reader F book, that minimises the error. (There are various techniques for that such as the stochastic gradient dissent that attempts to find a local minimum by trial… ) Once we have those values we can simply use them to fill in any gap in the matrix. For example, lets say we didn’t know the rating reader 1 gave for book 12. We had however enough data to calculate a factor 2 for that user and a factor 2 for that book. The rating would be 4, and so we should recommend book 12 to reader 1. (of course this is a very simplified example to describe the underlying mechanics. In reality a lot more factors would be involved and we would also need to account for other issues such as overfitting in our equations) Apart from measuring our alternatives in terms of algorithms, we should also consider alternatives in terms of data. Aside from explicit ratings, costumer feedback takes on many forms such as comments in either the provider’s platform or social media. It is possible, through sentimental analysis to capture the essence of those comments and use them to train our models and give more precise recommendations. A lot of those methods look at the presence and frequency of words and attempt to classify comments as positive or negative. For example the naive Bayer’s algorithm will measure the probability of a comment been positive and the probability of it been negative and decide on the one with the highest probability. Those probabilities are assigned by compering the presence and frequency of certain words compared to their average frequency and the label(negative/positive) available from the training sets (past data, labeled by humans used to train the algorithm). There is a lot more to sentimental analysis and its value on recommendation systems. It can often be used to deduce additional features/attributes, in the case of books, we could classify books depending on the frequency of certain words (e.g. vampires, fairy, solders). Last we should not exclude the option of classifying customers themselves. Although, it is generally preferred to adopt a product oriented approach, as the product is less complicated and more well defined, we can always recommend products that were bought by users that share similar attributes (e.g. sex, age, interests). 7.5 Conclusion This is by no means a complete guide on recommendation systems but I hope you were able to understand the concepts and appreciate the potential advantages as well as the challenges around them. "],
["basic-statistics-and-probabilities-review.html", "8 Basic Statistics and Probabilities Review 8.1 A useful cheatsheet in Probabilities 8.2 A useful cheatsheet in Distributions 8.3 Basic probability exercises 8.4 Understanding P-values 8.5 Confidence Intervals Problems 8.6 Chi-squared test 8.7 Anova", " 8 Basic Statistics and Probabilities Review 8.1 A useful cheatsheet in Probabilities 1.1 A useful cheatsheet in Probabilities 8.2 A useful cheatsheet in Distributions 1.2 A useful cheatsheet in Distributions 8.3 Basic probability exercises 8.3.1 Coin tossing: We are going to try and examine if the probability of getting heads or tails when tossing a coin if 0.5 by conducting n Bernoulli trials, summing the results of heads or tails and seeing if they are about half #using the random number generator N &lt;- 100 #will return true (1) if the output from 0 to 1 is &gt;0.5 flips &lt;- runif(N, min=0, max=1)&gt;0.5 #we sum them up, it will add all the ones for true and zero&#39;s for false of the above list of flips #so we expect half of them to be true and so to get a result close to 0.5 #we can increase the N to get closer sum(flips)/N ## [1] 0.59 #using sample function to perform a simulation sample.space &lt;- c(0,1) flips &lt;- sample(sample.space, N, replace = TRUE, prob = c(0.5, 0.5)) sum(flips)/N ## [1] 0.49 8.3.2 The famous birthday problem: Finding the probability that at least two people having the same birthday from a sample of n people. we ignore leap years, seasonal variations All the probable birthdays of the people would be 365^k if everyones birthday was unique (no one has the same birthday), there would be factorial(365)/factorial(365-k) ways for that to be true, from the permutations formula so the probability that at least two people have the same birthday would be 1 - (probability no one having the same birthday) #this library allows us to use larger numbers install.packages(&quot;Rmpfr&quot;) library(&quot;Rmpfr&quot;) ## Loading required package: gmp ## ## Attaching package: &#39;gmp&#39; ## The following objects are masked from &#39;package:Matrix&#39;: ## ## crossprod, tcrossprod ## The following objects are masked from &#39;package:base&#39;: ## ## %*%, apply, crossprod, matrix, tcrossprod ## C code of R package &#39;Rmpfr&#39;: GMP using 64 bits per limb ## ## Attaching package: &#39;Rmpfr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## dbinom, dnorm, dpois, pnorm ## The following objects are masked from &#39;package:base&#39;: ## ## cbind, pmax, pmin, rbind #create function to calculate permutations, this is the standard formula however we use the rmpfr library #that will let us deal with larger numbers perm &lt;- function(n, k) { factorialMpfr(n)/factorialMpfr(n-k) } #this is our n count &lt;- 100 #double so we can plot it p &lt;- double(count) for(k in 1:count){ #the formula from above d&lt;-(1 - perm(365, k)/ (mpfr(365, precBits = 1024)^k))*100 p[k] &lt;- asNumeric(d) } plot(1:count, p, xlab=&quot;number of people&quot;, ylab =&quot;probability in %&quot;) 8.4 Understanding P-values The p value if used to decide whether an Ho should be accepted or rejected. It is related to the probability of the event happening, assuming it is random (h0 is true) BUT it does not always equal that. p-value is : probability of event happening at random + probability of all other events of an equal chance + probability of all events that are less likely to happen #let&#39;s take the example of tossing a coin and landing 5 heads on a row, what is the p-value possibleOutcomes&lt;- 2^5 possibleOutcomes ## [1] 32 P4heads &lt;- 1/possibleOutcomes P4heads ## [1] 0.03125 #the probability of landing 5 tails in a row is equally likely P4tails &lt;- 1/possibleOutcomes #there is no other outcome less likely so we can get the p value from pValue &lt;- P4heads + P4tails pValue ## [1] 0.0625 #the probability was 0.031, however the p-value is 0.03, which is less than the usual threshold of 0.05 and so we could accept the H0 #this could occur at random #what would the p value be if we got 4 heads and 1 tail WaysForHeads1tail &lt;- factorial(5)/factorial(4)*factorial(5-4) P4heads1tail &lt;- WaysForHeads1tail/possibleOutcomes P4heads1tail ## [1] 0.15625 #the probability of something equally likely having (4 tails and 1 head) P4tails1head &lt;- P4heads1tail #probability of something less likely happening (5 heads or 5 tails), is already calculated so p would be: pValue &lt;- P4heads1tail + P4tails1head + P4heads + P4tails #also quite high so event could easily occurs at random pValue ## [1] 0.375 #what if we were dealing with continuous distributions, e.g hight measurements? #in this case we use a density function where the area under the graph represents the probability for that x1 to x2 occurring. #if we had the following height sample, what is the probability of a person having a height from 1.60 to 1.68 heights &lt;- c(1.50, 1.45, 1.54, 1.60,1.61,1.62,1.66,1.64,1.66, 1.66, 1.66, 1.66, 1.69, 1.70, 1.71, 1.72, 1.73, 1.74, 1.75, 1.80, 1.85, 1.90) #this how the graph looks like, where the blue line represents the pdf (probability density function) #assuming the heights of the population follows a normal distribution h&lt;-hist(heights, breaks=10, col=&quot;red&quot;) xfit&lt;-seq(min(heights),max(heights),length=40) yfit&lt;-dnorm(xfit,mean=mean(heights),sd=sd(heights)) yfit &lt;- yfit*diff(h$mids[1:2])*length(heights) lines(xfit, yfit, col=&quot;blue&quot;, lwd=2) # generally we can get the area under the graph by calculating the integral from x1 to x2, in a normal distribution we can use the z scores instead #to calculate the area from an x point to the mean, by relating to the standard normal distribution. We also know that the total area which represents #all the probabilities would be one, also the area from the left or right to the mean would be 0.5 due to the symmetry. #so in order to get what we are looking for we need to add the area from x1 to the mean and the area from x2 to the mean m &lt;- mean(heights) s &lt;- sd(heights) m ## [1] 1.675 #from x1 to m #the score is z1 &lt;- (1.60-m)/s #the area from z1 to end areaX1ToEnd &lt;- pnorm(z1) areaX1ToEnd ## [1] 0.2360952 #knowing that the aria from the mean to the end is 0.5, then the area from the mean to x1 is areaX1 &lt;- 0.5 - areaX1ToEnd areaX1 ## [1] 0.2639048 #from x2 to m z2 &lt;- (1.68-m)/s areaX2ToEnd &lt;- pnorm(z2) areaX2ToEnd ## [1] 0.5191132 areaX2 &lt;- areaX2ToEnd -0.5 areaX2 ## [1] 0.01911318 #since the one x is on the left of the mean and the other on the right we can just add them, otherwise we would have to also remove a common area TotalArea &lt;- areaX1+areaX2 TotalArea ## [1] 0.2830179 #that is equal to 30% probability that a persons hight is exactly in-between 1.60 to 1.68 #to measure the p value we need to also measure the probability of someone having a height less that 1.60 areaX1ToEnd ## [1] 0.2360952 #and the probability of someone having a height greater than 1.68 areaX2ToEnd ## [1] 0.5191132 #and add them up pValue &lt;- TotalArea + areaX1ToEnd + areaX2ToEnd pValue ## [1] 1.038226 #that is a large p value and therefor shows that having a height close to the mean is not uncommon 8.5 Confidence Intervals Problems 8.5.1 Confidence Intervals with t-values #the test scores of 9 randomly selected students are 83, 73, 71, 77, 77, 59, 92 #Compute the 99% confidence interval of the true mean. studentScores &lt;- c(83, 73, 71, 77, 77, 59, 92) scoreMean &lt;- mean(studentScores) n &lt;- length(studentScores) #since we do not know the standard deviation of the actual population, rather we only know the s of the sample and our sample n&lt;30 we will use t # values to calculate the error error &lt;- qt(0.99,df=n-1) * sd(studentScores)/sqrt(n) #so then our value is the sample mean +/- he error left &lt;- scoreMean - error right &lt;- scoreMean + error left ## [1] 63.8285 right ## [1] 88.1715 8.5.1.1 Confidence Intervals with s-values #Estimate the avarage weight for the adult male population.The avarage weight of 100 randomly selected adult males is 180lbs. Assume a population #standard deviation of 20lbs. Compute a 95% confidence interval for the population avarage weight. n &lt;- 100 meanWeight &lt;- 180 sPopulation &lt;- 20 #we can calculate the interval from a normal distribution error &lt;- qnorm(0.95)* sPopulation/sqrt(n) left &lt;- meanWeight - error right &lt;- meanWeight + error left ## [1] 176.7103 right ## [1] 183.2897 8.6 Chi-squared test 8.6.1 Chi-squared test manually step by step example #We want to find whether a dice is fair or not. The observed values were 22 for 1, 24 for 2, 38 for 3, 30 for 4, 46 for 5, 44 for 6. ####################### First step: state the null and alernative hypothesis############################################ #H0(null hypothesis): dice is fair so p=1/6 #Ha(alternative hypothesis): dice is not faire p != 1/6 trialDice &lt;- matrix(c(22,24,38,30, 46, 44), ncol=6) colnames(trialDice) &lt;- c(1,2,3,4,5,6) rownames(trialDice) &lt;- c(&quot;frequencies&quot;) trial.tableDice &lt;- as.table(trialDice) trial.tableDice ## 1 2 3 4 5 6 ## frequencies 22 24 38 30 46 44 n &lt;- sum(trial.tableDice[&quot;frequencies&quot;,]) expectedFr &lt;- 1/6*n ####################### Second step: choose the level of significance (a) ############################################# #a is the area under the curve in each tail where if ou result lies the H0 will be rejected (rejectipon region), in this case this is not given to use. #We will use a=0.01 for a &lt;- 0.01 ####################### Third step: Find critical value ############################################################### #critical value is the point (z value) that separates the tails as defined from a to the main curve #the standard deviation of the population is given so we will use a z test #(1-0.01)for R criticalValue &lt;-qchisq(0.99, df=5) criticalValue ## [1] 15.08627 ####################### Four step: Find test statistic ############################################################### tStat = sum((trial.tableDice[&quot;frequencies&quot;,]- expectedFr )^2)/expectedFr tStat ## [1] 15.29412 ####################### Five step: Draw a conclusion ############################################################### # tsat&lt;criticalValue and so it falls in the rejected area, so we can reject the null hypothesis and accept the Ha 8.6.2 Chi-squared test with contigency tables, manual step-by-step example #Does the sex of a person affects their choise of political part they support. We have 26 male rep, 13 male dem, 5 male other and #20 female rep, 29 female dem, 7 female other ####################### First step: state the null and alernative hypothesis############################################ #H0(null hypothesis): not affected #Ha(alternative hypothesis): affected #create our contingency table trial &lt;- matrix(c(26,20,13,29, 5, 7), ncol = 3) colnames(trial) &lt;- c(&quot;rep&quot;, &quot;dem&quot;, &quot;other&quot;) rownames(trial) &lt;- c(&quot;males&quot;, &quot;females&quot;) trial.table &lt;- as.table(trial) trial.table ## rep dem other ## males 26 13 5 ## females 20 29 7 totalFemales &lt;- sum(trial.table[&quot;females&quot;,]) totalMales &lt;- sum(trial.table[&quot;males&quot;,]) totalRep &lt;- sum(trial.table[,&quot;rep&quot;]) totalDem &lt;- sum(trial.table[,&quot;dem&quot;]) totalOther &lt;- sum(trial.table[,&quot;other&quot;]) totalSubjects &lt;- totalFemales + totalMales #expected values if Ho holds ExpMaleRep &lt;- totalMales * totalRep / totalSubjects ExpMaleDem &lt;- totalMales * totalDem / totalSubjects ExpMaleOther &lt;- totalMales * totalOther / totalSubjects ExFemaleRep &lt;- totalFemales * totalRep / totalSubjects ExFemaleDem &lt;- totalFemales * totalDem / totalSubjects ExFemaleOther &lt;- totalFemales * totalOther / totalSubjects exp &lt;- matrix(c(ExpMaleRep,ExFemaleRep,ExpMaleDem,ExFemaleDem,ExpMaleOther,ExFemaleOther), ncol=3) colnames(exp) &lt;- c(&quot;rep&quot;, &quot;dem&quot;, &quot;other&quot;) rownames(exp) &lt;- c(&quot;males&quot;, &quot;females&quot;) exp.table &lt;- as.table(exp) exp.table ## rep dem other ## males 20.24 18.48 5.28 ## females 25.76 23.52 6.72 ####################### Second step: choose the level of significance (a) ############################################# #a is the area under the curve in each tail where if ou result lies the H0 will be rejected (rejectipon region), in this case this is not given to use. #We will use a=0.05 for #example chi square distribution for visibility x &lt;- rchisq(100, 5) hist(x, prob=TRUE) curve( dchisq(x, df=5), col=&#39;green&#39;, add=TRUE) #aria after red line falles in rejectred area (this is an example) abline(v=10, col=&quot;red&quot;) a &lt;- 0.05 ####################### Third step: Find critical value ############################################################### #critical value is the point (z value) that separates the tails as defined from a to the main curve #the standard deviation of the population is given so we will use a z test #(1-0.05)for R criticalValue &lt;-qchisq( 0.95, df=2) criticalValue ## [1] 5.991465 ####################### Four step: Find test statistic ############################################################### eachExquaer = (trial.table - exp.table )^2/exp.table tStat &lt;- sum(eachExquaer) tStat ## [1] 5.855499 ####################### Five step: Draw a conclusion ############################################################### # tsat&gt;criticalValue and so it does not falls in the rejected area, so we can accept the null hypothesis and we cannot accept the Ha ###################################################################################################### ########################## CHI-SQUARE TEST Of INDEPENDANCY IN R ###################################### ###################################################################################################### chisq.test(trial.table) ## ## Pearson&#39;s Chi-squared test ## ## data: trial.table ## X-squared = 5.8555, df = 2, p-value = 0.05352 #this returned our x-squared value wich validated our t-statistic and a p-value of 0.05352, which is significant and therefore we can not reject the H0 chisq.test(trial.table)$expected ## rep dem other ## males 20.24 18.48 5.28 ## females 25.76 23.52 6.72 #this returns the expected values, it validetes the ones we calculated previously on exp.table and it can be used to compere with our actual values #to confirm this Rexpected &lt;- chisq.test(trial.table)$expected #returns true for all values :) :) Rexpected == exp.table ## rep dem other ## males TRUE TRUE TRUE ## females TRUE TRUE TRUE #lets make some charts barplot(trial.table, legend= TRUE, beside = T) 8.6.3 Chi-square goodness of fit in R #we use the dice example again frequenciesGiven&lt;- c(22,24,38,30, 46, 44) #calculated on top of the page pForEach&lt;- c(1/6,1/6,1/6,1/6, 1/6, 1/6) #this validates our previous results and so it gives a p=0.009177 which is a very small probability for the H0 to be tru, therefore we accept the Ha chisq.test(frequenciesGiven, p=pForEach) ## ## Chi-squared test for given probabilities ## ## data: frequenciesGiven ## X-squared = 15.294, df = 5, p-value = 0.009177 8.6.4 Fisher’s Exact test in R #this is used for non-parametric data (not following a normal distribution) #legend has it, it was discovered when testing if a lady in the UK could tell if milk was poured before or after #so lets take this example. Assume 20 trials out of which the lady gets guesses 9 times correctly that tea was poured before, out of the 10 in which it actually was. n &lt;- 20 s &lt;- 9 milkBefore &lt;- 10 #there are 4 ways out of 8 to choose the tea that was made with milk before tea comb = function(n, x) { factorial(n) / (factorial(n-x) * factorial(x)) } totalWays &lt;- comb(n,milkBefore) totalWays ## [1] 184756 #the lady got 4, so we need to calculate in how many ways she could have gotten 4 out of 5 #there are ten ways in 10 orders waysToGuessSuccess &lt;- 10*10 waysToGuessSuccess ## [1] 100 #if we assume the H0, the probability that she got it correctly at random would be: p &lt;- waysToGuessSuccess / totalWays p ## [1] 0.0005412544 # p= 0.02, which is quite small smaller that 0.05 which is usually the threshold so we can reject the H0. According to this trial the lady can, most likely, #tell whether or not milk was poured before or after the tea #let&#39;s try this using R built in functionality TeaTasting &lt;- matrix(c(9, 1, 1, 9), nrow = 2, dimnames = list(Guess = c(&quot;Milk&quot;, &quot;Tea&quot;), Truth = c(&quot;Milk&quot;, &quot;Tea&quot;))) fisher.test(TeaTasting, alternative = &quot;greater&quot;) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: TeaTasting ## p-value = 0.0005467 ## alternative hypothesis: true odds ratio is greater than 1 ## 95 percent confidence interval: ## 4.338108 Inf ## sample estimates: ## odds ratio ## 51.3254 8.7 Anova 8.7.1 Two-way ANOVA with interaction testing #built in data set head(warpbreaks) ## breaks wool tension ## 1 26 A L ## 2 30 A L ## 3 54 A L ## 4 25 A L ## 5 70 A L ## 6 52 A L summary(warpbreaks) ## breaks wool tension ## Min. :10.00 A:27 L:18 ## 1st Qu.:18.25 B:27 M:18 ## Median :26.00 H:18 ## Mean :28.15 ## 3rd Qu.:34.00 ## Max. :70.00 #standard model model1 &lt;- aov(breaks ~ wool + tension, data = warpbreaks) #we can see that tension is significant to the breaks summary(model1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wool 1 451 450.7 3.339 0.07361 . ## tension 2 2034 1017.1 7.537 0.00138 ** ## Residuals 50 6748 135.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #add an interaction manually, product of wool and tension model2 &lt;- aov(breaks ~ wool + tension + wool:tension, data = warpbreaks) #we see that the interaction of wool with tension (combination) is fairly significant summary(model2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wool 1 451 450.7 3.765 0.058213 . ## tension 2 2034 1017.1 8.498 0.000693 *** ## wool:tension 2 1003 501.4 4.189 0.021044 * ## Residuals 48 5745 119.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #check for all interactions, should return the same model model3 &lt;- aov(breaks ~ wool * tension, data = warpbreaks) summary(model3) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wool 1 451 450.7 3.765 0.058213 . ## tension 2 2034 1017.1 8.498 0.000693 *** ## wool:tension 2 1003 501.4 4.189 0.021044 * ## Residuals 48 5745 119.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 8.7.2 Manual step-by-step example #this is a balanced test since for every poisson the same amount of tests/treatments were used survivalTimeDS &lt;- read.csv(&quot;data/poison_balanced.tsv&quot;, sep=&quot;&quot;) #we can see the boxplot to get an idea of the variance boxplot(survivalTime~ treatment* poison ,data=survivalTimeDS) summary(survivalTimeDS) ## poison treatment survivalTime ## Min. :1 A:12 Min. :0.1800 ## 1st Qu.:1 B:12 1st Qu.:0.3000 ## Median :2 C:12 Median :0.4000 ## Mean :2 D:12 Mean :0.4794 ## 3rd Qu.:3 3rd Qu.:0.6225 ## Max. :3 Max. :1.2400 #H0: type of poison has no affect #H0: type o treatment has no affect #H0: combination of poison and type has no affect ############################ inspect the mean survival time for each combination ####################################### library(plyr) #per poison meanPerPoison &lt;-ddply(survivalTimeDS, .(poison), summarize, mean=mean(survivalTime)) meanPerPoison ## poison mean ## 1 1 0.617500 ## 2 2 0.544375 ## 3 3 0.276250 #per treatment meanPerTreatment &lt;-ddply(survivalTimeDS, .(treatment), summarize, mean=mean(survivalTime)) meanPerTreatment ## treatment mean ## 1 A 0.3141667 ## 2 B 0.6766667 ## 3 C 0.3925000 ## 4 D 0.5341667 #for each poisson each treatment meanPoisonTreat &lt;-ddply(survivalTimeDS, .(treatment, poison), summarize, mean=mean(survivalTime)) meanPoisonTreat ## treatment poison mean ## 1 A 1 0.4125 ## 2 A 2 0.3200 ## 3 A 3 0.2100 ## 4 B 1 0.8800 ## 5 B 2 0.8150 ## 6 B 3 0.3350 ## 7 C 1 0.5675 ## 8 C 2 0.3750 ## 9 C 3 0.2350 ## 10 D 1 0.6100 ## 11 D 2 0.6675 ## 12 D 3 0.3250 #total meanTimeForAll &lt;- mean(survivalTimeDS$survivalTime) meanTimeForAll ## [1] 0.479375 ##########################Sum square of first factor(poison)############################################################# #this is given by calculating the squared difference of the grand mean to the mean for each #poison and then summing the result #for robustness we make a function that selects the mean for a given poison and does the calculations sumOfSquaresForPoisonF &lt;- function(poisonGiven){ #we multiply by 4 because we have 4 treatments per poison 4*(((subset(meanPerPoison,poison==poisonGiven,select=&quot;mean&quot;))[1,]-meanTimeForAll)^2) } #then we apply this function for poison 1,2,3 and sum the result sumOfSquaresForPoison &lt;- sum(mapply(sumOfSquaresForPoisonF, c(1,2,3))) ##########################Sum square of second factor(treatment)###################################################### #as above but for treatments this time sumOfSquaresForTreatmentsF &lt;- function(treatmentGiven){ #we multiply by 3 because we have 3 poisons per treatment 3*(((subset(meanPerTreatment,treatment==treatmentGiven,select=&quot;mean&quot;))[1,]-meanTimeForAll)^2) } sumOfSquaresForTreatments &lt;- sum(mapply(sumOfSquaresForTreatmentsF, c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;))) sumOfSquaresForTreatments ## [1] 0.2303016 ########################## Sum square within error ###################################################################### #This is the square sum for each survival time in our dataset minus the average for that poison and treatment sumOfSquaresWithErrorF &lt;- function(treatmentGiven, poisonGiven){ subTrPoi &lt;- subset(survivalTimeDS, treatment==treatmentGiven &amp; poison== poisonGiven) meanOfTrPoi &lt;- subset(meanPoisonTreat, treatment==treatmentGiven&amp; poison== poisonGiven,select=&quot;mean&quot;) sumTrPoi &lt;- sum((subTrPoi$survivalTime - meanOfTrPoi)^2) return(sumTrPoi) } #to get all combinations A1, A2,A3,B1....we need to use outer product x &lt;- factor(c(&quot;A&quot;, &quot;B&quot;, &quot;c&quot;, &quot;D&quot;)) y &lt;- c(1,2,3) product &lt;- expand.grid(x, y) #apply the function to all possible combinations and sum them up sumOfSquaresWithError &lt;- sum(mapply( sumOfSquaresWithErrorF,treatmentGiven= as.character(product[1,&quot;Var1&quot;]),poisonGiven=product[1,&quot;Var2&quot;])) sumOfSquaresWithError ## [1] 0.01050625 ########################## Sum of Square Total ################################ sumSquareTotal &lt;- sum((survivalTimeDS$survivalTime - meanTimeForAll)^2) sumSquareTotal ## [1] 3.005081 ######################### sum square of both factors ############################################ #sum of both factors is given by sumOfSquareBothFactors &lt;- sumSquareTotal - sumOfSquaresWithError -sumOfSquaresForTreatments - sumOfSquaresForPoison ######################### calculating the degrees of freedom for each sum of squares ############ #for first factor (poison) dfFirstFactor &lt;- 3-1 #for second factor (treatment) dfSecondFactor &lt;- 4-1 #for within error we add up n-1 of each treatment for each poison so: #(4-1) a treatment for a poison *3 one treatment for each poison *4 each treatment for each poison dfWithinError &lt;- (4-1)*3*4 #sum of both squares, we multiply df of first and second dfSumOfBoth &lt;- dfFirstFactor * dfSecondFactor #total degree of freedoms, this is the sum of all of them dfTotal &lt;- dfFirstFactor + dfSecondFactor + dfWithinError + dfSumOfBoth dfTotal ## [1] 47 ######################### calculating the mean square of sum of square within error ############ #we will need this to calculate the f-scores which will allow us to draw our conclusions for each H0 #this is the sum of squares within error divided by its degrees of freedom so: meanSquareOfSumWithinError &lt;- sumOfSquaresWithError/dfWithinError meanSquareOfSumWithinError ## [1] 0.0002918403 ######################### H0: poison does not affect the survival time ############################ #we need to calculate the F-score for this which is meanSquareOfFirstFactor/meanSquareWithinError #so we need the mean square of 1st factor: meanSquareOfFirstFactor &lt;- sumOfSquaresForPoison/dfFirstFactor FscoreForPoison &lt;- meanSquareOfFirstFactor/meanSquareOfSumWithinError # F(dfFirstFactor ,dfWithinError) = FscoreForPoison p&lt;0.5 for a 95%confidence interval, #we can find the critical value for df of numerator dfFirstFactor and dfWithinError denominator from the F distribution cvForPoison &lt;- qf(.95, df1=dfFirstFactor, df2=dfWithinError) cvForPoison ## [1] 3.259446 #FscoreForPoison falls in the rejection area and so we can reject he H0 and accept that the poison does affect the #survival time ####### you can repeat this for the rest of the H0&#39;s ############################################## ### or we can be sensible and use..... r ##### #however you need to ensure your factors are actually of type factor!!! survivalTimeDS$poison &lt;- as.factor(survivalTimeDS$poison) modelForSurvivalTime &lt;- lm(survivalTime ~ treatment * poison, data = survivalTimeDS) #type does not matter since our test is balanced library(car) Anova(modelForSurvivalTime, type=&quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: survivalTime ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 0.68062 1 30.6004 2.937e-06 *** ## treatment 0.45395 3 6.8031 0.0009469 *** ## poison 0.08222 2 1.8482 0.1721570 ## treatment:poison 0.25014 6 1.8743 0.1122506 ## Residuals 0.80073 36 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #from the above we can reject that poison and treatment has no effect and accept that the interaction has no effect "]
]
