# Dimention Reducing Algorithms

We have previously seen a few alternatives for feature selection. In this chapter we will look into methods that allow us to reduce the number of predictors with the use of some linear algebra. We will see how we can reduce problems of multiple dimentions into 2-D space, by ploting new axis and exptrapolating the data on them. We will aslo discover ways to visually represent multidimentional data and perform classifications. 

Dimention redaction is often performend when:

* The problem requires regression or classification
* We want to visualize data of multiple dimentions
* The data we have has multiple features that are highly correlated, and out of all the features, a few of them are the majour drivers
* We do not have the responce variable (unsupervised learning)

##Unsupervised learning

Up until now the data that we were analyzed had been _labeled_, that is we knew the associated y/reaction variable. If we remeber, the salaries assosiated with the baseball players were known, when analyzing the Hitters dataset, similarly the price of the properties on the Boston dataset was also given. Tha helped us gain a clear understanding of the problem. We wonted to study the relationship between the features and the reaction variable, and use it to make predictions. Having the reaction variable readilly avalable, also meant that were able to measure the performance of our models with ease, using methods such us cross-validation and confution matrix. But what if our promblem was something like this?

* A company like Netfix wants to identify groups of users with similar taste, in order to improve their recommendations. What defines taste? We can look into the genre of the film, the users age and the users ratings, but there isnt really a clearly defined problem or responce variable. Similarly, they might want to classify movies depending on their popularity. How do we find What the predictors that make a movie popular, and what do we clasify as popularity?

* A researcher wants to group patiants that have a rare dissease using their gene expressions, symtoms and recovery progress. This will allow him to conduct his studies easier and treat each patiant more effectivily depending on their variation of the studied attributes. We do not know what or how many groups, this classification will result into, neither do we know which of the attributes collected are usufull. 

Unlike, our previous classificcation problem of bank account defauting, where the outcome was either defauted or not defauted, in such cases we simply do not know what the outcome will be.

When the responce is not clear, for example we do not know what categories will result from the data, we say that this is an _unsupervised_ problem. 


## Principal Component Analysis (PCA)

PCA is an unsupervised machine learning algorithm, used for dimention reduction. Just as with supervised learning we will be trying to gain insight on our problem by studing the variations in the datset, trying to explain them and use the explanations for classifications or predictions. Often we are faced with datasets containing multiple predictors, wich can be highly correlated, making it difitult to perform this analysis. PCA can reduce the number of variables, while maintining the measures that explain most of of the variability in a dataset. It does that by contracting new representativive variables.

We will see how this is achived step by in this section.


### Linear Transformations

First, we will need a small and friendly overviwe of some linear algebra, to get the idea of how or why we are following those steps. This is just to give you some backround.

Before we get into explaining linear transformations, we need to explain what we mean by vector. We can say a vector is just a list of coordinates that describe how to get to a point from the origin of the axis(and is drawn like an arrow).

A linear transformation, is nothing more than a function that takes on a vector input and outputs another vector. One way to think of how a vector is changed to another, is if the space around it is 'transformed' (scaled or rotated).The world linear is because we want this function to not transform the space in such a way that it causes the origin to move or to cause curves in the space and axis. A transformation can be described in a matrix. A matrix is a combination of vectors. Here its of those vectors will describe a change in each axis.

In order to identify a transofmation in 2-D space you just need to find out how the x, y axis has changed. All the vectors contained in that space will have changed proportionally. Similarly if you had more dimentions you whould need to find out how the rest of your axis have changed, in order to place everything else acordingly.

There is a spacial case of vectors that will have not been moved from their spam (their general direction) after a linear transformation. They may have been shrinked or streched but not rotated. This vectors are called Eigen vectors and the magnitute by which they have been schaled is called the eigen value. The coordinates of the eigen vector in PCA are refeared to as loading values.

The fact that its direction hasn't change means that the product of the transformation matrix and that vector was zero. The only way it is possible for a product of non-zero matrixs to be zero is if the transformation assosiated, squasies space into a lower dimention. If we imagine rotating a plane we will end up looking at a single line at some point. This whould represent the eigen vector.

Since we want to perform a transformation that reduces the dimentions but maintains the 'essence', we whould be very interested in finding out eigen values.


### Steb by step process


Let's start with a 2-D example, we have 2 predictors and we want to calculate the principal components for them. The goal is to perform a linear transformation, where the new axis (named PC1 and PC2) on which our data points are projected can better describe the data. If we had more that two axis we whould like to be able to remove some of them and still be able to describe the data. 


1. We swift the data so that the center of the data points is at the origin of the axis (they are swifted without loosing the relative distances to each other)

2. We find the best fit line between those points. This is done in a similar way to least squared distance in linear regression. However, since we want to perform a linear transformation we need this line to pass through the origin. Because this is true instead of minimizing the disctance of our points to the best fitting line, we can instead maximize the distance of the projected (to the best fiting line) points to the origin. This distance is at a 90 degree angle from the disctance of our points to the best fitting line and since the distance of each point to the origin is always constant, they are inversy proportianal (when the disctance of our points to the best fitting line decreases, the distance from the projected points to the origin increases), from the pythagorium theorum. 

3. This best fitting line that maximizes the sum of squared difference of the projected point to the origin is called the first principle component (PC1). 

4. PC2 will just be the line that also passes from the origin and is perpendicular to PC1















